{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a025310-4ef4-4fa0-9b42-e11f8a877135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as tt\n",
    "import torchvision\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import glob\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d80cf7-6eba-4710-b8cf-9f4c708538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/kaggle/input/segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/images\"\n",
    "mask_path = \"/kaggle/input/segmentation-full-body-mads-dataset/segmentation_full_body_mads_dataset_1192_img/segmentation_full_body_mads_dataset_1192_img/masks\"\n",
    "img_files = sorted(glob.glob(img_path + \"/*\"))\n",
    "mask_files = sorted(glob.glob(mask_path + \"/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c599e1fd-0395-4c0e-9f2d-4a3f4d7c679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(index):\n",
    "    img_input = np.array(Image.open(img_files[index]))\n",
    "    mask = np.array(Image.open(mask_files[index]))\n",
    "    print(mask.shape)\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(12,8))\n",
    "    ax[0].imshow(img_input)\n",
    "    ax[1].imshow(mask)\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5354d5b5-a5e3-4cc5-8bdd-797b21893012",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std =[0.229, 0.224, 0.225]\n",
    "\n",
    "class Humans(Dataset):\n",
    "    def __init__(self, img_paths, label_paths, H=1500, transforms=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transforms = transforms\n",
    "        self.H = H\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_input = (Image.open(self.img_paths[index]))\n",
    "        mask = (Image.open(self.label_paths[index]))\n",
    "        \n",
    "        img_transforms = [\n",
    "            tt.ToTensor(),\n",
    "            tt.Resize((self.H, self.H), antialias=True),\n",
    "            tt.Normalize(mean,std)\n",
    "        ]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img_transforms = img_transforms + self.transforms\n",
    "        \n",
    "        img_transforms = tt.Compose(img_transforms)\n",
    "        transforms_mask = tt.Compose([\n",
    "            tt.Grayscale(),\n",
    "            tt.Resize((self.H, self.H), antialias=True),\n",
    "            tt.ToTensor()\n",
    "        ])\n",
    "        mask_tensor = (transforms_mask(mask) > 0.05).float()\n",
    "        img_tensor = img_transforms(img_input)\n",
    "        \n",
    "        return img_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fde68b1-3bd9-41fb-9177-a93019ab9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def dice_loss(self, pred: torch.Tensor, gt: torch.tensor):\n",
    "        intersection = torch.sum((pred * gt))\n",
    "        epsilon = 1e-8\n",
    "        union = (pred + gt).sum() + epsilon\n",
    "        return 1 - ((2 * intersection) / union)\n",
    "    \n",
    "    def forward(self, gt, pred):\n",
    "        \n",
    "        dice_loss = 0\n",
    "        for cl in range(pred.shape[1]):\n",
    "            dice_loss += self.dice_loss(pred[:,cl,:,:], gt.eq(cl).float())\n",
    "        \n",
    "        dice_loss /= pred.shape[1]        \n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51370387-f6bc-4098-82ff-d006951cbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs: int, optimizer: torch.optim, model: nn.Module, learning_rate: float,\n",
    "       train_loader: torch.utils.data.DataLoader, val_loader: torch.utils.data.DataLoader, \n",
    "        learning_rate_scheduler: torch.optim.lr_scheduler,\n",
    "        model_name=\"best_model.pt\", **kwargs):\n",
    "    \n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)   \n",
    "    lrs = learning_rate_scheduler(optimizer, **kwargs)\n",
    "    history = []\n",
    "    min_val_loss = 10e20\n",
    "    old_lr = learning_rate\n",
    "    for epoch in range(epochs): \n",
    "        #model.select_loss(epoch)\n",
    "        for num, batch in enumerate(train_loader):\n",
    "            train_losses = []\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.detach())\n",
    "            print(f\"Batch num [{num}]: loss {loss})\")\n",
    "        \n",
    "        \n",
    "        result = model.evaluate(val_loader)\n",
    "        result[\"train_loss\"] = torch.stack(train_losses).detach().mean().item()\n",
    "        model.epoch_end_val(epoch, result)\n",
    "        history.append(result)\n",
    "        lrs.step(result[\"val_loss\"])\n",
    "        \n",
    "        if optimizer.param_groups[0][\"lr\"] != old_lr:\n",
    "            old_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"Updated learning rate to {old_lr:.4f}\")\n",
    "        \n",
    "        if result[\"val_loss\"] < min_val_loss:\n",
    "            torch.save(model, model_name)\n",
    "            min_val_loss = result[\"val_loss\"]\n",
    "\n",
    "    return history       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0be5f1-46c0-4832-bd81-c0f1353fdc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "        Training step which computes the loss and accuracy of a train batch\n",
    "        :param batch: batch of pytorch dataloader\n",
    "        :type batch: torch.utils.data.DataLoader\n",
    "        :return: loss, accuracy and f1_score of batch\n",
    "        :rtype: tuple[torch.tensor,...]\n",
    "        \"\"\"\n",
    "        # Runs the forward pass with autocasting.\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        self.train()\n",
    "        images, target_mask = batch\n",
    "        prediction_mask = self(images)\n",
    "        train_loss = self.loss(target_mask, prediction_mask)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            images, target_mask = batch\n",
    "            prediction_mask = self(images)\n",
    "            val_loss = self.loss(target_mask, prediction_mask)\n",
    "            return {\"val_loss\": val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Returns the epoch losses after computing the mean loss and accuracy of the test batches\n",
    "\n",
    "        :param outputs: List of test step outputs\n",
    "        :type outputs: list\n",
    "        :return: epoch loss and epoch accuracy\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean().item()\n",
    "\n",
    "        return {\"val_loss\": epoch_loss}\n",
    "\n",
    "    def evaluate(self, dl):\n",
    "        outputs = [self.validation_step(batch) for batch in dl]\n",
    "        return self.validation_epoch_end(outputs)\n",
    "\n",
    "    def epoch_end_val(self, epoch, results):\n",
    "        \"\"\"\n",
    "        Prints validation epoch summary after every epoch\n",
    "\n",
    "        :param epoch: epoch number\n",
    "        :type epoch: int\n",
    "        :param results: results from the evaluate method\n",
    "        :type results: dictionary\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        print(\n",
    "            f\"Epoch:[{epoch}]: |validation loss: {results['val_loss']}|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5952fb89-8164-4ec2-960d-0377bc1d24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling (ASPP) layer as described in DeepLab architectures.\n",
    "    This layer uses multiple dilated convolutions to capture multi-scale information.\n",
    "    \n",
    "    :param in_ch: Number of input channels.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # Convolution with dilation rate of 1 (normal convolution)\n",
    "        self.aconv1 = nn.Conv2d(in_ch, 256, 3, dilation=1, padding=\"same\")\n",
    "        \n",
    "        # Convolution with dilation rate of 6\n",
    "        self.aconv2 = nn.Conv2d(in_ch, 256, 3, dilation=6, padding=\"same\")\n",
    "        \n",
    "        # Convolution with dilation rate of 12\n",
    "        self.aconv3 = nn.Conv2d(in_ch, 256, 3, dilation=12, padding=\"same\")\n",
    "        \n",
    "        # Convolution with dilation rate of 18\n",
    "        self.aconv4 = nn.Conv2d(in_ch, 256, 3, dilation=18, padding=\"same\")\n",
    "        \n",
    "        # Convolution with dilation rate of 24\n",
    "        self.aconv5 = nn.Conv2d(in_ch, 256, 3, dilation=24, padding=\"same\")\n",
    "        \n",
    "        # Batch normalization for concatenated feature maps\n",
    "        self.bn = nn.BatchNorm2d(256 * 5)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Prediction convolution\n",
    "        self.pred_conv = nn.Conv2d(256 * 5 ,out_ch, 1, padding=\"same\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the ASPP layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after ASPP.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass the input through each dilated convolution and apply ReLU\n",
    "        out1 = self.relu(self.aconv1(x))\n",
    "        out2 = self.relu(self.aconv2(x))\n",
    "        out3 = self.relu(self.aconv3(x))\n",
    "        out4 = self.relu(self.aconv4(x))\n",
    "        out5 = self.relu(self.aconv5(x))\n",
    "        \n",
    "        # Concatenate the outputs along channel dimension\n",
    "        cat = torch.cat((out1, out2, out3, out4, out5), dim=1)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        out = self.bn(cat)\n",
    "        \n",
    "        # Apply the prediction convolution\n",
    "        pred = self.pred_conv(out)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a976647-1afd-4ca4-b966-95b100a2883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabv3(nn.Module, ModelBase):\n",
    "    def __init__(self, loss, final_softmax=True):\n",
    "        super().__init__()\n",
    "        backbone = torchvision.models.resnet101(\n",
    "            weights=torchvision.models.ResNet101_Weights.DEFAULT\n",
    "        )\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-3])\n",
    "        self.aspp = ASPP(1024, 2)\n",
    "        self.up = nn.Upsample(scale_factor = 16, mode=\"bilinear\")\n",
    "        \n",
    "        if final_softmax:\n",
    "            self.sm = nn.Softmax2d()\n",
    "        else:\n",
    "            self.sm = nn.Identity()\n",
    "            \n",
    "        self.loss = loss\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        out = self.aspp(x)\n",
    "        out = self.sm(out)\n",
    "        up = self.up(out)\n",
    "        \n",
    "        return up\n",
    "    \n",
    "    def select_loss(self, epoch):\n",
    "        if epoch < 10:\n",
    "            self.loss = SegLoss(0)\n",
    "        \n",
    "        else:\n",
    "            self.loss = SegLoss(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e58238-90b1-492d-a3a7-30f13685a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabv3Plus(nn.Module, ModelBase):\n",
    "    def __init__(self, loss, final_softmax=True):\n",
    "        super().__init__()\n",
    "        backbone = torchvision.models.resnet101(\n",
    "            weights=torchvision.models.ResNet101_Weights.DEFAULT\n",
    "        )\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-3])\n",
    "        self.aspp = ASPP(1024, 256)\n",
    "        self.onebyone_conv = nn.Conv2d(1024,256,1, padding=\"same\")\n",
    "        self.low_level_upsampler = nn.Upsample(scale_factor=4, mode=\"bilinear\")\n",
    "        self.up_encoder = nn.Upsample(scale_factor=4, mode=\"bilinear\")\n",
    "        self.up_backbone = nn.Upsample(scale_factor=4, mode=\"bilinear\")\n",
    "        self.decoder_conv = nn.Conv2d(512, 2, 3, padding=\"same\")\n",
    "        self.up_final = nn.Upsample(scale_factor=4, mode=\"bilinear\")\n",
    "        \n",
    "        if final_softmax:\n",
    "            self.sm = nn.Softmax2d()\n",
    "        else:\n",
    "            self.sm = nn.Identity()\n",
    "            \n",
    "        self.loss = loss \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        low_level_features = self.onebyone_conv(x)\n",
    "        low_level_features_up = self.low_level_upsampler(low_level_features)\n",
    "        encoder_out = self.aspp(x)\n",
    "        encoder_out_up = self.up_encoder(encoder_out)\n",
    "        \n",
    "\n",
    "        combined = torch.cat((low_level_features_up, encoder_out_up), dim=1)\n",
    "        out = self.decoder_conv(combined)\n",
    "        out = self.sm(out)\n",
    "        up = self.up_final(out)\n",
    "        \n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ebb0f0-6547-41aa-80c4-46cfbd10a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        if x2 is not None:\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "            # input is CHW\n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "            x = F.pad(x, (diffX // 2, diffX - diffX // 2), (diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        else:\n",
    "            x = x1\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return(x)\n",
    "    \n",
    "    \n",
    "class UnetResNet32(nn.Module, ModelBase):\n",
    "    def __init__(self, loss, final_softmax: bool = True, \n",
    "                 num_classes=2,train_backbone: bool = True):\n",
    "        \"\"\"\n",
    "        UnetResNet32 class.\n",
    "\n",
    "        :param final_resolution: The final resolution of the heatmap\n",
    "        :param num_classes: The number of output classes. Defaults to 1.\n",
    "        :param model_name: The name of the backbone model. Defaults to \"resnet18\".\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        basemodel = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.basemodel = nn.Sequential(*list(basemodel.children())[:-2])\n",
    "\n",
    "        for param in self.basemodel.parameters():\n",
    "            param.requires_grad = train_backbone\n",
    "\n",
    "        self._NUM_CLASSES = num_classes\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Instantiate hooks\n",
    "        # Keep the outputs of the intermediate layers\n",
    "        self.outputs = {}\n",
    "\n",
    "        for i, layer in enumerate(list(self.basemodel.children())):\n",
    "            layer.register_forward_hook(self.save_output)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Create decoding part\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.up5 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "\n",
    "        self.up_conv4 = DoubleConv(256 + 256, 256)\n",
    "        self.up_conv3 = DoubleConv(2 + 128, 128)\n",
    "        self.up_conv3 = DoubleConv(128 + 128, 128)\n",
    "        self.up_conv2 = DoubleConv(64 + 64, 64)\n",
    "        self.up_conv1 = DoubleConv(32 + 64, 64)\n",
    "\n",
    "        self.outc3 = nn.Conv2d(128, self._NUM_CLASSES, kernel_size=1)\n",
    "        self.outc2 = nn.Conv2d(64, self._NUM_CLASSES, kernel_size=1)\n",
    "        self.outc1 = nn.Conv2d(64, self._NUM_CLASSES, kernel_size=1)\n",
    "        \n",
    "        if final_softmax:\n",
    "            self.sm = nn.Softmax2d()\n",
    "        else:\n",
    "            self.sm = nn.Identity()\n",
    "            \n",
    "        self.loss = loss \n",
    "\n",
    "    def save_output(self, module, input, output):\n",
    "        self.outputs[module] = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.basemodel(x)\n",
    "\n",
    "        x = self.up2(self.outputs[list(self.basemodel.children())[-1]])\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-2]]), dim=1)\n",
    "        x = self.up_conv4(x)\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-3]]), dim=1)\n",
    "        x = self.up_conv3(x)\n",
    "        output_first_map = nn.Upsample(scale_factor=8, mode=\"bilinear\")(self.outc3(x))\n",
    "\n",
    "        x = self.up4(x)\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-4]]), dim=1)\n",
    "        x = self.up_conv2(x)\n",
    "        intermediate_output_second = nn.Upsample(scale_factor=4, mode=\"bilinear\")(self.outc2(x))\n",
    "        output_second_map = torch.add(output_first_map, intermediate_output_second)\n",
    "\n",
    "        x = self.up5(x)\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-6]]), dim=1)\n",
    "        x = self.up_conv1(x)\n",
    "        intermediate_output_third = nn.Upsample(scale_factor=2, mode=\"bilinear\")(self.outc1(x))\n",
    "        final_output = torch.add(output_second_map, intermediate_output_third)\n",
    "        final_output = self.sm(final_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a8bd5d-0d82-430f-a2c8-190948b38856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [x.to(device) for x in data]\n",
    "    return data.to(device)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    def __init__(self, dl): \n",
    "        self.dl = dl\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6af0a619-6bd5-41bf-9511-9dd69defec89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m ds \u001b[38;5;241m=\u001b[39m Humans(img_files, mask_files, \u001b[38;5;241m512\u001b[39m, transforms)\n\u001b[0;32m     10\u001b[0m ds_train, ds_val \u001b[38;5;241m=\u001b[39m random_split(ds, (\u001b[38;5;241m0.88\u001b[39m,\u001b[38;5;241m0.12\u001b[39m))\n\u001b[1;32m---> 11\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DeviceDataLoader(\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DeviceDataLoader(DataLoader(ds_val, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "transforms = [\n",
    "    tt.RandomAdjustSharpness(0.3),\n",
    "    tt.RandomAutocontrast(),\n",
    "    tt.RandomGrayscale(),\n",
    "    tt.RandomApply([\n",
    "        tt.ColorJitter()\n",
    "    ])]\n",
    "\n",
    "ds = Humans(img_files, mask_files, 512, transforms)\n",
    "ds_train, ds_val = random_split(ds, (0.88,0.12))\n",
    "train_loader = DeviceDataLoader(DataLoader(ds_train, num_workers=2, batch_size=8, shuffle=True))\n",
    "val_loader = DeviceDataLoader(DataLoader(ds_val, num_workers=2, batch_size=8, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48a040-f00a-4424-bd91-3356df27b7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
