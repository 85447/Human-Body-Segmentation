{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86023fa0",
   "metadata": {},
   "source": [
    "# IMPORTING NECESSARY LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08993b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as tt\n",
    "import torchvision\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import glob\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b0cf3",
   "metadata": {},
   "source": [
    "# IMPORTING DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04166fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"C:\\\\Users\\\\pcs\\\\OneDrive\\\\Desktop\\\\Project Major\\\\segmentation_full_body_mads_dataset_1192_img\\\\segmentation_full_body_mads_dataset_1192_img\\\\images\"\n",
    "mask_path = \"C:\\\\Users\\\\pcs\\\\OneDrive\\\\Desktop\\\\Project Major\\\\segmentation_full_body_mads_dataset_1192_img\\\\segmentation_full_body_mads_dataset_1192_img\\\\masks\"\n",
    "img_files = sorted(glob.glob(img_path + \"/*\"))\n",
    "mask_files = sorted(glob.glob(mask_path + \"/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f01ca",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6268617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(index):\n",
    "    img_input = np.array(Image.open(img_files[index]))\n",
    "    mask = np.array(Image.open(mask_files[index]))\n",
    "    print(mask.shape)\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(12,8))\n",
    "    ax[0].imshow(img_input)\n",
    "    ax[1].imshow(mask)\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c8ebe",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037097e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std =[0.229, 0.224, 0.225]\n",
    "\n",
    "class Humans(Dataset):\n",
    "    def __init__(self, img_paths, label_paths, H=1500, transforms=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transforms = transforms\n",
    "        self.H = H\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_input = (Image.open(self.img_paths[index]))\n",
    "        mask = (Image.open(self.label_paths[index]))\n",
    "        \n",
    "        img_transforms = [\n",
    "            tt.ToTensor(),\n",
    "            tt.Resize((self.H, self.H), antialias=True),\n",
    "            tt.Normalize(mean,std)\n",
    "        ]\n",
    "        \n",
    "        if self.transforms is not None: \n",
    "            img_transforms = img_transforms + self.transforms\n",
    "        \n",
    "        img_transforms = tt.Compose(img_transforms)\n",
    "        transforms_mask = tt.Compose([\n",
    "            tt.Grayscale(),\n",
    "            tt.Resize((self.H, self.H), antialias=True),\n",
    "            tt.ToTensor()\n",
    "        ])\n",
    "        mask_tensor = (transforms_mask(mask) > 0.05).float()\n",
    "        img_tensor = img_transforms(img_input)\n",
    "        \n",
    "        return img_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72337d80",
   "metadata": {},
   "source": [
    "# Defining model helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfef710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegLoss, self).__init__()\n",
    "\n",
    "    def dice_loss(self, pred: torch.Tensor, gt: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute Dice loss for a single class.\n",
    "        :param pred: Predicted probabilities for a specific class (batch_size, height, width).\n",
    "        :param gt: Ground truth binary mask for the same class (batch_size, height, width).\n",
    "        :return: Dice loss for the class.\n",
    "        \"\"\"\n",
    "        intersection = torch.sum(pred * gt)\n",
    "        epsilon = 1e-8  # Small value to avoid division by zero\n",
    "        union = pred.sum() + gt.sum() + epsilon\n",
    "        return 1 - (2 * intersection / union)\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, gt: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the average Dice loss across all classes.\n",
    "        :param pred: Predicted probabilities (batch_size, num_classes, height, width).\n",
    "        :param gt: Ground truth class indices (batch_size, height, width).\n",
    "        :return: Average Dice loss across all classes.\n",
    "        \"\"\"\n",
    "        num_classes = pred.shape[1]  # Number of classes\n",
    "        dice_loss = 0.0\n",
    "\n",
    "        for cl in range(num_classes):\n",
    "            pred_cl = pred[:, cl, :, :]  # Predicted probabilities for class `cl`\n",
    "            gt_cl = (gt == cl).float()  # Binary mask for class `cl`\n",
    "            dice_loss += self.dice_loss(pred_cl, gt_cl)\n",
    "\n",
    "        return dice_loss / num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa3f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def to_device(batch, device):\n",
    "    \"\"\"Helper function to move data to the correct device.\"\"\"\n",
    "    images, labels = batch\n",
    "    return images.to(device), labels.to(device)\n",
    "\n",
    "def fit(\n",
    "    epochs: int, \n",
    "    optimizer_class: torch.optim.Optimizer, \n",
    "    model: nn.Module, \n",
    "    learning_rate: float, \n",
    "    train_loader: torch.utils.data.DataLoader, \n",
    "    val_loader: torch.utils.data.DataLoader, \n",
    "    lr_scheduler_class: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "    model_name=\"best_model.pt\", \n",
    "    patience=3, \n",
    "    factor=0.5, \n",
    "    device='cpu', \n",
    "    **kwargs\n",
    "):\n",
    "    model = model.to(device)  # Move model to the specified device (CPU or GPU)\n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    scheduler = None\n",
    "    if lr_scheduler_class is not None:\n",
    "        scheduler = lr_scheduler_class(optimizer, **kwargs)\n",
    "    \n",
    "    # Track history, best loss, and patience for early stopping\n",
    "    history = []\n",
    "    min_val_loss = float('inf')\n",
    "    old_lr = learning_rate\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for num, batch in enumerate(train_loader):\n",
    "            # Move batch to device (CPU in this case)\n",
    "            images, labels = to_device(batch, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            print(f\"Batch [{num + 1}/{len(train_loader)}]: Loss = {loss.item()}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        result = model.evaluate(val_loader)\n",
    "        result[\"train_loss\"] = sum(train_losses) / len(train_losses)\n",
    "        model.epoch_end_val(epoch, result)  # Logging validation result\n",
    "        history.append(result)\n",
    "\n",
    "        # Update Learning Rate Scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(result[\"val_loss\"])  # Update the scheduler with the current validation loss\n",
    "            if optimizer.param_groups[0][\"lr\"] != old_lr:\n",
    "                old_lr = optimizer.param_groups[0][\"lr\"]\n",
    "                print(f\"Updated learning rate to {old_lr:.6f}\")\n",
    "        \n",
    "        # Save Best Model and Early Stopping Logic\n",
    "        if result[\"val_loss\"] < min_val_loss:\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f\"Validation loss improved: {min_val_loss:.4f} -> {result['val_loss']:.4f}\")\n",
    "            min_val_loss = result[\"val_loss\"]\n",
    "            epochs_without_improvement = 0  # Reset patience counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Validation loss did not improve for {epochs_without_improvement} epochs.\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epochs_without_improvement} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba5a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch):\n",
    "       \n",
    "        self.train()\n",
    "        images, target_mask = batch\n",
    "        prediction_mask = self(images)\n",
    "        train_loss = self.loss(target_mask, prediction_mask)\n",
    "        return train_loss  \n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            images, target_mask = batch\n",
    "            prediction_mask = self(images)\n",
    "            val_loss = self.loss(target_mask, prediction_mask)\n",
    "            return {\"val_loss\": val_loss} \n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean().item()\n",
    "\n",
    "        return {\"val_loss\": epoch_loss} \n",
    "    \n",
    "    def evaluate(self, dl):\n",
    "        outputs = [self.validation_step(batch) for batch in dl]\n",
    "        return self.validation_epoch_end(outputs)\n",
    "\n",
    "    def epoch_end_val(self, epoch, results):\n",
    "        \n",
    "\n",
    "        print(\n",
    "            f\"Epoch:[{epoch}]: |validation loss: {results['val_loss']}|\"\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de90191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "   \n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutions with different dilation rates\n",
    "        self.aconv1 = self._create_conv(in_ch, 256, dilation=1)\n",
    "        self.aconv2 = self._create_conv(in_ch, 256, dilation=6)\n",
    "        self.aconv3 = self._create_conv(in_ch, 256, dilation=12)\n",
    "        self.aconv4 = self._create_conv(in_ch, 256, dilation=18)\n",
    "        self.aconv5 = self._create_conv(in_ch, 256, dilation=24)\n",
    "\n",
    "        # Batch normalization for concatenated feature maps\n",
    "        self.bn = nn.BatchNorm2d(256 * 5)\n",
    "\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Final convolution to reduce feature maps to output channels\n",
    "        self.pred_conv = nn.Conv2d(256 * 5, out_ch, 1, padding=0)\n",
    "\n",
    "    def _create_conv(self, in_ch, out_ch, dilation):\n",
    "        \n",
    "        return nn.Conv2d(in_ch, out_ch, 3, dilation=dilation, padding=dilation)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # Apply convolutions followed by ReLU activation\n",
    "        out1 = self.relu(self.aconv1(x))\n",
    "        out2 = self.relu(self.aconv2(x))\n",
    "        out3 = self.relu(self.aconv3(x))\n",
    "        out4 = self.relu(self.aconv4(x))\n",
    "        out5 = self.relu(self.aconv5(x))\n",
    "\n",
    "        # Concatenate the outputs along the channel dimension\n",
    "        cat = torch.cat((out1, out2, out3, out4, out5), dim=1)\n",
    "\n",
    "        # Apply batch normalization\n",
    "        out = self.bn(cat)\n",
    "\n",
    "        # Apply the final convolution to predict output\n",
    "        pred = self.pred_conv(out)\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd2ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class DeepLabv3(nn.Module):\n",
    "    def __init__(self, loss, final_softmax=False, num_classes=2, device='cpu'):\n",
    "        super(DeepLabv3, self).__init__()\n",
    "        # Load ResNet-101 backbone without the fully connected layers\n",
    "        backbone = torchvision.models.resnet101(weights=torchvision.models.ResNet101_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-3])  # Remove FC and avgpool layers\n",
    "\n",
    "        # ASPP (Atrous Spatial Pyramid Pooling) module\n",
    "        self.aspp = ASPP(1024, num_classes)  # Input channels = 1024 (from ResNet-101), Output channels = num_classes\n",
    "        self.up = nn.Upsample(scale_factor=16, mode=\"bilinear\", align_corners=True)  # Upsample to original image size\n",
    "\n",
    "        # Optional Softmax layer for final output\n",
    "        self.sm = nn.Softmax2d() if final_softmax else nn.Identity()  \n",
    "        \n",
    "        self.loss = loss  # Loss function\n",
    "        self.device = device  # Device (CPU or GPU)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # Unpack batch and move to device\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(self.device), labels.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, val_loader):\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch in val_loader:\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self(images)\n",
    "                loss = self.loss(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return {\"val_loss\": val_loss / len(val_loader)}\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the backbone\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Apply ASPP (Atrous Spatial Pyramid Pooling)\n",
    "        x = self.aspp(x)\n",
    "\n",
    "        # Optional Softmax for classification\n",
    "        x = self.sm(x)\n",
    "\n",
    "        # Upsample to the original resolution\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "    def epoch_end_val(self, epoch, result):\n",
    "        # Example: Printing the validation loss\n",
    "        print(f\"Epoch {epoch+1}: Validation Loss = {result['val_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22a723e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ELU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x  \n",
    "    \n",
    "class UnetResNet32(nn.Module, ModelBase):\n",
    "    def __init__(self, loss, final_softmax: bool = True, num_classes=2, train_backbone: bool = True):  \n",
    "        super().__init__()\n",
    "\n",
    "        basemodel = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.basemodel = nn.Sequential(*list(basemodel.children())[:-2])\n",
    "\n",
    "        for param in self.basemodel.parameters():\n",
    "            param.requires_grad = train_backbone\n",
    "\n",
    "        self._NUM_CLASSES = num_classes\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Instantiate hooks to keep the outputs of intermediate layers\n",
    "        self.outputs = {}\n",
    "        for i, layer in enumerate(list(self.basemodel.children())):\n",
    "            layer.register_forward_hook(self.save_output)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2) \n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.up5 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "\n",
    "        self.up_conv4 = DoubleConv(256 + 256, 256)\n",
    "        self.up_conv3 = DoubleConv(128 + 128, 128)\n",
    "        self.up_conv2 = DoubleConv(64 + 64, 64)\n",
    "        self.up_conv1 = DoubleConv(32 + 64, 64)\n",
    "\n",
    "        self.outc3 = nn.Conv2d(128, self._NUM_CLASSES, kernel_size=1)\n",
    "        self.outc2 = nn.Conv2d(64, self._NUM_CLASSES, kernel_size=1)\n",
    "        self.outc1 = nn.Conv2d(64, self._NUM_CLASSES, kernel_size=1)  \n",
    "        \n",
    "        if final_softmax:\n",
    "            self.sm = nn.Softmax2d()\n",
    "        else:\n",
    "            self.sm = nn.Identity()\n",
    "\n",
    "        self.loss = loss \n",
    "    \n",
    "    def save_output(self, module, input, output):\n",
    "        self.outputs[module] = output\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.loss(outputs, labels)\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "            for batch in val_loader:\n",
    "                images, labels = batch\n",
    "                outputs = self(images)\n",
    "                loss = self.loss(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        return {\"val_loss\": val_loss / len(val_loader)}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.basemodel(x)\n",
    "\n",
    "        x = self.up2(self.outputs[list(self.basemodel.children())[-1]])\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-2]]), dim=1)\n",
    "        x = self.up_conv4(x)  \n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-3]]), dim=1)\n",
    "        x = self.up_conv3(x)\n",
    "        output_first_map = nn.Upsample(scale_factor=8, mode=\"bilinear\")(self.outc3(x))\n",
    "\n",
    "        x = self.up4(x)\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-4]]), dim=1)\n",
    "        x = self.up_conv2(x)\n",
    "        intermediate_output_second = nn.Upsample(scale_factor=4, mode=\"bilinear\")(self.outc2(x))\n",
    "        output_second_map = torch.add(output_first_map, intermediate_output_second)\n",
    "\n",
    "        x = self.up5(x)\n",
    "        x = torch.cat((x, self.outputs[list(self.basemodel.children())[-6]]), dim=1)\n",
    "        x = self.up_conv1(x)\n",
    "        intermediate_output_third = nn.Upsample(scale_factor=2, mode=\"bilinear\")(self.outc1(x))\n",
    "        final_output = torch.add(output_second_map, intermediate_output_third)\n",
    "        final_output = self.sm(final_output)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53c90277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader:\n",
    "    def __init__(self, dl):\n",
    "        self.dl = dl\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6399b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "transforms = [\n",
    "    tt.RandomAdjustSharpness(0.3),\n",
    "    tt.RandomAutocontrast(),\n",
    "    tt.RandomGrayscale(),\n",
    "    tt.RandomApply([tt.ColorJitter()])\n",
    "]\n",
    "\n",
    "# Create the dataset\n",
    "ds = Humans(img_files, mask_files, 512, transforms)\n",
    "\n",
    "# Correctly split the dataset ensuring the total sum equals the dataset length\n",
    "total_length = len(ds)\n",
    "train_length = int(0.88 * total_length)\n",
    "val_length = total_length - train_length  # Adjust the remaining for validation\n",
    "ds_train, ds_val = random_split(ds, [train_length, val_length])\n",
    "\n",
    "# Create data loaders for train and validation sets\n",
    "train_loader = DeviceDataLoader(DataLoader(ds_train, num_workers=0, batch_size=8, shuffle=True))\n",
    "val_loader = DeviceDataLoader(DataLoader(ds_val, num_workers=0, batch_size=8, shuffle=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f79a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = SegLoss()\n",
    "\n",
    "# Initialize models\n",
    "model = DeepLabv3(loss)\n",
    "model2 = UnetResNet32(loss)\n",
    "\n",
    "# Force the models to use CPU only\n",
    "device = \"cpu\"  # Set to CPU explicitly\n",
    "\n",
    "# Move models to CPU\n",
    "model = model.to(device)\n",
    "model2 = model2.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72b3eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"\n",
    "    Move data to the specified device.\n",
    "    \"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [x.to(device) for x in data]\n",
    "    elif isinstance(data, dict):  # Handle dictionary-based batches\n",
    "        return {key: value.to(device) for key, value in data.items()}\n",
    "    return data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90914722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Batch [1/131]: Loss = -6.93204927444458\n",
      "Batch [2/131]: Loss = -4.461627960205078\n",
      "Batch [3/131]: Loss = -6.909221172332764\n",
      "Batch [4/131]: Loss = -7.223175048828125\n",
      "Batch [5/131]: Loss = -7.133052825927734\n",
      "Batch [6/131]: Loss = -7.143922328948975\n",
      "Batch [7/131]: Loss = -7.1514997482299805\n",
      "Batch [8/131]: Loss = -7.165430545806885\n",
      "Batch [9/131]: Loss = -7.520780563354492\n",
      "Batch [10/131]: Loss = -7.933130741119385\n",
      "Batch [11/131]: Loss = -8.395240783691406\n",
      "Batch [12/131]: Loss = -6.524741172790527\n",
      "Batch [13/131]: Loss = -7.9664716720581055\n",
      "Batch [14/131]: Loss = -9.38386344909668\n",
      "Batch [15/131]: Loss = -8.778350830078125\n",
      "Batch [16/131]: Loss = -9.895115852355957\n",
      "Batch [17/131]: Loss = -10.111024856567383\n",
      "Batch [18/131]: Loss = -12.536389350891113\n",
      "Batch [19/131]: Loss = -14.117339134216309\n",
      "Batch [20/131]: Loss = -18.30915641784668\n",
      "Batch [21/131]: Loss = -24.31777000427246\n",
      "Batch [22/131]: Loss = -19.19626808166504\n",
      "Batch [23/131]: Loss = -26.284547805786133\n",
      "Batch [24/131]: Loss = -102.91845703125\n",
      "Batch [25/131]: Loss = 0.26624393463134766\n",
      "Batch [26/131]: Loss = 9.434165954589844\n",
      "Batch [27/131]: Loss = -27.50101661682129\n",
      "Batch [28/131]: Loss = 32.82853698730469\n",
      "Batch [29/131]: Loss = 10.404346466064453\n",
      "Batch [30/131]: Loss = 6.557878017425537\n",
      "Batch [31/131]: Loss = -0.04601764678955078\n",
      "Batch [32/131]: Loss = -2.4130301475524902\n",
      "Batch [33/131]: Loss = 1.1024494171142578\n",
      "Batch [34/131]: Loss = -3.0249788761138916\n",
      "Batch [35/131]: Loss = -7.349923610687256\n",
      "Batch [36/131]: Loss = -3.6572041511535645\n",
      "Batch [37/131]: Loss = -10.987140655517578\n",
      "Batch [38/131]: Loss = -8.044690132141113\n",
      "Batch [39/131]: Loss = -8.822518348693848\n",
      "Batch [40/131]: Loss = -10.579352378845215\n",
      "Batch [41/131]: Loss = -9.781225204467773\n",
      "Batch [42/131]: Loss = -10.607749938964844\n",
      "Batch [43/131]: Loss = -13.174556732177734\n",
      "Batch [44/131]: Loss = -15.45216178894043\n",
      "Batch [45/131]: Loss = -9.94527816772461\n",
      "Batch [46/131]: Loss = -17.573604583740234\n",
      "Batch [47/131]: Loss = -13.984556198120117\n",
      "Batch [48/131]: Loss = -17.601669311523438\n",
      "Batch [49/131]: Loss = 45.80853271484375\n",
      "Batch [50/131]: Loss = 13.319382667541504\n",
      "Batch [51/131]: Loss = -6.596254348754883\n",
      "Batch [52/131]: Loss = -9.728321075439453\n",
      "Batch [53/131]: Loss = -11.383729934692383\n",
      "Batch [54/131]: Loss = -18.5952091217041\n",
      "Batch [55/131]: Loss = -18.206275939941406\n",
      "Batch [56/131]: Loss = -38.5203857421875\n",
      "Batch [57/131]: Loss = -20.822633743286133\n",
      "Batch [58/131]: Loss = -35.48400115966797\n",
      "Batch [59/131]: Loss = -150.3603973388672\n",
      "Batch [60/131]: Loss = 40.965274810791016\n",
      "Batch [61/131]: Loss = 17.85603141784668\n",
      "Batch [62/131]: Loss = 9.279190063476562\n",
      "Batch [63/131]: Loss = 4.728019714355469\n",
      "Batch [64/131]: Loss = 3.6993556022644043\n",
      "Batch [65/131]: Loss = 2.2077698707580566\n",
      "Batch [66/131]: Loss = 1.3156867027282715\n",
      "Batch [67/131]: Loss = -2.3947954177856445\n",
      "Batch [68/131]: Loss = -3.942077875137329\n",
      "Batch [69/131]: Loss = -5.284670829772949\n",
      "Batch [70/131]: Loss = -4.331487655639648\n",
      "Batch [71/131]: Loss = -3.634489059448242\n",
      "Batch [72/131]: Loss = -3.7659435272216797\n",
      "Batch [73/131]: Loss = -4.851367950439453\n",
      "Batch [74/131]: Loss = -6.004315376281738\n",
      "Batch [75/131]: Loss = -5.925532341003418\n",
      "Batch [76/131]: Loss = -6.387473106384277\n",
      "Batch [77/131]: Loss = -5.823798656463623\n",
      "Batch [78/131]: Loss = -6.297018051147461\n",
      "Batch [79/131]: Loss = -7.228307723999023\n",
      "Batch [80/131]: Loss = -7.413854598999023\n",
      "Batch [81/131]: Loss = -7.3563055992126465\n",
      "Batch [82/131]: Loss = -7.740880966186523\n",
      "Batch [83/131]: Loss = -8.440598487854004\n",
      "Batch [84/131]: Loss = -8.392036437988281\n",
      "Batch [85/131]: Loss = -8.833191871643066\n",
      "Batch [86/131]: Loss = -10.524893760681152\n",
      "Batch [87/131]: Loss = -12.461840629577637\n",
      "Batch [88/131]: Loss = -15.36131763458252\n",
      "Batch [89/131]: Loss = -44.1049690246582\n",
      "Batch [90/131]: Loss = -161.3360137939453\n",
      "Batch [91/131]: Loss = 35.311790466308594\n",
      "Batch [92/131]: Loss = 21.698339462280273\n",
      "Batch [93/131]: Loss = 1.46435546875\n",
      "Batch [94/131]: Loss = -0.9401293396949768\n",
      "Batch [95/131]: Loss = -1.7449758052825928\n",
      "Batch [96/131]: Loss = -1.9363676309585571\n",
      "Batch [97/131]: Loss = -4.948010444641113\n",
      "Batch [98/131]: Loss = -5.830509662628174\n",
      "Batch [99/131]: Loss = -8.891165733337402\n",
      "Batch [100/131]: Loss = -9.650599479675293\n",
      "Batch [101/131]: Loss = -73.99220275878906\n",
      "Batch [102/131]: Loss = 3.277506113052368\n",
      "Batch [103/131]: Loss = -6.282461166381836\n",
      "Batch [104/131]: Loss = -8.198173522949219\n",
      "Batch [105/131]: Loss = -6.942640781402588\n",
      "Batch [106/131]: Loss = -10.735973358154297\n",
      "Batch [107/131]: Loss = -8.859580039978027\n",
      "Batch [108/131]: Loss = -11.388833999633789\n",
      "Batch [109/131]: Loss = -17.66825294494629\n",
      "Batch [110/131]: Loss = 397.16082763671875\n",
      "Batch [111/131]: Loss = -2.158165454864502\n",
      "Batch [112/131]: Loss = -4.991343021392822\n",
      "Batch [113/131]: Loss = -5.227084159851074\n",
      "Batch [114/131]: Loss = -6.063183784484863\n",
      "Batch [115/131]: Loss = -6.651573657989502\n",
      "Batch [116/131]: Loss = -8.009492874145508\n",
      "Batch [117/131]: Loss = -8.43689250946045\n",
      "Batch [118/131]: Loss = -9.148347854614258\n",
      "Batch [119/131]: Loss = -8.743911743164062\n",
      "Batch [120/131]: Loss = -9.503633499145508\n",
      "Batch [121/131]: Loss = -10.599157333374023\n",
      "Batch [122/131]: Loss = -11.280073165893555\n",
      "Batch [123/131]: Loss = -13.909817695617676\n",
      "Batch [124/131]: Loss = -19.67040252685547\n",
      "Batch [125/131]: Loss = -19.526447296142578\n",
      "Batch [126/131]: Loss = -34.91062545776367\n",
      "Batch [127/131]: Loss = -325.7208251953125\n",
      "Batch [128/131]: Loss = 6.592634677886963\n",
      "Batch [129/131]: Loss = 4.481050491333008\n",
      "Batch [130/131]: Loss = -1.639636516571045\n",
      "Batch [131/131]: Loss = -2.545762062072754\n",
      "Epoch 1: Validation Loss = -6.924365546968248\n",
      "Validation loss improved: inf -> -6.9244\n",
      "Epoch 2/20\n",
      "Batch [1/131]: Loss = -3.4000558853149414\n",
      "Batch [2/131]: Loss = -5.779705047607422\n",
      "Batch [3/131]: Loss = -6.083995819091797\n",
      "Batch [4/131]: Loss = -7.915346145629883\n",
      "Batch [5/131]: Loss = -9.334199905395508\n",
      "Batch [6/131]: Loss = -11.456657409667969\n",
      "Batch [7/131]: Loss = 8.908166885375977\n",
      "Batch [8/131]: Loss = -1.0603039264678955\n",
      "Batch [9/131]: Loss = -0.7653946876525879\n",
      "Batch [10/131]: Loss = -1.3549857139587402\n",
      "Batch [11/131]: Loss = -4.211858749389648\n",
      "Batch [12/131]: Loss = -8.716023445129395\n",
      "Batch [13/131]: Loss = -16.118122100830078\n",
      "Batch [14/131]: Loss = -30.6639404296875\n",
      "Batch [15/131]: Loss = 2.0961685180664062\n",
      "Batch [16/131]: Loss = -1.2923884391784668\n",
      "Batch [17/131]: Loss = -3.3165481090545654\n",
      "Batch [18/131]: Loss = -5.334203720092773\n",
      "Batch [19/131]: Loss = -6.304534912109375\n",
      "Batch [20/131]: Loss = -6.100723743438721\n",
      "Batch [21/131]: Loss = -6.571310520172119\n",
      "Batch [22/131]: Loss = -8.80941390991211\n",
      "Batch [23/131]: Loss = -9.131197929382324\n",
      "Batch [24/131]: Loss = -11.840180397033691\n",
      "Batch [25/131]: Loss = -16.965621948242188\n",
      "Batch [26/131]: Loss = -69.3281478881836\n",
      "Batch [27/131]: Loss = 49.18253707885742\n",
      "Batch [28/131]: Loss = 0.8715834617614746\n",
      "Batch [29/131]: Loss = 0.6153435707092285\n",
      "Batch [30/131]: Loss = -0.9987916946411133\n",
      "Batch [31/131]: Loss = -4.849003791809082\n",
      "Batch [32/131]: Loss = -5.4959397315979\n",
      "Batch [33/131]: Loss = -5.057136535644531\n",
      "Batch [34/131]: Loss = -6.627847671508789\n",
      "Batch [35/131]: Loss = -8.674336433410645\n",
      "Batch [36/131]: Loss = -9.866968154907227\n",
      "Batch [37/131]: Loss = -10.893769264221191\n",
      "Batch [38/131]: Loss = -16.97110366821289\n",
      "Batch [39/131]: Loss = -60.30561447143555\n",
      "Batch [40/131]: Loss = 6.244795322418213\n",
      "Batch [41/131]: Loss = -1.7204034328460693\n",
      "Batch [42/131]: Loss = -3.0050508975982666\n",
      "Batch [43/131]: Loss = -6.33275032043457\n",
      "Batch [44/131]: Loss = -6.381473064422607\n",
      "Batch [45/131]: Loss = -6.916623115539551\n",
      "Batch [46/131]: Loss = -8.241241455078125\n",
      "Batch [47/131]: Loss = -10.584953308105469\n",
      "Batch [48/131]: Loss = -11.755728721618652\n",
      "Batch [49/131]: Loss = 19.160810470581055\n",
      "Batch [50/131]: Loss = 7.994349002838135\n",
      "Batch [51/131]: Loss = -3.627025842666626\n",
      "Batch [52/131]: Loss = -3.2202372550964355\n",
      "Batch [53/131]: Loss = -5.33258056640625\n",
      "Batch [54/131]: Loss = -5.140156269073486\n",
      "Batch [55/131]: Loss = -5.7493205070495605\n",
      "Batch [56/131]: Loss = -7.4443511962890625\n",
      "Batch [57/131]: Loss = -8.593192100524902\n",
      "Batch [58/131]: Loss = -11.880411148071289\n",
      "Batch [59/131]: Loss = -29.034717559814453\n",
      "Batch [60/131]: Loss = 46.68193817138672\n",
      "Batch [61/131]: Loss = -125.78575134277344\n",
      "Batch [62/131]: Loss = 35.965423583984375\n",
      "Batch [63/131]: Loss = 9.353950500488281\n",
      "Batch [64/131]: Loss = 3.2680797576904297\n",
      "Batch [65/131]: Loss = 0.25867247581481934\n",
      "Batch [66/131]: Loss = -3.6499433517456055\n",
      "Batch [67/131]: Loss = -6.582454681396484\n",
      "Batch [68/131]: Loss = -7.9121856689453125\n",
      "Batch [69/131]: Loss = -16.94935417175293\n",
      "Batch [70/131]: Loss = 1.298879623413086\n",
      "Batch [71/131]: Loss = -2.8946146965026855\n",
      "Batch [72/131]: Loss = -3.4164037704467773\n",
      "Batch [73/131]: Loss = -3.9310882091522217\n",
      "Batch [74/131]: Loss = -7.082195281982422\n",
      "Batch [75/131]: Loss = -9.60911750793457\n",
      "Batch [76/131]: Loss = -40.808135986328125\n",
      "Batch [77/131]: Loss = 17.85006332397461\n",
      "Batch [78/131]: Loss = -0.5035891532897949\n",
      "Batch [79/131]: Loss = -1.7554740905761719\n",
      "Batch [80/131]: Loss = -3.992642402648926\n",
      "Batch [81/131]: Loss = -4.030290126800537\n",
      "Batch [82/131]: Loss = -5.189966201782227\n",
      "Batch [83/131]: Loss = -6.917851448059082\n",
      "Batch [84/131]: Loss = -7.710216999053955\n",
      "Batch [85/131]: Loss = -9.135746002197266\n",
      "Batch [86/131]: Loss = -25.345985412597656\n",
      "Batch [87/131]: Loss = 8074.44970703125\n",
      "Batch [88/131]: Loss = 1.502974510192871\n",
      "Batch [89/131]: Loss = -5.155374526977539\n",
      "Batch [90/131]: Loss = 3.3399391174316406\n",
      "Batch [91/131]: Loss = 0.6065540313720703\n",
      "Batch [92/131]: Loss = 0.5851845741271973\n",
      "Batch [93/131]: Loss = -2.8006863594055176\n",
      "Batch [94/131]: Loss = -4.808074951171875\n",
      "Batch [95/131]: Loss = -5.033095836639404\n",
      "Batch [96/131]: Loss = -5.4951629638671875\n",
      "Batch [97/131]: Loss = -7.370615005493164\n",
      "Batch [98/131]: Loss = -9.998268127441406\n",
      "Batch [99/131]: Loss = -19.978551864624023\n",
      "Batch [100/131]: Loss = -0.5145523548126221\n",
      "Batch [101/131]: Loss = 5.211236476898193\n",
      "Batch [102/131]: Loss = 0.912745475769043\n",
      "Batch [103/131]: Loss = -3.83866548538208\n",
      "Batch [104/131]: Loss = -3.376645088195801\n",
      "Batch [105/131]: Loss = -8.14855670928955\n",
      "Batch [106/131]: Loss = -5.573043346405029\n",
      "Batch [107/131]: Loss = -4.120700359344482\n",
      "Batch [108/131]: Loss = -7.125035285949707\n",
      "Batch [109/131]: Loss = -9.17957878112793\n",
      "Batch [110/131]: Loss = -7.930050849914551\n",
      "Batch [111/131]: Loss = -7.988066673278809\n",
      "Batch [112/131]: Loss = -8.708891868591309\n",
      "Batch [113/131]: Loss = -9.039155960083008\n",
      "Batch [114/131]: Loss = -10.298012733459473\n",
      "Batch [115/131]: Loss = -10.185874938964844\n",
      "Batch [116/131]: Loss = -10.733321189880371\n",
      "Batch [117/131]: Loss = -3.559532642364502\n",
      "Batch [118/131]: Loss = -15.215919494628906\n",
      "Batch [119/131]: Loss = -89.93968200683594\n",
      "Batch [120/131]: Loss = -1.1056183576583862\n",
      "Batch [121/131]: Loss = -6.08298921585083\n",
      "Batch [122/131]: Loss = -7.14613151550293\n",
      "Batch [123/131]: Loss = -7.73200798034668\n",
      "Batch [124/131]: Loss = -7.121223449707031\n",
      "Batch [125/131]: Loss = -9.429766654968262\n",
      "Batch [126/131]: Loss = -85.75423431396484\n",
      "Batch [127/131]: Loss = -0.9690470695495605\n",
      "Batch [128/131]: Loss = -2.4565441608428955\n",
      "Batch [129/131]: Loss = -3.715071201324463\n",
      "Batch [130/131]: Loss = -4.303766250610352\n",
      "Batch [131/131]: Loss = -4.928802967071533\n",
      "Epoch 2: Validation Loss = -5.637617031733195\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 3/20\n",
      "Batch [1/131]: Loss = -5.1051154136657715\n",
      "Batch [2/131]: Loss = -5.93069314956665\n",
      "Batch [3/131]: Loss = -9.634800910949707\n",
      "Batch [4/131]: Loss = -11.693731307983398\n",
      "Batch [5/131]: Loss = -9.996920585632324\n",
      "Batch [6/131]: Loss = -34.33319854736328\n",
      "Batch [7/131]: Loss = 25.840713500976562\n",
      "Batch [8/131]: Loss = -2.806281566619873\n",
      "Batch [9/131]: Loss = -0.3819093704223633\n",
      "Batch [10/131]: Loss = -5.699448585510254\n",
      "Batch [11/131]: Loss = -8.1979398727417\n",
      "Batch [12/131]: Loss = -8.785889625549316\n",
      "Batch [13/131]: Loss = -10.927216529846191\n",
      "Batch [14/131]: Loss = -10.489245414733887\n",
      "Batch [15/131]: Loss = -11.781972885131836\n",
      "Batch [16/131]: Loss = -12.636457443237305\n",
      "Batch [17/131]: Loss = -19.63753318786621\n",
      "Batch [18/131]: Loss = -79.68470764160156\n",
      "Batch [19/131]: Loss = 5.26547908782959\n",
      "Batch [20/131]: Loss = 1.783626675605774\n",
      "Batch [21/131]: Loss = -3.500645399093628\n",
      "Batch [22/131]: Loss = -5.253178596496582\n",
      "Batch [23/131]: Loss = -5.130213737487793\n",
      "Batch [24/131]: Loss = -6.197055816650391\n",
      "Batch [25/131]: Loss = -10.253571510314941\n",
      "Batch [26/131]: Loss = -14.402593612670898\n",
      "Batch [27/131]: Loss = 71.83289337158203\n",
      "Batch [28/131]: Loss = 4.516862869262695\n",
      "Batch [29/131]: Loss = 1.0589097738265991\n",
      "Batch [30/131]: Loss = -4.58592414855957\n",
      "Batch [31/131]: Loss = -6.254445552825928\n",
      "Batch [32/131]: Loss = -5.52664041519165\n",
      "Batch [33/131]: Loss = -4.9210309982299805\n",
      "Batch [34/131]: Loss = -6.388839244842529\n",
      "Batch [35/131]: Loss = -7.909913063049316\n",
      "Batch [36/131]: Loss = -15.742752075195312\n",
      "Batch [37/131]: Loss = -27.46261978149414\n",
      "Batch [38/131]: Loss = 13.357081413269043\n",
      "Batch [39/131]: Loss = 2.81919002532959\n",
      "Batch [40/131]: Loss = 1.7896270751953125\n",
      "Batch [41/131]: Loss = -4.788670539855957\n",
      "Batch [42/131]: Loss = -5.587735176086426\n",
      "Batch [43/131]: Loss = -8.256484031677246\n",
      "Batch [44/131]: Loss = -9.926228523254395\n",
      "Batch [45/131]: Loss = -10.637170791625977\n",
      "Batch [46/131]: Loss = -10.935287475585938\n",
      "Batch [47/131]: Loss = -35.294654846191406\n",
      "Batch [48/131]: Loss = 5.359274387359619\n",
      "Batch [49/131]: Loss = -2.209624767303467\n",
      "Batch [50/131]: Loss = -4.880884170532227\n",
      "Batch [51/131]: Loss = -4.152934551239014\n",
      "Batch [52/131]: Loss = -6.619919776916504\n",
      "Batch [53/131]: Loss = -5.568478107452393\n",
      "Batch [54/131]: Loss = -15.091813087463379\n",
      "Batch [55/131]: Loss = -13.814140319824219\n",
      "Batch [56/131]: Loss = -31.968996047973633\n",
      "Batch [57/131]: Loss = -24.73345184326172\n",
      "Batch [58/131]: Loss = -961.3557739257812\n",
      "Batch [59/131]: Loss = 5.002324104309082\n",
      "Batch [60/131]: Loss = -4.17854118347168\n",
      "Batch [61/131]: Loss = -4.008737564086914\n",
      "Batch [62/131]: Loss = -6.61185884475708\n",
      "Batch [63/131]: Loss = -6.105344772338867\n",
      "Batch [64/131]: Loss = -7.476628303527832\n",
      "Batch [65/131]: Loss = -9.374738693237305\n",
      "Batch [66/131]: Loss = -9.135140419006348\n",
      "Batch [67/131]: Loss = -10.45141315460205\n",
      "Batch [68/131]: Loss = -13.365815162658691\n",
      "Batch [69/131]: Loss = -18.239233016967773\n",
      "Batch [70/131]: Loss = 380.6214904785156\n",
      "Batch [71/131]: Loss = 2.4519691467285156\n",
      "Batch [72/131]: Loss = -2.2553820610046387\n",
      "Batch [73/131]: Loss = -2.9112372398376465\n",
      "Batch [74/131]: Loss = -4.187327861785889\n",
      "Batch [75/131]: Loss = -5.374527454376221\n",
      "Batch [76/131]: Loss = -6.833587646484375\n",
      "Batch [77/131]: Loss = -8.64271068572998\n",
      "Batch [78/131]: Loss = -8.879032135009766\n",
      "Batch [79/131]: Loss = -15.077705383300781\n",
      "Batch [80/131]: Loss = 2.184274673461914\n",
      "Batch [81/131]: Loss = -3.586129665374756\n",
      "Batch [82/131]: Loss = -4.547144889831543\n",
      "Batch [83/131]: Loss = -6.200060844421387\n",
      "Batch [84/131]: Loss = -6.746038913726807\n",
      "Batch [85/131]: Loss = -7.415430068969727\n",
      "Batch [86/131]: Loss = -7.086763381958008\n",
      "Batch [87/131]: Loss = -7.435020923614502\n",
      "Batch [88/131]: Loss = -8.255906105041504\n",
      "Batch [89/131]: Loss = -7.817946434020996\n",
      "Batch [90/131]: Loss = -10.363826751708984\n",
      "Batch [91/131]: Loss = -14.543669700622559\n",
      "Batch [92/131]: Loss = 4.286313533782959\n",
      "Batch [93/131]: Loss = -3.8896117210388184\n",
      "Batch [94/131]: Loss = -4.3537821769714355\n",
      "Batch [95/131]: Loss = -5.751007556915283\n",
      "Batch [96/131]: Loss = -13.688158988952637\n",
      "Batch [97/131]: Loss = -14.101842880249023\n",
      "Batch [98/131]: Loss = -69.19493103027344\n",
      "Batch [99/131]: Loss = 32.36355209350586\n",
      "Batch [100/131]: Loss = 0.7991487979888916\n",
      "Batch [101/131]: Loss = -2.7225542068481445\n",
      "Batch [102/131]: Loss = -4.627821922302246\n",
      "Batch [103/131]: Loss = -6.409844398498535\n",
      "Batch [104/131]: Loss = -6.855731010437012\n",
      "Batch [105/131]: Loss = -6.496190071105957\n",
      "Batch [106/131]: Loss = -10.10873794555664\n",
      "Batch [107/131]: Loss = -10.443385124206543\n",
      "Batch [108/131]: Loss = -13.416358947753906\n",
      "Batch [109/131]: Loss = 114.5254898071289\n",
      "Batch [110/131]: Loss = 35.25259017944336\n",
      "Batch [111/131]: Loss = -11.012166976928711\n",
      "Batch [112/131]: Loss = -10.359111785888672\n",
      "Batch [113/131]: Loss = -19.32553482055664\n",
      "Batch [114/131]: Loss = 71.42911529541016\n",
      "Batch [115/131]: Loss = -0.9972963333129883\n",
      "Batch [116/131]: Loss = -4.749835014343262\n",
      "Batch [117/131]: Loss = 16.25857162475586\n",
      "Batch [118/131]: Loss = -12.597599983215332\n",
      "Batch [119/131]: Loss = -2.3405046463012695\n",
      "Batch [120/131]: Loss = -10.022490501403809\n",
      "Batch [121/131]: Loss = -11.657712936401367\n",
      "Batch [122/131]: Loss = -21.587234497070312\n",
      "Batch [123/131]: Loss = -21.19901466369629\n",
      "Batch [124/131]: Loss = 40.469730377197266\n",
      "Batch [125/131]: Loss = -0.9734244346618652\n",
      "Batch [126/131]: Loss = -2.7265491485595703\n",
      "Batch [127/131]: Loss = -2.8394393920898438\n",
      "Batch [128/131]: Loss = -4.41974401473999\n",
      "Batch [129/131]: Loss = -9.65121841430664\n",
      "Batch [130/131]: Loss = -7.7012834548950195\n",
      "Batch [131/131]: Loss = -6.928295612335205\n",
      "Epoch 3: Validation Loss = -7.36779605017768\n",
      "Validation loss improved: -6.9244 -> -7.3678\n",
      "Epoch 4/20\n",
      "Batch [1/131]: Loss = -7.292318344116211\n",
      "Batch [2/131]: Loss = -8.083744049072266\n",
      "Batch [3/131]: Loss = -6.803593635559082\n",
      "Batch [4/131]: Loss = -11.234172821044922\n",
      "Batch [5/131]: Loss = -13.412862777709961\n",
      "Batch [6/131]: Loss = -12.219078063964844\n",
      "Batch [7/131]: Loss = -9.522933959960938\n",
      "Batch [8/131]: Loss = -10.66618537902832\n",
      "Batch [9/131]: Loss = -24.584291458129883\n",
      "Batch [10/131]: Loss = -9.834488868713379\n",
      "Batch [11/131]: Loss = -50.68428039550781\n",
      "Batch [12/131]: Loss = -77.13771057128906\n",
      "Batch [13/131]: Loss = 1.2527031898498535\n",
      "Batch [14/131]: Loss = 0.9812049865722656\n",
      "Batch [15/131]: Loss = -2.2099218368530273\n",
      "Batch [16/131]: Loss = -2.782219886779785\n",
      "Batch [17/131]: Loss = -5.682313919067383\n",
      "Batch [18/131]: Loss = -7.937563896179199\n",
      "Batch [19/131]: Loss = -7.996875286102295\n",
      "Batch [20/131]: Loss = -8.104827880859375\n",
      "Batch [21/131]: Loss = -8.115339279174805\n",
      "Batch [22/131]: Loss = -8.563444137573242\n",
      "Batch [23/131]: Loss = -7.081516742706299\n",
      "Batch [24/131]: Loss = -9.28034496307373\n",
      "Batch [25/131]: Loss = -10.156142234802246\n",
      "Batch [26/131]: Loss = -9.909381866455078\n",
      "Batch [27/131]: Loss = -11.59803581237793\n",
      "Batch [28/131]: Loss = -12.06114387512207\n",
      "Batch [29/131]: Loss = -12.46446704864502\n",
      "Batch [30/131]: Loss = -18.539743423461914\n",
      "Batch [31/131]: Loss = -28.58272361755371\n",
      "Batch [32/131]: Loss = 322.8780517578125\n",
      "Batch [33/131]: Loss = 6.3728251457214355\n",
      "Batch [34/131]: Loss = 0.04131126403808594\n",
      "Batch [35/131]: Loss = -3.2621254920959473\n",
      "Batch [36/131]: Loss = -3.45819091796875\n",
      "Batch [37/131]: Loss = -4.024711608886719\n",
      "Batch [38/131]: Loss = -5.64028263092041\n",
      "Batch [39/131]: Loss = -7.619362831115723\n",
      "Batch [40/131]: Loss = -11.788641929626465\n",
      "Batch [41/131]: Loss = -38.31187057495117\n",
      "Batch [42/131]: Loss = 22.817289352416992\n",
      "Batch [43/131]: Loss = 2.7980661392211914\n",
      "Batch [44/131]: Loss = 5.3708648681640625\n",
      "Batch [45/131]: Loss = 0.3636901378631592\n",
      "Batch [46/131]: Loss = -3.0397448539733887\n",
      "Batch [47/131]: Loss = 0.6713920831680298\n",
      "Batch [48/131]: Loss = -2.0967984199523926\n",
      "Batch [49/131]: Loss = -3.2255406379699707\n",
      "Batch [50/131]: Loss = -3.141460657119751\n",
      "Batch [51/131]: Loss = -3.4985122680664062\n",
      "Batch [52/131]: Loss = -4.316331386566162\n",
      "Batch [53/131]: Loss = -5.0861382484436035\n",
      "Batch [54/131]: Loss = -6.076395511627197\n",
      "Batch [55/131]: Loss = -6.792647361755371\n",
      "Batch [56/131]: Loss = -6.698437690734863\n",
      "Batch [57/131]: Loss = -6.6578898429870605\n",
      "Batch [58/131]: Loss = -7.649697780609131\n",
      "Batch [59/131]: Loss = -9.349921226501465\n",
      "Batch [60/131]: Loss = -9.119013786315918\n",
      "Batch [61/131]: Loss = -9.75744915008545\n",
      "Batch [62/131]: Loss = -13.542669296264648\n",
      "Batch [63/131]: Loss = -18.224685668945312\n",
      "Batch [64/131]: Loss = -24.135160446166992\n",
      "Batch [65/131]: Loss = -8.172126770019531\n",
      "Batch [66/131]: Loss = 0.45898962020874023\n",
      "Batch [67/131]: Loss = 8.345979690551758\n",
      "Batch [68/131]: Loss = -5.83631706237793\n",
      "Batch [69/131]: Loss = -7.08879280090332\n",
      "Batch [70/131]: Loss = -7.87074089050293\n",
      "Batch [71/131]: Loss = -8.43745231628418\n",
      "Batch [72/131]: Loss = -6.437880039215088\n",
      "Batch [73/131]: Loss = -9.360210418701172\n",
      "Batch [74/131]: Loss = -13.194892883300781\n",
      "Batch [75/131]: Loss = -12.80453872680664\n",
      "Batch [76/131]: Loss = -8.037931442260742\n",
      "Batch [77/131]: Loss = -10.816417694091797\n",
      "Batch [78/131]: Loss = -17.324764251708984\n",
      "Batch [79/131]: Loss = -15.24718952178955\n",
      "Batch [80/131]: Loss = -16.45978355407715\n",
      "Batch [81/131]: Loss = -8.760672569274902\n",
      "Batch [82/131]: Loss = -23.025617599487305\n",
      "Batch [83/131]: Loss = -18.56871795654297\n",
      "Batch [84/131]: Loss = -28.996753692626953\n",
      "Batch [85/131]: Loss = -49.248294830322266\n",
      "Batch [86/131]: Loss = -126.64366912841797\n",
      "Batch [87/131]: Loss = 25.636384963989258\n",
      "Batch [88/131]: Loss = 42.62488555908203\n",
      "Batch [89/131]: Loss = 24.62468719482422\n",
      "Batch [90/131]: Loss = 7.708240509033203\n",
      "Batch [91/131]: Loss = 0.7557311058044434\n",
      "Batch [92/131]: Loss = -4.085125923156738\n",
      "Batch [93/131]: Loss = -3.684025764465332\n",
      "Batch [94/131]: Loss = -6.028219223022461\n",
      "Batch [95/131]: Loss = -6.866915225982666\n",
      "Batch [96/131]: Loss = -7.086308002471924\n",
      "Batch [97/131]: Loss = -6.766110420227051\n",
      "Batch [98/131]: Loss = -7.072237014770508\n",
      "Batch [99/131]: Loss = -7.375829219818115\n",
      "Batch [100/131]: Loss = -7.811993598937988\n",
      "Batch [101/131]: Loss = -7.25088357925415\n",
      "Batch [102/131]: Loss = -6.569412708282471\n",
      "Batch [103/131]: Loss = -7.596604347229004\n",
      "Batch [104/131]: Loss = -5.042973518371582\n",
      "Batch [105/131]: Loss = -8.077738761901855\n",
      "Batch [106/131]: Loss = -9.264174461364746\n",
      "Batch [107/131]: Loss = -10.39813232421875\n",
      "Batch [108/131]: Loss = -12.795539855957031\n",
      "Batch [109/131]: Loss = -13.154438018798828\n",
      "Batch [110/131]: Loss = -19.107595443725586\n",
      "Batch [111/131]: Loss = -77.03194427490234\n",
      "Batch [112/131]: Loss = 18.716890335083008\n",
      "Batch [113/131]: Loss = 4.042365550994873\n",
      "Batch [114/131]: Loss = 0.5497016906738281\n",
      "Batch [115/131]: Loss = -0.7891249656677246\n",
      "Batch [116/131]: Loss = -1.9106683731079102\n",
      "Batch [117/131]: Loss = -2.4067440032958984\n",
      "Batch [118/131]: Loss = -2.6751298904418945\n",
      "Batch [119/131]: Loss = -3.880425453186035\n",
      "Batch [120/131]: Loss = -8.818039894104004\n",
      "Batch [121/131]: Loss = -6.310474395751953\n",
      "Batch [122/131]: Loss = -5.833388805389404\n",
      "Batch [123/131]: Loss = -6.6730852127075195\n",
      "Batch [124/131]: Loss = -5.923635959625244\n",
      "Batch [125/131]: Loss = -10.21327018737793\n",
      "Batch [126/131]: Loss = -10.83730411529541\n",
      "Batch [127/131]: Loss = -8.982335090637207\n",
      "Batch [128/131]: Loss = -12.390897750854492\n",
      "Batch [129/131]: Loss = -13.487855911254883\n",
      "Batch [130/131]: Loss = -30.119892120361328\n",
      "Batch [131/131]: Loss = 71.95240020751953\n",
      "Epoch 4: Validation Loss = -0.29870279630025226\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 5/20\n",
      "Batch [1/131]: Loss = -187.96678161621094\n",
      "Batch [2/131]: Loss = 49.952945709228516\n",
      "Batch [3/131]: Loss = 12.48275375366211\n",
      "Batch [4/131]: Loss = 8.363051414489746\n",
      "Batch [5/131]: Loss = 1.5533370971679688\n",
      "Batch [6/131]: Loss = 2.482729434967041\n",
      "Batch [7/131]: Loss = 0.6136674880981445\n",
      "Batch [8/131]: Loss = -2.3074145317077637\n",
      "Batch [9/131]: Loss = -2.515299081802368\n",
      "Batch [10/131]: Loss = -3.1047511100769043\n",
      "Batch [11/131]: Loss = -3.9149250984191895\n",
      "Batch [12/131]: Loss = -5.298089981079102\n",
      "Batch [13/131]: Loss = -6.022324562072754\n",
      "Batch [14/131]: Loss = -6.830901145935059\n",
      "Batch [15/131]: Loss = -5.6853179931640625\n",
      "Batch [16/131]: Loss = -5.2635345458984375\n",
      "Batch [17/131]: Loss = -7.0111589431762695\n",
      "Batch [18/131]: Loss = -7.221292018890381\n",
      "Batch [19/131]: Loss = -6.945479869842529\n",
      "Batch [20/131]: Loss = -7.203833103179932\n",
      "Batch [21/131]: Loss = -6.435052394866943\n",
      "Batch [22/131]: Loss = -7.684165954589844\n",
      "Batch [23/131]: Loss = -9.239337921142578\n",
      "Batch [24/131]: Loss = -9.520174980163574\n",
      "Batch [25/131]: Loss = -11.676172256469727\n",
      "Batch [26/131]: Loss = -11.584846496582031\n",
      "Batch [27/131]: Loss = -9.393518447875977\n",
      "Batch [28/131]: Loss = -13.770426750183105\n",
      "Batch [29/131]: Loss = -11.198116302490234\n",
      "Batch [30/131]: Loss = -14.72079086303711\n",
      "Batch [31/131]: Loss = -17.73284912109375\n",
      "Batch [32/131]: Loss = -15.146245956420898\n",
      "Batch [33/131]: Loss = -5.8837361335754395\n",
      "Batch [34/131]: Loss = -20.83597183227539\n",
      "Batch [35/131]: Loss = -37.775604248046875\n",
      "Batch [36/131]: Loss = -25.8480167388916\n",
      "Batch [37/131]: Loss = -120.92302703857422\n",
      "Batch [38/131]: Loss = 10.46397590637207\n",
      "Batch [39/131]: Loss = -29.169593811035156\n",
      "Batch [40/131]: Loss = -14.714096069335938\n",
      "Batch [41/131]: Loss = -70.02484893798828\n",
      "Batch [42/131]: Loss = 13.645557403564453\n",
      "Batch [43/131]: Loss = 7.987088203430176\n",
      "Batch [44/131]: Loss = -0.6612725257873535\n",
      "Batch [45/131]: Loss = -0.4378005266189575\n",
      "Batch [46/131]: Loss = -2.4387834072113037\n",
      "Batch [47/131]: Loss = -1.4665470123291016\n",
      "Batch [48/131]: Loss = -4.31862735748291\n",
      "Batch [49/131]: Loss = -4.5813798904418945\n",
      "Batch [50/131]: Loss = -5.513255596160889\n",
      "Batch [51/131]: Loss = -4.331816673278809\n",
      "Batch [52/131]: Loss = -5.795629978179932\n",
      "Batch [53/131]: Loss = -6.6359100341796875\n",
      "Batch [54/131]: Loss = -6.7676825523376465\n",
      "Batch [55/131]: Loss = -8.02759075164795\n",
      "Batch [56/131]: Loss = -6.541811466217041\n",
      "Batch [57/131]: Loss = -8.288816452026367\n",
      "Batch [58/131]: Loss = -9.721899032592773\n",
      "Batch [59/131]: Loss = -10.37477970123291\n",
      "Batch [60/131]: Loss = -17.137489318847656\n",
      "Batch [61/131]: Loss = -14.770101547241211\n",
      "Batch [62/131]: Loss = -3.189239263534546\n",
      "Batch [63/131]: Loss = -3.754081964492798\n",
      "Batch [64/131]: Loss = -8.063248634338379\n",
      "Batch [65/131]: Loss = -10.297101020812988\n",
      "Batch [66/131]: Loss = -14.181483268737793\n",
      "Batch [67/131]: Loss = -33.96547317504883\n",
      "Batch [68/131]: Loss = 89.92196655273438\n",
      "Batch [69/131]: Loss = 17.259342193603516\n",
      "Batch [70/131]: Loss = 1.5836703777313232\n",
      "Batch [71/131]: Loss = -3.5469443798065186\n",
      "Batch [72/131]: Loss = -3.7155966758728027\n",
      "Batch [73/131]: Loss = -4.148909568786621\n",
      "Batch [74/131]: Loss = -5.3513875007629395\n",
      "Batch [75/131]: Loss = -5.988607883453369\n",
      "Batch [76/131]: Loss = -6.172077178955078\n",
      "Batch [77/131]: Loss = -6.530357360839844\n",
      "Batch [78/131]: Loss = -6.413084983825684\n",
      "Batch [79/131]: Loss = -7.385682582855225\n",
      "Batch [80/131]: Loss = -7.484328269958496\n",
      "Batch [81/131]: Loss = -10.257225036621094\n",
      "Batch [82/131]: Loss = -9.127819061279297\n",
      "Batch [83/131]: Loss = -8.694536209106445\n",
      "Batch [84/131]: Loss = -14.84347152709961\n",
      "Batch [85/131]: Loss = -16.28714370727539\n",
      "Batch [86/131]: Loss = -240.4909210205078\n",
      "Batch [87/131]: Loss = 1.5891809463500977\n",
      "Batch [88/131]: Loss = -10.929336547851562\n",
      "Batch [89/131]: Loss = -112.59855651855469\n",
      "Batch [90/131]: Loss = -1.0612404346466064\n",
      "Batch [91/131]: Loss = -1.8646948337554932\n",
      "Batch [92/131]: Loss = -2.040344476699829\n",
      "Batch [93/131]: Loss = -4.2708353996276855\n",
      "Batch [94/131]: Loss = -4.17856502532959\n",
      "Batch [95/131]: Loss = -4.923208236694336\n",
      "Batch [96/131]: Loss = -5.639984130859375\n",
      "Batch [97/131]: Loss = -6.513122081756592\n",
      "Batch [98/131]: Loss = -5.98185396194458\n",
      "Batch [99/131]: Loss = -5.873191833496094\n",
      "Batch [100/131]: Loss = -6.446287631988525\n",
      "Batch [101/131]: Loss = -7.098782539367676\n",
      "Batch [102/131]: Loss = -6.9284186363220215\n",
      "Batch [103/131]: Loss = -7.31148099899292\n",
      "Batch [104/131]: Loss = -7.513805389404297\n",
      "Batch [105/131]: Loss = -9.451873779296875\n",
      "Batch [106/131]: Loss = -16.15477752685547\n",
      "Batch [107/131]: Loss = 1.9073309898376465\n",
      "Batch [108/131]: Loss = -6.721606254577637\n",
      "Batch [109/131]: Loss = -10.137763023376465\n",
      "Batch [110/131]: Loss = -1.7898799180984497\n",
      "Batch [111/131]: Loss = -40.84031295776367\n",
      "Batch [112/131]: Loss = 2.3173489570617676\n",
      "Batch [113/131]: Loss = -6.008480548858643\n",
      "Batch [114/131]: Loss = -6.914128303527832\n",
      "Batch [115/131]: Loss = -5.90132999420166\n",
      "Batch [116/131]: Loss = -8.205253601074219\n",
      "Batch [117/131]: Loss = -7.761004447937012\n",
      "Batch [118/131]: Loss = -7.693142890930176\n",
      "Batch [119/131]: Loss = -7.332291603088379\n",
      "Batch [120/131]: Loss = -8.070535659790039\n",
      "Batch [121/131]: Loss = -7.460624694824219\n",
      "Batch [122/131]: Loss = -9.211956977844238\n",
      "Batch [123/131]: Loss = -9.169166564941406\n",
      "Batch [124/131]: Loss = -9.667276382446289\n",
      "Batch [125/131]: Loss = -11.230083465576172\n",
      "Batch [126/131]: Loss = -9.012863159179688\n",
      "Batch [127/131]: Loss = -11.402242660522461\n",
      "Batch [128/131]: Loss = -12.246109008789062\n",
      "Batch [129/131]: Loss = -12.36628532409668\n",
      "Batch [130/131]: Loss = -12.392217636108398\n",
      "Batch [131/131]: Loss = -12.935846328735352\n",
      "Epoch 5: Validation Loss = -9.55172840754191\n",
      "Validation loss improved: -7.3678 -> -9.5517\n",
      "Epoch 6/20\n",
      "Batch [1/131]: Loss = -13.278932571411133\n",
      "Batch [2/131]: Loss = -16.311325073242188\n",
      "Batch [3/131]: Loss = -17.41330337524414\n",
      "Batch [4/131]: Loss = -20.950027465820312\n",
      "Batch [5/131]: Loss = -24.283395767211914\n",
      "Batch [6/131]: Loss = -32.066993713378906\n",
      "Batch [7/131]: Loss = -36.97270202636719\n",
      "Batch [8/131]: Loss = 124.22046661376953\n",
      "Batch [9/131]: Loss = 16.813289642333984\n",
      "Batch [10/131]: Loss = 2.2277488708496094\n",
      "Batch [11/131]: Loss = 0.090850830078125\n",
      "Batch [12/131]: Loss = -16.432605743408203\n",
      "Batch [13/131]: Loss = 31.299087524414062\n",
      "Batch [14/131]: Loss = 1.6146049499511719\n",
      "Batch [15/131]: Loss = 0.5947942733764648\n",
      "Batch [16/131]: Loss = -2.6677308082580566\n",
      "Batch [17/131]: Loss = -4.0239739418029785\n",
      "Batch [18/131]: Loss = -4.529716491699219\n",
      "Batch [19/131]: Loss = -4.861904144287109\n",
      "Batch [20/131]: Loss = -5.132784843444824\n",
      "Batch [21/131]: Loss = -5.612811088562012\n",
      "Batch [22/131]: Loss = -6.19020938873291\n",
      "Batch [23/131]: Loss = -6.328466415405273\n",
      "Batch [24/131]: Loss = -6.427163124084473\n",
      "Batch [25/131]: Loss = -6.551681995391846\n",
      "Batch [26/131]: Loss = -6.538848876953125\n",
      "Batch [27/131]: Loss = -6.60590934753418\n",
      "Batch [28/131]: Loss = -6.745030403137207\n",
      "Batch [29/131]: Loss = -6.892402648925781\n",
      "Batch [30/131]: Loss = -7.068283557891846\n",
      "Batch [31/131]: Loss = -7.030972957611084\n",
      "Batch [32/131]: Loss = -7.205478191375732\n",
      "Batch [33/131]: Loss = -7.617808818817139\n",
      "Batch [34/131]: Loss = -7.685148239135742\n",
      "Batch [35/131]: Loss = -8.000436782836914\n",
      "Batch [36/131]: Loss = -8.470489501953125\n",
      "Batch [37/131]: Loss = -9.261588096618652\n",
      "Batch [38/131]: Loss = -7.921087265014648\n",
      "Batch [39/131]: Loss = -11.693150520324707\n",
      "Batch [40/131]: Loss = -11.700370788574219\n",
      "Batch [41/131]: Loss = -11.872509956359863\n",
      "Batch [42/131]: Loss = -10.964643478393555\n",
      "Batch [43/131]: Loss = -14.157100677490234\n",
      "Batch [44/131]: Loss = -17.122406005859375\n",
      "Batch [45/131]: Loss = -17.390480041503906\n",
      "Batch [46/131]: Loss = -18.688762664794922\n",
      "Batch [47/131]: Loss = -24.13457489013672\n",
      "Batch [48/131]: Loss = -77.29808807373047\n",
      "Batch [49/131]: Loss = 478.73712158203125\n",
      "Batch [50/131]: Loss = 18.44670295715332\n",
      "Batch [51/131]: Loss = 1.0079607963562012\n",
      "Batch [52/131]: Loss = -2.1513986587524414\n",
      "Batch [53/131]: Loss = -8.662818908691406\n",
      "Batch [54/131]: Loss = -9.340768814086914\n",
      "Batch [55/131]: Loss = -15.330137252807617\n",
      "Batch [56/131]: Loss = -20.561370849609375\n",
      "Batch [57/131]: Loss = -250.291015625\n",
      "Batch [58/131]: Loss = 57.77003860473633\n",
      "Batch [59/131]: Loss = 20.114585876464844\n",
      "Batch [60/131]: Loss = 10.715292930603027\n",
      "Batch [61/131]: Loss = 5.058908939361572\n",
      "Batch [62/131]: Loss = 1.3743114471435547\n",
      "Batch [63/131]: Loss = 1.5024981498718262\n",
      "Batch [64/131]: Loss = -1.3867244720458984\n",
      "Batch [65/131]: Loss = -0.9254312515258789\n",
      "Batch [66/131]: Loss = -1.8313684463500977\n",
      "Batch [67/131]: Loss = -2.3788001537323\n",
      "Batch [68/131]: Loss = -1.9669615030288696\n",
      "Batch [69/131]: Loss = -2.912172555923462\n",
      "Batch [70/131]: Loss = -4.417068004608154\n",
      "Batch [71/131]: Loss = -3.9766266345977783\n",
      "Batch [72/131]: Loss = -4.930104732513428\n",
      "Batch [73/131]: Loss = -6.0224809646606445\n",
      "Batch [74/131]: Loss = -5.345147132873535\n",
      "Batch [75/131]: Loss = -6.544260501861572\n",
      "Batch [76/131]: Loss = -7.1528706550598145\n",
      "Batch [77/131]: Loss = -7.317392349243164\n",
      "Batch [78/131]: Loss = -7.359414100646973\n",
      "Batch [79/131]: Loss = -8.291515350341797\n",
      "Batch [80/131]: Loss = -8.661921501159668\n",
      "Batch [81/131]: Loss = -9.617545127868652\n",
      "Batch [82/131]: Loss = -11.325329780578613\n",
      "Batch [83/131]: Loss = -16.046722412109375\n",
      "Batch [84/131]: Loss = -74.30956268310547\n",
      "Batch [85/131]: Loss = 1.6093961000442505\n",
      "Batch [86/131]: Loss = -4.720188140869141\n",
      "Batch [87/131]: Loss = -7.626404762268066\n",
      "Batch [88/131]: Loss = -29.70901107788086\n",
      "Batch [89/131]: Loss = 11.906617164611816\n",
      "Batch [90/131]: Loss = -1.0755009651184082\n",
      "Batch [91/131]: Loss = -2.6879587173461914\n",
      "Batch [92/131]: Loss = -3.87577486038208\n",
      "Batch [93/131]: Loss = -5.458897590637207\n",
      "Batch [94/131]: Loss = -4.838228702545166\n",
      "Batch [95/131]: Loss = -5.8836989402771\n",
      "Batch [96/131]: Loss = -5.84583044052124\n",
      "Batch [97/131]: Loss = -6.7950310707092285\n",
      "Batch [98/131]: Loss = -6.657885551452637\n",
      "Batch [99/131]: Loss = -6.280572891235352\n",
      "Batch [100/131]: Loss = -7.214199066162109\n",
      "Batch [101/131]: Loss = -6.428046226501465\n",
      "Batch [102/131]: Loss = -7.805899620056152\n",
      "Batch [103/131]: Loss = -7.690073013305664\n",
      "Batch [104/131]: Loss = -6.8185715675354\n",
      "Batch [105/131]: Loss = -7.46092414855957\n",
      "Batch [106/131]: Loss = -7.597902297973633\n",
      "Batch [107/131]: Loss = -8.080233573913574\n",
      "Batch [108/131]: Loss = -14.147668838500977\n",
      "Batch [109/131]: Loss = 1.5519556999206543\n",
      "Batch [110/131]: Loss = 10.532572746276855\n",
      "Batch [111/131]: Loss = -3.1822128295898438\n",
      "Batch [112/131]: Loss = -13.974176406860352\n",
      "Batch [113/131]: Loss = -13.710524559020996\n",
      "Batch [114/131]: Loss = -33.72855758666992\n",
      "Batch [115/131]: Loss = -3.095057964324951\n",
      "Batch [116/131]: Loss = -1.494136929512024\n",
      "Batch [117/131]: Loss = -4.845674991607666\n",
      "Batch [118/131]: Loss = -5.125335216522217\n",
      "Batch [119/131]: Loss = -6.287066459655762\n",
      "Batch [120/131]: Loss = -8.322999954223633\n",
      "Batch [121/131]: Loss = -8.687004089355469\n",
      "Batch [122/131]: Loss = -9.48841381072998\n",
      "Batch [123/131]: Loss = -9.95413875579834\n",
      "Batch [124/131]: Loss = -9.92403793334961\n",
      "Batch [125/131]: Loss = -11.488049507141113\n",
      "Batch [126/131]: Loss = -13.099380493164062\n",
      "Batch [127/131]: Loss = -13.272285461425781\n",
      "Batch [128/131]: Loss = -15.135165214538574\n",
      "Batch [129/131]: Loss = -17.842464447021484\n",
      "Batch [130/131]: Loss = -25.420734405517578\n",
      "Batch [131/131]: Loss = -83.76466369628906\n",
      "Epoch 6: Validation Loss = -6.370033343633016\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 7/20\n",
      "Batch [1/131]: Loss = 339.218505859375\n",
      "Batch [2/131]: Loss = 17.98025894165039\n",
      "Batch [3/131]: Loss = -2.771954298019409\n",
      "Batch [4/131]: Loss = -3.996264696121216\n",
      "Batch [5/131]: Loss = -4.642648220062256\n",
      "Batch [6/131]: Loss = -5.079952716827393\n",
      "Batch [7/131]: Loss = -5.84043025970459\n",
      "Batch [8/131]: Loss = -5.9462456703186035\n",
      "Batch [9/131]: Loss = -6.241477966308594\n",
      "Batch [10/131]: Loss = -7.065288543701172\n",
      "Batch [11/131]: Loss = -7.393090724945068\n",
      "Batch [12/131]: Loss = -6.909152030944824\n",
      "Batch [13/131]: Loss = -7.51974630355835\n",
      "Batch [14/131]: Loss = -7.170241355895996\n",
      "Batch [15/131]: Loss = -7.657191276550293\n",
      "Batch [16/131]: Loss = -7.672694206237793\n",
      "Batch [17/131]: Loss = -8.644618034362793\n",
      "Batch [18/131]: Loss = -11.362529754638672\n",
      "Batch [19/131]: Loss = 32.48136901855469\n",
      "Batch [20/131]: Loss = 5.6136088371276855\n",
      "Batch [21/131]: Loss = 0.7094893455505371\n",
      "Batch [22/131]: Loss = -1.1015839576721191\n",
      "Batch [23/131]: Loss = -0.5815277099609375\n",
      "Batch [24/131]: Loss = -3.5234110355377197\n",
      "Batch [25/131]: Loss = -2.4615421295166016\n",
      "Batch [26/131]: Loss = -2.7663283348083496\n",
      "Batch [27/131]: Loss = -5.611843585968018\n",
      "Batch [28/131]: Loss = -10.1102294921875\n",
      "Batch [29/131]: Loss = -14.113847732543945\n",
      "Batch [30/131]: Loss = -194.56988525390625\n",
      "Batch [31/131]: Loss = -36.34339141845703\n",
      "Batch [32/131]: Loss = 35.869895935058594\n",
      "Batch [33/131]: Loss = 14.79117488861084\n",
      "Batch [34/131]: Loss = 10.513607025146484\n",
      "Batch [35/131]: Loss = 0.7214148044586182\n",
      "Batch [36/131]: Loss = -2.590240955352783\n",
      "Batch [37/131]: Loss = -4.783618450164795\n",
      "Batch [38/131]: Loss = -5.316335678100586\n",
      "Batch [39/131]: Loss = -6.576198101043701\n",
      "Batch [40/131]: Loss = -6.836860179901123\n",
      "Batch [41/131]: Loss = -8.247427940368652\n",
      "Batch [42/131]: Loss = -8.728155136108398\n",
      "Batch [43/131]: Loss = -9.058403968811035\n",
      "Batch [44/131]: Loss = -10.871216773986816\n",
      "Batch [45/131]: Loss = -19.003917694091797\n",
      "Batch [46/131]: Loss = 9.301071166992188\n",
      "Batch [47/131]: Loss = -0.1730642318725586\n",
      "Batch [48/131]: Loss = -6.008866310119629\n",
      "Batch [49/131]: Loss = -3.7819173336029053\n",
      "Batch [50/131]: Loss = -9.042680740356445\n",
      "Batch [51/131]: Loss = 2.5210142135620117\n",
      "Batch [52/131]: Loss = -1.6900792121887207\n",
      "Batch [53/131]: Loss = -5.040721893310547\n",
      "Batch [54/131]: Loss = -5.110754489898682\n",
      "Batch [55/131]: Loss = -4.921899318695068\n",
      "Batch [56/131]: Loss = -6.184329986572266\n",
      "Batch [57/131]: Loss = -6.184126853942871\n",
      "Batch [58/131]: Loss = -6.8181471824646\n",
      "Batch [59/131]: Loss = -6.6156110763549805\n",
      "Batch [60/131]: Loss = -7.138426780700684\n",
      "Batch [61/131]: Loss = -8.756887435913086\n",
      "Batch [62/131]: Loss = -9.457239151000977\n",
      "Batch [63/131]: Loss = -12.87208366394043\n",
      "Batch [64/131]: Loss = 8.487020492553711\n",
      "Batch [65/131]: Loss = -3.2363100051879883\n",
      "Batch [66/131]: Loss = 1.1581757068634033\n",
      "Batch [67/131]: Loss = -0.713355302810669\n",
      "Batch [68/131]: Loss = -2.0266149044036865\n",
      "Batch [69/131]: Loss = -2.8170557022094727\n",
      "Batch [70/131]: Loss = -3.2642159461975098\n",
      "Batch [71/131]: Loss = -3.3048057556152344\n",
      "Batch [72/131]: Loss = -4.7850446701049805\n",
      "Batch [73/131]: Loss = -5.827182769775391\n",
      "Batch [74/131]: Loss = -6.709900856018066\n",
      "Batch [75/131]: Loss = -8.061017990112305\n",
      "Batch [76/131]: Loss = -11.710960388183594\n",
      "Batch [77/131]: Loss = -19.036697387695312\n",
      "Batch [78/131]: Loss = -178.4070281982422\n",
      "Batch [79/131]: Loss = -53.56959915161133\n",
      "Batch [80/131]: Loss = 69.27381896972656\n",
      "Batch [81/131]: Loss = 24.122785568237305\n",
      "Batch [82/131]: Loss = 4.772007942199707\n",
      "Batch [83/131]: Loss = 7.478189945220947\n",
      "Batch [84/131]: Loss = 1.9886674880981445\n",
      "Batch [85/131]: Loss = -5.768111228942871\n",
      "Batch [86/131]: Loss = -3.351912498474121\n",
      "Batch [87/131]: Loss = -7.0275492668151855\n",
      "Batch [88/131]: Loss = -5.513634204864502\n",
      "Batch [89/131]: Loss = -6.498083114624023\n",
      "Batch [90/131]: Loss = -6.558208465576172\n",
      "Batch [91/131]: Loss = -7.011600494384766\n",
      "Batch [92/131]: Loss = -7.015832424163818\n",
      "Batch [93/131]: Loss = -9.200936317443848\n",
      "Batch [94/131]: Loss = -9.487750053405762\n",
      "Batch [95/131]: Loss = -8.744888305664062\n",
      "Batch [96/131]: Loss = -10.329907417297363\n",
      "Batch [97/131]: Loss = -12.170103073120117\n",
      "Batch [98/131]: Loss = 0.20896393060684204\n",
      "Batch [99/131]: Loss = -3.1422839164733887\n",
      "Batch [100/131]: Loss = -3.5937705039978027\n",
      "Batch [101/131]: Loss = -5.093444347381592\n",
      "Batch [102/131]: Loss = -6.26624059677124\n",
      "Batch [103/131]: Loss = -9.093338012695312\n",
      "Batch [104/131]: Loss = -4.5192952156066895\n",
      "Batch [105/131]: Loss = -13.19746208190918\n",
      "Batch [106/131]: Loss = -17.564905166625977\n",
      "Batch [107/131]: Loss = -13.337212562561035\n",
      "Batch [108/131]: Loss = -14.07820987701416\n",
      "Batch [109/131]: Loss = -158.489501953125\n",
      "Batch [110/131]: Loss = 3.8323750495910645\n",
      "Batch [111/131]: Loss = -6.549046993255615\n",
      "Batch [112/131]: Loss = -4.713109970092773\n",
      "Batch [113/131]: Loss = 0.35447001457214355\n",
      "Batch [114/131]: Loss = -11.747727394104004\n",
      "Batch [115/131]: Loss = -6.9196624755859375\n",
      "Batch [116/131]: Loss = -10.766298294067383\n",
      "Batch [117/131]: Loss = -9.792312622070312\n",
      "Batch [118/131]: Loss = -11.38050651550293\n",
      "Batch [119/131]: Loss = -10.886543273925781\n",
      "Batch [120/131]: Loss = -11.824028015136719\n",
      "Batch [121/131]: Loss = -13.97513484954834\n",
      "Batch [122/131]: Loss = -11.93890380859375\n",
      "Batch [123/131]: Loss = -6.31471586227417\n",
      "Batch [124/131]: Loss = -16.49435806274414\n",
      "Batch [125/131]: Loss = -16.437374114990234\n",
      "Batch [126/131]: Loss = -24.749032974243164\n",
      "Batch [127/131]: Loss = -24.73419761657715\n",
      "Batch [128/131]: Loss = -34.64658737182617\n",
      "Batch [129/131]: Loss = -13.024413108825684\n",
      "Batch [130/131]: Loss = 9.640037536621094\n",
      "Batch [131/131]: Loss = 57.895912170410156\n",
      "Epoch 7: Validation Loss = -5.960519499248928\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 8/20\n",
      "Batch [1/131]: Loss = -51.21310806274414\n",
      "Batch [2/131]: Loss = -1.628936767578125\n",
      "Batch [3/131]: Loss = -0.005875289440155029\n",
      "Batch [4/131]: Loss = -2.133950710296631\n",
      "Batch [5/131]: Loss = -4.105770111083984\n",
      "Batch [6/131]: Loss = -12.38333511352539\n",
      "Batch [7/131]: Loss = 1.3337254524230957\n",
      "Batch [8/131]: Loss = -1.3622894287109375\n",
      "Batch [9/131]: Loss = -4.594510078430176\n",
      "Batch [10/131]: Loss = -6.32041072845459\n",
      "Batch [11/131]: Loss = -2.95222806930542\n",
      "Batch [12/131]: Loss = -11.68533706665039\n",
      "Batch [13/131]: Loss = -10.727119445800781\n",
      "Batch [14/131]: Loss = -5.340780258178711\n",
      "Batch [15/131]: Loss = -7.845309257507324\n",
      "Batch [16/131]: Loss = -6.427927494049072\n",
      "Batch [17/131]: Loss = -11.814980506896973\n",
      "Batch [18/131]: Loss = -11.139699935913086\n",
      "Batch [19/131]: Loss = -10.477821350097656\n",
      "Batch [20/131]: Loss = -12.206666946411133\n",
      "Batch [21/131]: Loss = -7.021915435791016\n",
      "Batch [22/131]: Loss = -8.263128280639648\n",
      "Batch [23/131]: Loss = -13.12060832977295\n",
      "Batch [24/131]: Loss = -14.357083320617676\n",
      "Batch [25/131]: Loss = -9.312126159667969\n",
      "Batch [26/131]: Loss = -12.103517532348633\n",
      "Batch [27/131]: Loss = -9.9552001953125\n",
      "Batch [28/131]: Loss = -12.947511672973633\n",
      "Batch [29/131]: Loss = -8.685405731201172\n",
      "Batch [30/131]: Loss = -18.123422622680664\n",
      "Batch [31/131]: Loss = -12.867006301879883\n",
      "Batch [32/131]: Loss = -17.273311614990234\n",
      "Batch [33/131]: Loss = -13.73598861694336\n",
      "Batch [34/131]: Loss = -11.681940078735352\n",
      "Batch [35/131]: Loss = -16.127490997314453\n",
      "Batch [36/131]: Loss = -128.23672485351562\n",
      "Batch [37/131]: Loss = 30.144363403320312\n",
      "Batch [38/131]: Loss = -3.788477897644043\n",
      "Batch [39/131]: Loss = -3.585322856903076\n",
      "Batch [40/131]: Loss = -5.581913471221924\n",
      "Batch [41/131]: Loss = -5.710920333862305\n",
      "Batch [42/131]: Loss = -6.220705032348633\n",
      "Batch [43/131]: Loss = -6.559905052185059\n",
      "Batch [44/131]: Loss = -6.697892665863037\n",
      "Batch [45/131]: Loss = -6.876774311065674\n",
      "Batch [46/131]: Loss = -7.631645679473877\n",
      "Batch [47/131]: Loss = -7.506248474121094\n",
      "Batch [48/131]: Loss = -7.355320453643799\n",
      "Batch [49/131]: Loss = -7.999737739562988\n",
      "Batch [50/131]: Loss = -8.627062797546387\n",
      "Batch [51/131]: Loss = -9.86170768737793\n",
      "Batch [52/131]: Loss = -7.998012542724609\n",
      "Batch [53/131]: Loss = -8.707561492919922\n",
      "Batch [54/131]: Loss = -9.469094276428223\n",
      "Batch [55/131]: Loss = -12.296037673950195\n",
      "Batch [56/131]: Loss = -15.724252700805664\n",
      "Batch [57/131]: Loss = -15.154902458190918\n",
      "Batch [58/131]: Loss = -23.67680549621582\n",
      "Batch [59/131]: Loss = -71.19035339355469\n",
      "Batch [60/131]: Loss = -5.371713638305664\n",
      "Batch [61/131]: Loss = 4.489084243774414\n",
      "Batch [62/131]: Loss = -13.452606201171875\n",
      "Batch [63/131]: Loss = -26.091428756713867\n",
      "Batch [64/131]: Loss = -2.340949058532715\n",
      "Batch [65/131]: Loss = -22.83237075805664\n",
      "Batch [66/131]: Loss = -82.95133209228516\n",
      "Batch [67/131]: Loss = 11.634581565856934\n",
      "Batch [68/131]: Loss = 26.78386116027832\n",
      "Batch [69/131]: Loss = 40.60890197753906\n",
      "Batch [70/131]: Loss = 0.29598069190979004\n",
      "Batch [71/131]: Loss = 10.46953010559082\n",
      "Batch [72/131]: Loss = -1.7606604099273682\n",
      "Batch [73/131]: Loss = -9.229229927062988\n",
      "Batch [74/131]: Loss = -6.064929962158203\n",
      "Batch [75/131]: Loss = -10.224095344543457\n",
      "Batch [76/131]: Loss = -12.639870643615723\n",
      "Batch [77/131]: Loss = -7.789360046386719\n",
      "Batch [78/131]: Loss = -2.752528190612793\n",
      "Batch [79/131]: Loss = 11.316620826721191\n",
      "Batch [80/131]: Loss = 9.284835815429688\n",
      "Batch [81/131]: Loss = -13.539838790893555\n",
      "Batch [82/131]: Loss = 14.006434440612793\n",
      "Batch [83/131]: Loss = -1.9091269969940186\n",
      "Batch [84/131]: Loss = -49.75106430053711\n",
      "Batch [85/131]: Loss = 6.107826232910156\n",
      "Batch [86/131]: Loss = -7.035160064697266\n",
      "Batch [87/131]: Loss = -31.307558059692383\n",
      "Batch [88/131]: Loss = 2.3614163398742676\n",
      "Batch [89/131]: Loss = -24.28069496154785\n",
      "Batch [90/131]: Loss = -9.867777824401855\n",
      "Batch [91/131]: Loss = -1194.5181884765625\n",
      "Batch [92/131]: Loss = 56.04144287109375\n",
      "Batch [93/131]: Loss = -11.356752395629883\n",
      "Batch [94/131]: Loss = -2.1131343841552734\n",
      "Batch [95/131]: Loss = 5.117775917053223\n",
      "Batch [96/131]: Loss = 1.9541664123535156\n",
      "Batch [97/131]: Loss = 0.9044899940490723\n",
      "Batch [98/131]: Loss = -9.418405532836914\n",
      "Batch [99/131]: Loss = 2.0294859409332275\n",
      "Batch [100/131]: Loss = -3.640416145324707\n",
      "Batch [101/131]: Loss = -13.408769607543945\n",
      "Batch [102/131]: Loss = -24.444639205932617\n",
      "Batch [103/131]: Loss = -18.56615447998047\n",
      "Batch [104/131]: Loss = -16.221988677978516\n",
      "Batch [105/131]: Loss = -4.604994773864746\n",
      "Batch [106/131]: Loss = -0.6503434181213379\n",
      "Batch [107/131]: Loss = -1.912861943244934\n",
      "Batch [108/131]: Loss = -2.361323356628418\n",
      "Batch [109/131]: Loss = -4.907853603363037\n",
      "Batch [110/131]: Loss = -33.456207275390625\n",
      "Batch [111/131]: Loss = -22.196046829223633\n",
      "Batch [112/131]: Loss = -27.09508514404297\n",
      "Batch [113/131]: Loss = -9.47585678100586\n",
      "Batch [114/131]: Loss = -144.26388549804688\n",
      "Batch [115/131]: Loss = 41.790496826171875\n",
      "Batch [116/131]: Loss = 18.881961822509766\n",
      "Batch [117/131]: Loss = 10.073579788208008\n",
      "Batch [118/131]: Loss = -4.343114852905273\n",
      "Batch [119/131]: Loss = -1.672669529914856\n",
      "Batch [120/131]: Loss = -3.9311039447784424\n",
      "Batch [121/131]: Loss = -10.67080307006836\n",
      "Batch [122/131]: Loss = -5.161550521850586\n",
      "Batch [123/131]: Loss = -7.101140975952148\n",
      "Batch [124/131]: Loss = -7.927861213684082\n",
      "Batch [125/131]: Loss = -11.694940567016602\n",
      "Batch [126/131]: Loss = -9.143476486206055\n",
      "Batch [127/131]: Loss = -12.106184005737305\n",
      "Batch [128/131]: Loss = -14.964813232421875\n",
      "Batch [129/131]: Loss = -12.407010078430176\n",
      "Batch [130/131]: Loss = -14.578332901000977\n",
      "Batch [131/131]: Loss = -11.52188491821289\n",
      "Epoch 8: Validation Loss = -7.423646132151286\n",
      "Validation loss did not improve for 3 epochs.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "Training Complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': -6.924365546968248, 'train_loss': -9.338752471308672},\n",
       " {'val_loss': -5.637617031733195, 'train_loss': 53.96748235844474},\n",
       " {'val_loss': -7.36779605017768, 'train_loss': -9.527189462239505},\n",
       " {'val_loss': -0.29870279630025226, 'train_loss': -6.311014724141769},\n",
       " {'val_loss': -9.55172840754191, 'train_loss': -11.506827671109265},\n",
       " {'val_loss': -6.370033343633016, 'train_loss': -5.500076391314733},\n",
       " {'val_loss': -5.960519499248928, 'train_loss': -6.086949739292378},\n",
       " {'val_loss': -7.423646132151286, 'train_loss': -18.731287182287407}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(\n",
    "    epochs=20, \n",
    "    optimizer_class=torch.optim.Adam, \n",
    "    model=model, \n",
    "    learning_rate=0.001, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    lr_scheduler_class=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "    model_name=\"best_model_dl3.pt\", \n",
    "    patience=3, \n",
    "    factor=0.5,\n",
    "    device='cpu'  # Use CPU instead of CUDA\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b48c054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Batch [1/131]: Loss = 2.377835512161255\n",
      "Batch [2/131]: Loss = -6.94901704788208\n",
      "Batch [3/131]: Loss = -15.470351219177246\n",
      "Batch [4/131]: Loss = -94.61636352539062\n",
      "Batch [5/131]: Loss = 7.566617965698242\n",
      "Batch [6/131]: Loss = 1.086634874343872\n",
      "Batch [7/131]: Loss = -1.7111866474151611\n",
      "Batch [8/131]: Loss = -2.643752098083496\n",
      "Batch [9/131]: Loss = -3.3463828563690186\n",
      "Batch [10/131]: Loss = -3.80133056640625\n",
      "Batch [11/131]: Loss = -4.364409446716309\n",
      "Batch [12/131]: Loss = -4.702343463897705\n",
      "Batch [13/131]: Loss = -5.218607425689697\n",
      "Batch [14/131]: Loss = -5.17597770690918\n",
      "Batch [15/131]: Loss = -5.549098014831543\n",
      "Batch [16/131]: Loss = -6.113402366638184\n",
      "Batch [17/131]: Loss = -7.205503463745117\n",
      "Batch [18/131]: Loss = -7.849090099334717\n",
      "Batch [19/131]: Loss = -11.637273788452148\n",
      "Batch [20/131]: Loss = -22.72888946533203\n",
      "Batch [21/131]: Loss = 48.79161071777344\n",
      "Batch [22/131]: Loss = 3.4767730236053467\n",
      "Batch [23/131]: Loss = 0.8643150925636292\n",
      "Batch [24/131]: Loss = -5.246467590332031\n",
      "Batch [25/131]: Loss = -20.109302520751953\n",
      "Batch [26/131]: Loss = 0.9594688415527344\n",
      "Batch [27/131]: Loss = -6.479296684265137\n",
      "Batch [28/131]: Loss = -9.137523651123047\n",
      "Batch [29/131]: Loss = 8.917375564575195\n",
      "Batch [30/131]: Loss = -7.303028106689453\n",
      "Batch [31/131]: Loss = -69.14382934570312\n",
      "Batch [32/131]: Loss = -7.480070114135742\n",
      "Batch [33/131]: Loss = -88.15716552734375\n",
      "Batch [34/131]: Loss = -9.375831604003906\n",
      "Batch [35/131]: Loss = -12.431900024414062\n",
      "Batch [36/131]: Loss = -36.256526947021484\n",
      "Batch [37/131]: Loss = 20.20641326904297\n",
      "Batch [38/131]: Loss = -4.934492588043213\n",
      "Batch [39/131]: Loss = -10.814838409423828\n",
      "Batch [40/131]: Loss = -19.44866180419922\n",
      "Batch [41/131]: Loss = -9.42878532409668\n",
      "Batch [42/131]: Loss = -18.47393798828125\n",
      "Batch [43/131]: Loss = 41.39852523803711\n",
      "Batch [44/131]: Loss = 5.887326240539551\n",
      "Batch [45/131]: Loss = -3.6369481086730957\n",
      "Batch [46/131]: Loss = -7.124573707580566\n",
      "Batch [47/131]: Loss = -4.42110538482666\n",
      "Batch [48/131]: Loss = -10.01114273071289\n",
      "Batch [49/131]: Loss = -12.808980941772461\n",
      "Batch [50/131]: Loss = -28.478578567504883\n",
      "Batch [51/131]: Loss = 18.740713119506836\n",
      "Batch [52/131]: Loss = -6.953558921813965\n",
      "Batch [53/131]: Loss = -10.681272506713867\n",
      "Batch [54/131]: Loss = -23.24892807006836\n",
      "Batch [55/131]: Loss = -1436.67041015625\n",
      "Batch [56/131]: Loss = 10.291634559631348\n",
      "Batch [57/131]: Loss = -6.80570125579834\n",
      "Batch [58/131]: Loss = -7.99967098236084\n",
      "Batch [59/131]: Loss = -8.802824974060059\n",
      "Batch [60/131]: Loss = 23.402061462402344\n",
      "Batch [61/131]: Loss = -18.761337280273438\n",
      "Batch [62/131]: Loss = -66.4646987915039\n",
      "Batch [63/131]: Loss = 3.012176513671875\n",
      "Batch [64/131]: Loss = 1.1101417541503906\n",
      "Batch [65/131]: Loss = -7.524040699005127\n",
      "Batch [66/131]: Loss = -12.290704727172852\n",
      "Batch [67/131]: Loss = -13.532197952270508\n",
      "Batch [68/131]: Loss = -17.699047088623047\n",
      "Batch [69/131]: Loss = -10.357582092285156\n",
      "Batch [70/131]: Loss = -45.296146392822266\n",
      "Batch [71/131]: Loss = 86.99826049804688\n",
      "Batch [72/131]: Loss = -5.647658824920654\n",
      "Batch [73/131]: Loss = -5.207189083099365\n",
      "Batch [74/131]: Loss = -6.831670761108398\n",
      "Batch [75/131]: Loss = -7.636790752410889\n",
      "Batch [76/131]: Loss = -5.963980197906494\n",
      "Batch [77/131]: Loss = -9.703245162963867\n",
      "Batch [78/131]: Loss = -12.039071083068848\n",
      "Batch [79/131]: Loss = -14.339179039001465\n",
      "Batch [80/131]: Loss = -20.237756729125977\n",
      "Batch [81/131]: Loss = -14.644896507263184\n",
      "Batch [82/131]: Loss = 49.673126220703125\n",
      "Batch [83/131]: Loss = -2.122375726699829\n",
      "Batch [84/131]: Loss = -0.3033638000488281\n",
      "Batch [85/131]: Loss = -0.22692346572875977\n",
      "Batch [86/131]: Loss = -0.7142186164855957\n",
      "Batch [87/131]: Loss = -7.3272528648376465\n",
      "Batch [88/131]: Loss = -5.2599382400512695\n",
      "Batch [89/131]: Loss = -8.22315788269043\n",
      "Batch [90/131]: Loss = -8.051309585571289\n",
      "Batch [91/131]: Loss = -9.775566101074219\n",
      "Batch [92/131]: Loss = -13.943358421325684\n",
      "Batch [93/131]: Loss = -14.108392715454102\n",
      "Batch [94/131]: Loss = -11.235236167907715\n",
      "Batch [95/131]: Loss = -10.595441818237305\n",
      "Batch [96/131]: Loss = -17.714479446411133\n",
      "Batch [97/131]: Loss = -22.130046844482422\n",
      "Batch [98/131]: Loss = -23.91292381286621\n",
      "Batch [99/131]: Loss = -22.349241256713867\n",
      "Batch [100/131]: Loss = 119.32614135742188\n",
      "Batch [101/131]: Loss = -1.6924457550048828\n",
      "Batch [102/131]: Loss = -5.957285404205322\n",
      "Batch [103/131]: Loss = -6.681245803833008\n",
      "Batch [104/131]: Loss = -7.556804656982422\n",
      "Batch [105/131]: Loss = -7.620001792907715\n",
      "Batch [106/131]: Loss = -9.031024932861328\n",
      "Batch [107/131]: Loss = -10.850971221923828\n",
      "Batch [108/131]: Loss = -13.987249374389648\n",
      "Batch [109/131]: Loss = -19.57324981689453\n",
      "Batch [110/131]: Loss = -128.51783752441406\n",
      "Batch [111/131]: Loss = 18.027690887451172\n",
      "Batch [112/131]: Loss = -4.004746913909912\n",
      "Batch [113/131]: Loss = 2.1430249214172363\n",
      "Batch [114/131]: Loss = -0.28945398330688477\n",
      "Batch [115/131]: Loss = -4.797348976135254\n",
      "Batch [116/131]: Loss = -5.252676963806152\n",
      "Batch [117/131]: Loss = -15.925224304199219\n",
      "Batch [118/131]: Loss = -12.522900581359863\n",
      "Batch [119/131]: Loss = -5.869855880737305\n",
      "Batch [120/131]: Loss = -15.322833061218262\n",
      "Batch [121/131]: Loss = -20.00484275817871\n",
      "Batch [122/131]: Loss = -14.766913414001465\n",
      "Batch [123/131]: Loss = -11.10076904296875\n",
      "Batch [124/131]: Loss = -26.146469116210938\n",
      "Batch [125/131]: Loss = -15.718757629394531\n",
      "Batch [126/131]: Loss = -21.86398696899414\n",
      "Batch [127/131]: Loss = 190.98593139648438\n",
      "Batch [128/131]: Loss = 5.121522903442383\n",
      "Batch [129/131]: Loss = -1.3676385879516602\n",
      "Batch [130/131]: Loss = -6.626721382141113\n",
      "Batch [131/131]: Loss = -8.011209487915039\n",
      "Epoch 1: Validation Loss = -9.043763743506538\n",
      "Validation loss improved: inf -> -9.0438\n",
      "Epoch 2/20\n",
      "Batch [1/131]: Loss = -8.81900691986084\n",
      "Batch [2/131]: Loss = -11.076204299926758\n",
      "Batch [3/131]: Loss = -10.991483688354492\n",
      "Batch [4/131]: Loss = -14.141033172607422\n",
      "Batch [5/131]: Loss = -18.853321075439453\n",
      "Batch [6/131]: Loss = -31.62368392944336\n",
      "Batch [7/131]: Loss = -158.81231689453125\n",
      "Batch [8/131]: Loss = 16.900196075439453\n",
      "Batch [9/131]: Loss = 9.338006973266602\n",
      "Batch [10/131]: Loss = 2.2422924041748047\n",
      "Batch [11/131]: Loss = -5.0834197998046875\n",
      "Batch [12/131]: Loss = -1.2996149063110352\n",
      "Batch [13/131]: Loss = -5.405663967132568\n",
      "Batch [14/131]: Loss = -4.896895408630371\n",
      "Batch [15/131]: Loss = -7.982767105102539\n",
      "Batch [16/131]: Loss = -13.204183578491211\n",
      "Batch [17/131]: Loss = -22.63932991027832\n",
      "Batch [18/131]: Loss = -15.464780807495117\n",
      "Batch [19/131]: Loss = 57.19115447998047\n",
      "Batch [20/131]: Loss = 12.935705184936523\n",
      "Batch [21/131]: Loss = 5.852433204650879\n",
      "Batch [22/131]: Loss = 4.702120304107666\n",
      "Batch [23/131]: Loss = -1.0737180709838867\n",
      "Batch [24/131]: Loss = -4.989227294921875\n",
      "Batch [25/131]: Loss = -5.470227241516113\n",
      "Batch [26/131]: Loss = -5.714288711547852\n",
      "Batch [27/131]: Loss = -7.3834228515625\n",
      "Batch [28/131]: Loss = -6.611617088317871\n",
      "Batch [29/131]: Loss = -8.33093547821045\n",
      "Batch [30/131]: Loss = -9.487828254699707\n",
      "Batch [31/131]: Loss = -10.951615333557129\n",
      "Batch [32/131]: Loss = -11.392097473144531\n",
      "Batch [33/131]: Loss = -15.736151695251465\n",
      "Batch [34/131]: Loss = -28.892478942871094\n",
      "Batch [35/131]: Loss = 68.52783203125\n",
      "Batch [36/131]: Loss = 2.1999406814575195\n",
      "Batch [37/131]: Loss = -0.3539975881576538\n",
      "Batch [38/131]: Loss = -4.713509559631348\n",
      "Batch [39/131]: Loss = -22.785659790039062\n",
      "Batch [40/131]: Loss = 1.0363707542419434\n",
      "Batch [41/131]: Loss = -2.487973690032959\n",
      "Batch [42/131]: Loss = -4.214870452880859\n",
      "Batch [43/131]: Loss = -4.822120666503906\n",
      "Batch [44/131]: Loss = -5.415194988250732\n",
      "Batch [45/131]: Loss = -6.171006202697754\n",
      "Batch [46/131]: Loss = -6.525199890136719\n",
      "Batch [47/131]: Loss = -7.502355575561523\n",
      "Batch [48/131]: Loss = -9.275487899780273\n",
      "Batch [49/131]: Loss = -15.61489486694336\n",
      "Batch [50/131]: Loss = -44.84362030029297\n",
      "Batch [51/131]: Loss = 16.574949264526367\n",
      "Batch [52/131]: Loss = 3.9850425720214844\n",
      "Batch [53/131]: Loss = 1.6026616096496582\n",
      "Batch [54/131]: Loss = 0.9046573638916016\n",
      "Batch [55/131]: Loss = -7.923066139221191\n",
      "Batch [56/131]: Loss = -7.6133294105529785\n",
      "Batch [57/131]: Loss = -7.1070098876953125\n",
      "Batch [58/131]: Loss = -7.959142208099365\n",
      "Batch [59/131]: Loss = -2.6668643951416016\n",
      "Batch [60/131]: Loss = -14.518936157226562\n",
      "Batch [61/131]: Loss = -7.9880828857421875\n",
      "Batch [62/131]: Loss = -12.817817687988281\n",
      "Batch [63/131]: Loss = -11.344213485717773\n",
      "Batch [64/131]: Loss = -13.615066528320312\n",
      "Batch [65/131]: Loss = -12.336603164672852\n",
      "Batch [66/131]: Loss = -16.579118728637695\n",
      "Batch [67/131]: Loss = -1.884875774383545\n",
      "Batch [68/131]: Loss = -25.716266632080078\n",
      "Batch [69/131]: Loss = -27.974380493164062\n",
      "Batch [70/131]: Loss = 10.379121780395508\n",
      "Batch [71/131]: Loss = 16.382335662841797\n",
      "Batch [72/131]: Loss = 11.008378028869629\n",
      "Batch [73/131]: Loss = -5.353451251983643\n",
      "Batch [74/131]: Loss = -2.600701332092285\n",
      "Batch [75/131]: Loss = -1.098703384399414\n",
      "Batch [76/131]: Loss = -4.699165344238281\n",
      "Batch [77/131]: Loss = -5.592952728271484\n",
      "Batch [78/131]: Loss = -4.041419982910156\n",
      "Batch [79/131]: Loss = -6.215459823608398\n",
      "Batch [80/131]: Loss = -6.104722023010254\n",
      "Batch [81/131]: Loss = -6.375616073608398\n",
      "Batch [82/131]: Loss = -4.978519916534424\n",
      "Batch [83/131]: Loss = -5.9615044593811035\n",
      "Batch [84/131]: Loss = -6.560756206512451\n",
      "Batch [85/131]: Loss = -6.467409610748291\n",
      "Batch [86/131]: Loss = -6.807251930236816\n",
      "Batch [87/131]: Loss = -7.329111099243164\n",
      "Batch [88/131]: Loss = -7.653368949890137\n",
      "Batch [89/131]: Loss = -8.42426872253418\n",
      "Batch [90/131]: Loss = -8.556167602539062\n",
      "Batch [91/131]: Loss = -8.930512428283691\n",
      "Batch [92/131]: Loss = -10.47747802734375\n",
      "Batch [93/131]: Loss = -11.544317245483398\n",
      "Batch [94/131]: Loss = -14.69415283203125\n",
      "Batch [95/131]: Loss = -19.393043518066406\n",
      "Batch [96/131]: Loss = -59.408390045166016\n",
      "Batch [97/131]: Loss = 30.28485679626465\n",
      "Batch [98/131]: Loss = 11.642217636108398\n",
      "Batch [99/131]: Loss = 0.4703798294067383\n",
      "Batch [100/131]: Loss = 2.1059622764587402\n",
      "Batch [101/131]: Loss = -1.1192355155944824\n",
      "Batch [102/131]: Loss = -1.257706642150879\n",
      "Batch [103/131]: Loss = -1.4802510738372803\n",
      "Batch [104/131]: Loss = -2.7977709770202637\n",
      "Batch [105/131]: Loss = -2.6226446628570557\n",
      "Batch [106/131]: Loss = -3.5149459838867188\n",
      "Batch [107/131]: Loss = -3.2621452808380127\n",
      "Batch [108/131]: Loss = -4.234667778015137\n",
      "Batch [109/131]: Loss = -4.479769706726074\n",
      "Batch [110/131]: Loss = -4.74364709854126\n",
      "Batch [111/131]: Loss = -4.6340227127075195\n",
      "Batch [112/131]: Loss = -5.203764915466309\n",
      "Batch [113/131]: Loss = -5.660921096801758\n",
      "Batch [114/131]: Loss = -5.364212989807129\n",
      "Batch [115/131]: Loss = -5.682060241699219\n",
      "Batch [116/131]: Loss = -6.107035160064697\n",
      "Batch [117/131]: Loss = -6.271234512329102\n",
      "Batch [118/131]: Loss = -6.394316673278809\n",
      "Batch [119/131]: Loss = -6.647071838378906\n",
      "Batch [120/131]: Loss = -6.774352550506592\n",
      "Batch [121/131]: Loss = -7.8431572914123535\n",
      "Batch [122/131]: Loss = -7.7596330642700195\n",
      "Batch [123/131]: Loss = -8.176515579223633\n",
      "Batch [124/131]: Loss = -8.036765098571777\n",
      "Batch [125/131]: Loss = -8.670793533325195\n",
      "Batch [126/131]: Loss = -9.82027816772461\n",
      "Batch [127/131]: Loss = -9.642502784729004\n",
      "Batch [128/131]: Loss = -9.896121978759766\n",
      "Batch [129/131]: Loss = -10.202983856201172\n",
      "Batch [130/131]: Loss = -12.03908634185791\n",
      "Batch [131/131]: Loss = -11.62753677368164\n",
      "Epoch 2: Validation Loss = -8.032420449786716\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 3/20\n",
      "Batch [1/131]: Loss = -12.005990028381348\n",
      "Batch [2/131]: Loss = -15.993757247924805\n",
      "Batch [3/131]: Loss = -16.605600357055664\n",
      "Batch [4/131]: Loss = -23.85348129272461\n",
      "Batch [5/131]: Loss = -25.227527618408203\n",
      "Batch [6/131]: Loss = -46.561527252197266\n",
      "Batch [7/131]: Loss = 419.4728088378906\n",
      "Batch [8/131]: Loss = 10.586986541748047\n",
      "Batch [9/131]: Loss = 3.485238552093506\n",
      "Batch [10/131]: Loss = -0.20920944213867188\n",
      "Batch [11/131]: Loss = -2.7478795051574707\n",
      "Batch [12/131]: Loss = -5.025042533874512\n",
      "Batch [13/131]: Loss = -6.863674640655518\n",
      "Batch [14/131]: Loss = -8.217300415039062\n",
      "Batch [15/131]: Loss = -8.920936584472656\n",
      "Batch [16/131]: Loss = -10.469104766845703\n",
      "Batch [17/131]: Loss = -12.04167652130127\n",
      "Batch [18/131]: Loss = -16.584121704101562\n",
      "Batch [19/131]: Loss = 259.5848693847656\n",
      "Batch [20/131]: Loss = -4.787697792053223\n",
      "Batch [21/131]: Loss = -7.701522350311279\n",
      "Batch [22/131]: Loss = -30.61431121826172\n",
      "Batch [23/131]: Loss = 5.1583251953125\n",
      "Batch [24/131]: Loss = -2.151475429534912\n",
      "Batch [25/131]: Loss = -5.560473918914795\n",
      "Batch [26/131]: Loss = -8.248334884643555\n",
      "Batch [27/131]: Loss = -8.544098854064941\n",
      "Batch [28/131]: Loss = -9.345171928405762\n",
      "Batch [29/131]: Loss = -10.83824348449707\n",
      "Batch [30/131]: Loss = -11.872167587280273\n",
      "Batch [31/131]: Loss = 12.245400428771973\n",
      "Batch [32/131]: Loss = -8.848249435424805\n",
      "Batch [33/131]: Loss = -8.930695533752441\n",
      "Batch [34/131]: Loss = -15.257023811340332\n",
      "Batch [35/131]: Loss = -6.18331241607666\n",
      "Batch [36/131]: Loss = -112.32001495361328\n",
      "Batch [37/131]: Loss = 12.994361877441406\n",
      "Batch [38/131]: Loss = -2.2592275142669678\n",
      "Batch [39/131]: Loss = -3.755312204360962\n",
      "Batch [40/131]: Loss = -4.329441070556641\n",
      "Batch [41/131]: Loss = -6.144906044006348\n",
      "Batch [42/131]: Loss = -7.630960941314697\n",
      "Batch [43/131]: Loss = -8.898507118225098\n",
      "Batch [44/131]: Loss = -9.277979850769043\n",
      "Batch [45/131]: Loss = -20.199230194091797\n",
      "Batch [46/131]: Loss = 9.935778617858887\n",
      "Batch [47/131]: Loss = -3.2592880725860596\n",
      "Batch [48/131]: Loss = -4.701308250427246\n",
      "Batch [49/131]: Loss = -7.684152603149414\n",
      "Batch [50/131]: Loss = -9.188069343566895\n",
      "Batch [51/131]: Loss = -20.60308074951172\n",
      "Batch [52/131]: Loss = 9.614968299865723\n",
      "Batch [53/131]: Loss = -4.686503887176514\n",
      "Batch [54/131]: Loss = -5.01024055480957\n",
      "Batch [55/131]: Loss = -5.868051528930664\n",
      "Batch [56/131]: Loss = -6.19769287109375\n",
      "Batch [57/131]: Loss = -5.823978424072266\n",
      "Batch [58/131]: Loss = -7.025017738342285\n",
      "Batch [59/131]: Loss = -6.4737420082092285\n",
      "Batch [60/131]: Loss = -8.74258804321289\n",
      "Batch [61/131]: Loss = -22.399553298950195\n",
      "Batch [62/131]: Loss = -1019.3540649414062\n",
      "Batch [63/131]: Loss = 5.531877517700195\n",
      "Batch [64/131]: Loss = -12.276582717895508\n",
      "Batch [65/131]: Loss = -15.964094161987305\n",
      "Batch [66/131]: Loss = -24.922576904296875\n",
      "Batch [67/131]: Loss = -32.962345123291016\n",
      "Batch [68/131]: Loss = -92.4425277709961\n",
      "Batch [69/131]: Loss = 6.741390705108643\n",
      "Batch [70/131]: Loss = 2.7960033416748047\n",
      "Batch [71/131]: Loss = -6.644208908081055\n",
      "Batch [72/131]: Loss = -10.119562149047852\n",
      "Batch [73/131]: Loss = -7.115873336791992\n",
      "Batch [74/131]: Loss = -11.465776443481445\n",
      "Batch [75/131]: Loss = -8.951597213745117\n",
      "Batch [76/131]: Loss = -13.859455108642578\n",
      "Batch [77/131]: Loss = -15.12480354309082\n",
      "Batch [78/131]: Loss = -13.789910316467285\n",
      "Batch [79/131]: Loss = -25.073110580444336\n",
      "Batch [80/131]: Loss = -31.686128616333008\n",
      "Batch [81/131]: Loss = 134.57130432128906\n",
      "Batch [82/131]: Loss = 33.5929069519043\n",
      "Batch [83/131]: Loss = 13.959449768066406\n",
      "Batch [84/131]: Loss = 6.944786548614502\n",
      "Batch [85/131]: Loss = 3.5330610275268555\n",
      "Batch [86/131]: Loss = 1.9310235977172852\n",
      "Batch [87/131]: Loss = -0.6166958808898926\n",
      "Batch [88/131]: Loss = -0.8263678550720215\n",
      "Batch [89/131]: Loss = -1.9909214973449707\n",
      "Batch [90/131]: Loss = -3.061066150665283\n",
      "Batch [91/131]: Loss = -3.996094226837158\n",
      "Batch [92/131]: Loss = -3.957927703857422\n",
      "Batch [93/131]: Loss = -5.258517265319824\n",
      "Batch [94/131]: Loss = -4.486627578735352\n",
      "Batch [95/131]: Loss = -6.101327896118164\n",
      "Batch [96/131]: Loss = -5.4509477615356445\n",
      "Batch [97/131]: Loss = -5.821703910827637\n",
      "Batch [98/131]: Loss = -6.737605571746826\n",
      "Batch [99/131]: Loss = -7.626813888549805\n",
      "Batch [100/131]: Loss = -8.857882499694824\n",
      "Batch [101/131]: Loss = -8.190715789794922\n",
      "Batch [102/131]: Loss = -10.243059158325195\n",
      "Batch [103/131]: Loss = -12.783651351928711\n",
      "Batch [104/131]: Loss = -12.29452133178711\n",
      "Batch [105/131]: Loss = -23.459192276000977\n",
      "Batch [106/131]: Loss = -64.13645935058594\n",
      "Batch [107/131]: Loss = 14.701509475708008\n",
      "Batch [108/131]: Loss = 5.207747936248779\n",
      "Batch [109/131]: Loss = -0.6939964294433594\n",
      "Batch [110/131]: Loss = -0.8536416292190552\n",
      "Batch [111/131]: Loss = -6.270118713378906\n",
      "Batch [112/131]: Loss = -3.467491388320923\n",
      "Batch [113/131]: Loss = -3.0693869590759277\n",
      "Batch [114/131]: Loss = -5.667433738708496\n",
      "Batch [115/131]: Loss = -6.940889358520508\n",
      "Batch [116/131]: Loss = -8.32710075378418\n",
      "Batch [117/131]: Loss = -5.8339009284973145\n",
      "Batch [118/131]: Loss = -5.892609119415283\n",
      "Batch [119/131]: Loss = -7.167314529418945\n",
      "Batch [120/131]: Loss = -10.001768112182617\n",
      "Batch [121/131]: Loss = -8.665193557739258\n",
      "Batch [122/131]: Loss = -5.284356594085693\n",
      "Batch [123/131]: Loss = -9.54139232635498\n",
      "Batch [124/131]: Loss = -9.297710418701172\n",
      "Batch [125/131]: Loss = -9.685855865478516\n",
      "Batch [126/131]: Loss = -12.37357234954834\n",
      "Batch [127/131]: Loss = -9.266855239868164\n",
      "Batch [128/131]: Loss = -9.403213500976562\n",
      "Batch [129/131]: Loss = -9.643423080444336\n",
      "Batch [130/131]: Loss = -11.953926086425781\n",
      "Batch [131/131]: Loss = -31.88410758972168\n",
      "Epoch 3: Validation Loss = -0.9122463332282172\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 4/20\n",
      "Batch [1/131]: Loss = 285.0997619628906\n",
      "Batch [2/131]: Loss = 1.0447256565093994\n",
      "Batch [3/131]: Loss = 42.00117492675781\n",
      "Batch [4/131]: Loss = 6.55181360244751\n",
      "Batch [5/131]: Loss = 1.0474077463150024\n",
      "Batch [6/131]: Loss = -5.6647725105285645\n",
      "Batch [7/131]: Loss = 2.271428108215332\n",
      "Batch [8/131]: Loss = 3.7713463306427\n",
      "Batch [9/131]: Loss = 1.0111148357391357\n",
      "Batch [10/131]: Loss = -1.9632973670959473\n",
      "Batch [11/131]: Loss = -4.609501838684082\n",
      "Batch [12/131]: Loss = -6.028086185455322\n",
      "Batch [13/131]: Loss = -6.750118732452393\n",
      "Batch [14/131]: Loss = -5.82535982131958\n",
      "Batch [15/131]: Loss = -8.165987968444824\n",
      "Batch [16/131]: Loss = -7.269780158996582\n",
      "Batch [17/131]: Loss = -9.461094856262207\n",
      "Batch [18/131]: Loss = -13.12778377532959\n",
      "Batch [19/131]: Loss = -10.457243919372559\n",
      "Batch [20/131]: Loss = 145.61981201171875\n",
      "Batch [21/131]: Loss = -3.5411791801452637\n",
      "Batch [22/131]: Loss = -6.564184188842773\n",
      "Batch [23/131]: Loss = -7.804787635803223\n",
      "Batch [24/131]: Loss = -7.510874271392822\n",
      "Batch [25/131]: Loss = -8.110406875610352\n",
      "Batch [26/131]: Loss = -8.341995239257812\n",
      "Batch [27/131]: Loss = -8.59305191040039\n",
      "Batch [28/131]: Loss = -8.34034538269043\n",
      "Batch [29/131]: Loss = -9.652599334716797\n",
      "Batch [30/131]: Loss = -10.32328987121582\n",
      "Batch [31/131]: Loss = -10.888764381408691\n",
      "Batch [32/131]: Loss = -11.254155158996582\n",
      "Batch [33/131]: Loss = -13.25100040435791\n",
      "Batch [34/131]: Loss = -13.022819519042969\n",
      "Batch [35/131]: Loss = -15.353487014770508\n",
      "Batch [36/131]: Loss = -44.95926284790039\n",
      "Batch [37/131]: Loss = -130.60922241210938\n",
      "Batch [38/131]: Loss = 21.56680679321289\n",
      "Batch [39/131]: Loss = 10.910717964172363\n",
      "Batch [40/131]: Loss = -1.1865224838256836\n",
      "Batch [41/131]: Loss = 4.3835601806640625\n",
      "Batch [42/131]: Loss = -2.834756374359131\n",
      "Batch [43/131]: Loss = -3.2046070098876953\n",
      "Batch [44/131]: Loss = -4.459704399108887\n",
      "Batch [45/131]: Loss = -4.091761589050293\n",
      "Batch [46/131]: Loss = -5.534239768981934\n",
      "Batch [47/131]: Loss = -3.499277353286743\n",
      "Batch [48/131]: Loss = -7.081486701965332\n",
      "Batch [49/131]: Loss = -8.371131896972656\n",
      "Batch [50/131]: Loss = -7.34645414352417\n",
      "Batch [51/131]: Loss = -13.819307327270508\n",
      "Batch [52/131]: Loss = 3.875643253326416\n",
      "Batch [53/131]: Loss = -7.643074989318848\n",
      "Batch [54/131]: Loss = -6.178879737854004\n",
      "Batch [55/131]: Loss = -5.597399711608887\n",
      "Batch [56/131]: Loss = -6.122442722320557\n",
      "Batch [57/131]: Loss = -5.970973968505859\n",
      "Batch [58/131]: Loss = -8.21983528137207\n",
      "Batch [59/131]: Loss = -7.336901664733887\n",
      "Batch [60/131]: Loss = -7.391800880432129\n",
      "Batch [61/131]: Loss = -8.509992599487305\n",
      "Batch [62/131]: Loss = -8.470932960510254\n",
      "Batch [63/131]: Loss = -12.057268142700195\n",
      "Batch [64/131]: Loss = -11.790924072265625\n",
      "Batch [65/131]: Loss = -13.887182235717773\n",
      "Batch [66/131]: Loss = -16.65310287475586\n",
      "Batch [67/131]: Loss = 486.5398864746094\n",
      "Batch [68/131]: Loss = -10.313727378845215\n",
      "Batch [69/131]: Loss = 4.39541482925415\n",
      "Batch [70/131]: Loss = -9.165719985961914\n",
      "Batch [71/131]: Loss = -1.3606762886047363\n",
      "Batch [72/131]: Loss = -2.5240466594696045\n",
      "Batch [73/131]: Loss = -2.1667795181274414\n",
      "Batch [74/131]: Loss = -2.2612271308898926\n",
      "Batch [75/131]: Loss = -5.918184757232666\n",
      "Batch [76/131]: Loss = -3.679509401321411\n",
      "Batch [77/131]: Loss = -5.180448532104492\n",
      "Batch [78/131]: Loss = -4.442774772644043\n",
      "Batch [79/131]: Loss = -5.868818283081055\n",
      "Batch [80/131]: Loss = -5.461092472076416\n",
      "Batch [81/131]: Loss = -5.972951889038086\n",
      "Batch [82/131]: Loss = -6.772887229919434\n",
      "Batch [83/131]: Loss = -6.597349166870117\n",
      "Batch [84/131]: Loss = -6.651144981384277\n",
      "Batch [85/131]: Loss = -5.974505424499512\n",
      "Batch [86/131]: Loss = -7.631537437438965\n",
      "Batch [87/131]: Loss = -6.528960704803467\n",
      "Batch [88/131]: Loss = -7.086216449737549\n",
      "Batch [89/131]: Loss = -7.576607704162598\n",
      "Batch [90/131]: Loss = -8.174552917480469\n",
      "Batch [91/131]: Loss = -8.463763236999512\n",
      "Batch [92/131]: Loss = -7.699423313140869\n",
      "Batch [93/131]: Loss = -8.547709465026855\n",
      "Batch [94/131]: Loss = -9.43010139465332\n",
      "Batch [95/131]: Loss = -9.097129821777344\n",
      "Batch [96/131]: Loss = -8.030275344848633\n",
      "Batch [97/131]: Loss = -9.03481388092041\n",
      "Batch [98/131]: Loss = -8.982754707336426\n",
      "Batch [99/131]: Loss = -9.229745864868164\n",
      "Batch [100/131]: Loss = -10.523103713989258\n",
      "Batch [101/131]: Loss = -10.747602462768555\n",
      "Batch [102/131]: Loss = -9.482013702392578\n",
      "Batch [103/131]: Loss = -10.409420013427734\n",
      "Batch [104/131]: Loss = -12.765817642211914\n",
      "Batch [105/131]: Loss = -12.766485214233398\n",
      "Batch [106/131]: Loss = -12.26628589630127\n",
      "Batch [107/131]: Loss = -12.056451797485352\n",
      "Batch [108/131]: Loss = -14.446622848510742\n",
      "Batch [109/131]: Loss = -18.317480087280273\n",
      "Batch [110/131]: Loss = -17.378093719482422\n",
      "Batch [111/131]: Loss = -16.181339263916016\n",
      "Batch [112/131]: Loss = -19.065868377685547\n",
      "Batch [113/131]: Loss = -21.181806564331055\n",
      "Batch [114/131]: Loss = -28.254226684570312\n",
      "Batch [115/131]: Loss = -52.98148727416992\n",
      "Batch [116/131]: Loss = 77.21255493164062\n",
      "Batch [117/131]: Loss = -9.424924850463867\n",
      "Batch [118/131]: Loss = 23.699668884277344\n",
      "Batch [119/131]: Loss = 4.741630554199219\n",
      "Batch [120/131]: Loss = -0.01329183578491211\n",
      "Batch [121/131]: Loss = -0.9494625329971313\n",
      "Batch [122/131]: Loss = -3.6364455223083496\n",
      "Batch [123/131]: Loss = -4.694714069366455\n",
      "Batch [124/131]: Loss = -4.728687286376953\n",
      "Batch [125/131]: Loss = -6.368335723876953\n",
      "Batch [126/131]: Loss = -6.316803932189941\n",
      "Batch [127/131]: Loss = -7.833708763122559\n",
      "Batch [128/131]: Loss = -8.827765464782715\n",
      "Batch [129/131]: Loss = -8.759273529052734\n",
      "Batch [130/131]: Loss = -9.466222763061523\n",
      "Batch [131/131]: Loss = -13.917492866516113\n",
      "Epoch 4: Validation Loss = -9.280166202121311\n",
      "Validation loss improved: -9.0438 -> -9.2802\n",
      "Epoch 5/20\n",
      "Batch [1/131]: Loss = -26.41136360168457\n",
      "Batch [2/131]: Loss = -90.95918273925781\n",
      "Batch [3/131]: Loss = 16.98493194580078\n",
      "Batch [4/131]: Loss = 6.630460262298584\n",
      "Batch [5/131]: Loss = -3.732517719268799\n",
      "Batch [6/131]: Loss = -0.5575685501098633\n",
      "Batch [7/131]: Loss = -5.541297435760498\n",
      "Batch [8/131]: Loss = -6.672375679016113\n",
      "Batch [9/131]: Loss = -5.138942718505859\n",
      "Batch [10/131]: Loss = -5.772165298461914\n",
      "Batch [11/131]: Loss = -6.639143466949463\n",
      "Batch [12/131]: Loss = -6.83083438873291\n",
      "Batch [13/131]: Loss = -6.9797043800354\n",
      "Batch [14/131]: Loss = -7.842817783355713\n",
      "Batch [15/131]: Loss = -7.942461013793945\n",
      "Batch [16/131]: Loss = -8.771800994873047\n",
      "Batch [17/131]: Loss = -10.926887512207031\n",
      "Batch [18/131]: Loss = -12.701109886169434\n",
      "Batch [19/131]: Loss = -12.04377555847168\n",
      "Batch [20/131]: Loss = -17.355716705322266\n",
      "Batch [21/131]: Loss = -31.751754760742188\n",
      "Batch [22/131]: Loss = 38.47653579711914\n",
      "Batch [23/131]: Loss = 6.2733306884765625\n",
      "Batch [24/131]: Loss = 2.900345802307129\n",
      "Batch [25/131]: Loss = -0.8853573799133301\n",
      "Batch [26/131]: Loss = -2.0807151794433594\n",
      "Batch [27/131]: Loss = -3.9587016105651855\n",
      "Batch [28/131]: Loss = -3.8040740489959717\n",
      "Batch [29/131]: Loss = -5.694492340087891\n",
      "Batch [30/131]: Loss = -5.781472206115723\n",
      "Batch [31/131]: Loss = -7.480810642242432\n",
      "Batch [32/131]: Loss = -9.60358715057373\n",
      "Batch [33/131]: Loss = -10.24809455871582\n",
      "Batch [34/131]: Loss = -14.373929977416992\n",
      "Batch [35/131]: Loss = -14.550522804260254\n",
      "Batch [36/131]: Loss = -13.19639778137207\n",
      "Batch [37/131]: Loss = -2.75272798538208\n",
      "Batch [38/131]: Loss = -9.41281795501709\n",
      "Batch [39/131]: Loss = -11.76739501953125\n",
      "Batch [40/131]: Loss = -35.78478240966797\n",
      "Batch [41/131]: Loss = 7.117990016937256\n",
      "Batch [42/131]: Loss = -4.860795497894287\n",
      "Batch [43/131]: Loss = 0.32462170720100403\n",
      "Batch [44/131]: Loss = -3.1993157863616943\n",
      "Batch [45/131]: Loss = -7.633744239807129\n",
      "Batch [46/131]: Loss = -8.66811752319336\n",
      "Batch [47/131]: Loss = -8.72840690612793\n",
      "Batch [48/131]: Loss = -9.280776977539062\n",
      "Batch [49/131]: Loss = -15.585033416748047\n",
      "Batch [50/131]: Loss = -2.330099582672119\n",
      "Batch [51/131]: Loss = -11.555057525634766\n",
      "Batch [52/131]: Loss = -3.4414520263671875\n",
      "Batch [53/131]: Loss = -4.627001762390137\n",
      "Batch [54/131]: Loss = -5.190155982971191\n",
      "Batch [55/131]: Loss = -4.744140148162842\n",
      "Batch [56/131]: Loss = -5.309021472930908\n",
      "Batch [57/131]: Loss = -5.970448017120361\n",
      "Batch [58/131]: Loss = -7.397093772888184\n",
      "Batch [59/131]: Loss = -6.071465015411377\n",
      "Batch [60/131]: Loss = -8.285707473754883\n",
      "Batch [61/131]: Loss = -8.053912162780762\n",
      "Batch [62/131]: Loss = -8.311460494995117\n",
      "Batch [63/131]: Loss = -9.148109436035156\n",
      "Batch [64/131]: Loss = -9.056623458862305\n",
      "Batch [65/131]: Loss = -10.317136764526367\n",
      "Batch [66/131]: Loss = -12.510881423950195\n",
      "Batch [67/131]: Loss = -13.062519073486328\n",
      "Batch [68/131]: Loss = -40.6676025390625\n",
      "Batch [69/131]: Loss = 12.711431503295898\n",
      "Batch [70/131]: Loss = 1.365332007408142\n",
      "Batch [71/131]: Loss = -4.936897277832031\n",
      "Batch [72/131]: Loss = -4.919641017913818\n",
      "Batch [73/131]: Loss = -7.202651023864746\n",
      "Batch [74/131]: Loss = -9.29633617401123\n",
      "Batch [75/131]: Loss = -10.927645683288574\n",
      "Batch [76/131]: Loss = -11.695170402526855\n",
      "Batch [77/131]: Loss = -18.365449905395508\n",
      "Batch [78/131]: Loss = -854.0048217773438\n",
      "Batch [79/131]: Loss = 8.926508903503418\n",
      "Batch [80/131]: Loss = -3.595629930496216\n",
      "Batch [81/131]: Loss = -3.5808897018432617\n",
      "Batch [82/131]: Loss = 42.13311767578125\n",
      "Batch [83/131]: Loss = -2.048475503921509\n",
      "Batch [84/131]: Loss = -2.336679697036743\n",
      "Batch [85/131]: Loss = -4.760913848876953\n",
      "Batch [86/131]: Loss = -4.8249664306640625\n",
      "Batch [87/131]: Loss = -4.199016094207764\n",
      "Batch [88/131]: Loss = -3.4709317684173584\n",
      "Batch [89/131]: Loss = -5.432690143585205\n",
      "Batch [90/131]: Loss = -5.44539213180542\n",
      "Batch [91/131]: Loss = -6.474717140197754\n",
      "Batch [92/131]: Loss = -6.266683101654053\n",
      "Batch [93/131]: Loss = -6.9146013259887695\n",
      "Batch [94/131]: Loss = -7.8902363777160645\n",
      "Batch [95/131]: Loss = -7.337591648101807\n",
      "Batch [96/131]: Loss = -4.79814338684082\n",
      "Batch [97/131]: Loss = -3.7118582725524902\n",
      "Batch [98/131]: Loss = -7.784620761871338\n",
      "Batch [99/131]: Loss = -6.141632080078125\n",
      "Batch [100/131]: Loss = -5.886064529418945\n",
      "Batch [101/131]: Loss = -9.328079223632812\n",
      "Batch [102/131]: Loss = -8.96721363067627\n",
      "Batch [103/131]: Loss = -11.799169540405273\n",
      "Batch [104/131]: Loss = -9.245540618896484\n",
      "Batch [105/131]: Loss = -10.330322265625\n",
      "Batch [106/131]: Loss = -12.42436408996582\n",
      "Batch [107/131]: Loss = -32.79615020751953\n",
      "Batch [108/131]: Loss = 14.814602851867676\n",
      "Batch [109/131]: Loss = -5.4024128913879395\n",
      "Batch [110/131]: Loss = -6.924755096435547\n",
      "Batch [111/131]: Loss = -4.708714485168457\n",
      "Batch [112/131]: Loss = -5.44108772277832\n",
      "Batch [113/131]: Loss = -6.063196659088135\n",
      "Batch [114/131]: Loss = -7.515831470489502\n",
      "Batch [115/131]: Loss = -7.941950798034668\n",
      "Batch [116/131]: Loss = -7.664428234100342\n",
      "Batch [117/131]: Loss = -8.022542953491211\n",
      "Batch [118/131]: Loss = -8.25943660736084\n",
      "Batch [119/131]: Loss = -9.501240730285645\n",
      "Batch [120/131]: Loss = -9.104684829711914\n",
      "Batch [121/131]: Loss = -10.147775650024414\n",
      "Batch [122/131]: Loss = -10.261292457580566\n",
      "Batch [123/131]: Loss = -11.184560775756836\n",
      "Batch [124/131]: Loss = -12.445286750793457\n",
      "Batch [125/131]: Loss = -13.61560344696045\n",
      "Batch [126/131]: Loss = -13.883325576782227\n",
      "Batch [127/131]: Loss = -11.438220024108887\n",
      "Batch [128/131]: Loss = -14.202193260192871\n",
      "Batch [129/131]: Loss = -12.571001052856445\n",
      "Batch [130/131]: Loss = -20.651165008544922\n",
      "Batch [131/131]: Loss = -28.337352752685547\n",
      "Epoch 5: Validation Loss = -4.6812880171669855\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 6/20\n",
      "Batch [1/131]: Loss = 3.1989030838012695\n",
      "Batch [2/131]: Loss = 13.150918006896973\n",
      "Batch [3/131]: Loss = -4.061728477478027\n",
      "Batch [4/131]: Loss = -29.256649017333984\n",
      "Batch [5/131]: Loss = 82.41976928710938\n",
      "Batch [6/131]: Loss = 15.685734748840332\n",
      "Batch [7/131]: Loss = -12.918235778808594\n",
      "Batch [8/131]: Loss = -39.00706100463867\n",
      "Batch [9/131]: Loss = 514.4751586914062\n",
      "Batch [10/131]: Loss = 32.838706970214844\n",
      "Batch [11/131]: Loss = 87.362548828125\n",
      "Batch [12/131]: Loss = 11.497222900390625\n",
      "Batch [13/131]: Loss = 7.182981491088867\n",
      "Batch [14/131]: Loss = 4.783321857452393\n",
      "Batch [15/131]: Loss = 0.7145696878433228\n",
      "Batch [16/131]: Loss = 1.6702752113342285\n",
      "Batch [17/131]: Loss = -2.0342679023742676\n",
      "Batch [18/131]: Loss = -1.909741759300232\n",
      "Batch [19/131]: Loss = -2.4062423706054688\n",
      "Batch [20/131]: Loss = -3.543710470199585\n",
      "Batch [21/131]: Loss = -4.425463676452637\n",
      "Batch [22/131]: Loss = -3.8924338817596436\n",
      "Batch [23/131]: Loss = -4.740981578826904\n",
      "Batch [24/131]: Loss = -5.681276798248291\n",
      "Batch [25/131]: Loss = -4.994970321655273\n",
      "Batch [26/131]: Loss = -7.76350736618042\n",
      "Batch [27/131]: Loss = -6.248895168304443\n",
      "Batch [28/131]: Loss = -7.033780574798584\n",
      "Batch [29/131]: Loss = -8.196996688842773\n",
      "Batch [30/131]: Loss = -7.8007893562316895\n",
      "Batch [31/131]: Loss = -8.097162246704102\n",
      "Batch [32/131]: Loss = -8.1746826171875\n",
      "Batch [33/131]: Loss = -8.72644329071045\n",
      "Batch [34/131]: Loss = -9.412323951721191\n",
      "Batch [35/131]: Loss = -10.122872352600098\n",
      "Batch [36/131]: Loss = -9.380277633666992\n",
      "Batch [37/131]: Loss = -10.055329322814941\n",
      "Batch [38/131]: Loss = -10.1707124710083\n",
      "Batch [39/131]: Loss = -11.841150283813477\n",
      "Batch [40/131]: Loss = -11.794025421142578\n",
      "Batch [41/131]: Loss = -11.266998291015625\n",
      "Batch [42/131]: Loss = -13.159762382507324\n",
      "Batch [43/131]: Loss = -17.005142211914062\n",
      "Batch [44/131]: Loss = -45.5480842590332\n",
      "Batch [45/131]: Loss = -114.33892059326172\n",
      "Batch [46/131]: Loss = 11.600772857666016\n",
      "Batch [47/131]: Loss = 2.5887045860290527\n",
      "Batch [48/131]: Loss = -1.1058931350708008\n",
      "Batch [49/131]: Loss = -2.943899631500244\n",
      "Batch [50/131]: Loss = -3.871335029602051\n",
      "Batch [51/131]: Loss = -3.8681442737579346\n",
      "Batch [52/131]: Loss = -4.711063385009766\n",
      "Batch [53/131]: Loss = -5.649370193481445\n",
      "Batch [54/131]: Loss = -6.421884536743164\n",
      "Batch [55/131]: Loss = -7.643097400665283\n",
      "Batch [56/131]: Loss = -9.216008186340332\n",
      "Batch [57/131]: Loss = -12.983535766601562\n",
      "Batch [58/131]: Loss = -34.944793701171875\n",
      "Batch [59/131]: Loss = 27.28273582458496\n",
      "Batch [60/131]: Loss = 10.222640037536621\n",
      "Batch [61/131]: Loss = -0.7081003189086914\n",
      "Batch [62/131]: Loss = -2.1550965309143066\n",
      "Batch [63/131]: Loss = -2.5508651733398438\n",
      "Batch [64/131]: Loss = -3.8790087699890137\n",
      "Batch [65/131]: Loss = -4.369555473327637\n",
      "Batch [66/131]: Loss = -5.919609069824219\n",
      "Batch [67/131]: Loss = -6.8454484939575195\n",
      "Batch [68/131]: Loss = -7.227194786071777\n",
      "Batch [69/131]: Loss = -7.486042499542236\n",
      "Batch [70/131]: Loss = -7.355566501617432\n",
      "Batch [71/131]: Loss = -7.94938850402832\n",
      "Batch [72/131]: Loss = -8.218426704406738\n",
      "Batch [73/131]: Loss = -7.855894088745117\n",
      "Batch [74/131]: Loss = -9.48621940612793\n",
      "Batch [75/131]: Loss = -10.561820983886719\n",
      "Batch [76/131]: Loss = -10.539020538330078\n",
      "Batch [77/131]: Loss = -9.073188781738281\n",
      "Batch [78/131]: Loss = -10.48459243774414\n",
      "Batch [79/131]: Loss = -11.723971366882324\n",
      "Batch [80/131]: Loss = -9.51852798461914\n",
      "Batch [81/131]: Loss = -12.186695098876953\n",
      "Batch [82/131]: Loss = -14.517491340637207\n",
      "Batch [83/131]: Loss = -14.323912620544434\n",
      "Batch [84/131]: Loss = -14.40526008605957\n",
      "Batch [85/131]: Loss = -19.242998123168945\n",
      "Batch [86/131]: Loss = -20.74259376525879\n",
      "Batch [87/131]: Loss = -29.69487762451172\n",
      "Batch [88/131]: Loss = -44.322322845458984\n",
      "Batch [89/131]: Loss = -104.60944366455078\n",
      "Batch [90/131]: Loss = 76.70784759521484\n",
      "Batch [91/131]: Loss = 10.626531600952148\n",
      "Batch [92/131]: Loss = 8.84575080871582\n",
      "Batch [93/131]: Loss = -0.9386754035949707\n",
      "Batch [94/131]: Loss = -3.010610818862915\n",
      "Batch [95/131]: Loss = -3.6207151412963867\n",
      "Batch [96/131]: Loss = -4.457730770111084\n",
      "Batch [97/131]: Loss = -5.319206714630127\n",
      "Batch [98/131]: Loss = -5.816926002502441\n",
      "Batch [99/131]: Loss = -5.735531806945801\n",
      "Batch [100/131]: Loss = -6.081754207611084\n",
      "Batch [101/131]: Loss = -6.727952480316162\n",
      "Batch [102/131]: Loss = -6.47213077545166\n",
      "Batch [103/131]: Loss = -6.700876712799072\n",
      "Batch [104/131]: Loss = -6.734816551208496\n",
      "Batch [105/131]: Loss = -7.484589099884033\n",
      "Batch [106/131]: Loss = -8.084855079650879\n",
      "Batch [107/131]: Loss = -7.322571754455566\n",
      "Batch [108/131]: Loss = -7.944586277008057\n",
      "Batch [109/131]: Loss = -7.9504241943359375\n",
      "Batch [110/131]: Loss = -9.75481128692627\n",
      "Batch [111/131]: Loss = -9.455731391906738\n",
      "Batch [112/131]: Loss = -9.768327713012695\n",
      "Batch [113/131]: Loss = -11.741169929504395\n",
      "Batch [114/131]: Loss = -9.784330368041992\n",
      "Batch [115/131]: Loss = -11.407496452331543\n",
      "Batch [116/131]: Loss = -12.875679016113281\n",
      "Batch [117/131]: Loss = -12.993223190307617\n",
      "Batch [118/131]: Loss = -15.589411735534668\n",
      "Batch [119/131]: Loss = -18.943527221679688\n",
      "Batch [120/131]: Loss = -12.99573802947998\n",
      "Batch [121/131]: Loss = -70.9923095703125\n",
      "Batch [122/131]: Loss = -1274.3822021484375\n",
      "Batch [123/131]: Loss = 51.882286071777344\n",
      "Batch [124/131]: Loss = 21.22410774230957\n",
      "Batch [125/131]: Loss = 10.339410781860352\n",
      "Batch [126/131]: Loss = 5.768403053283691\n",
      "Batch [127/131]: Loss = 1.7535510063171387\n",
      "Batch [128/131]: Loss = -5.854881763458252\n",
      "Batch [129/131]: Loss = 0.2871837615966797\n",
      "Batch [130/131]: Loss = -3.4746084213256836\n",
      "Batch [131/131]: Loss = 0.7925190925598145\n",
      "Epoch 6: Validation Loss = -306.43472099635335\n",
      "Validation loss improved: -9.2802 -> -306.4347\n",
      "Epoch 7/20\n",
      "Batch [1/131]: Loss = -1.3664417266845703\n",
      "Batch [2/131]: Loss = -3.4665884971618652\n",
      "Batch [3/131]: Loss = -5.198118686676025\n",
      "Batch [4/131]: Loss = -5.9376678466796875\n",
      "Batch [5/131]: Loss = -5.719977378845215\n",
      "Batch [6/131]: Loss = -11.900604248046875\n",
      "Batch [7/131]: Loss = -8.538520812988281\n",
      "Batch [8/131]: Loss = -8.436033248901367\n",
      "Batch [9/131]: Loss = -9.729826927185059\n",
      "Batch [10/131]: Loss = -13.165108680725098\n",
      "Batch [11/131]: Loss = -10.042881965637207\n",
      "Batch [12/131]: Loss = -13.188338279724121\n",
      "Batch [13/131]: Loss = -13.219589233398438\n",
      "Batch [14/131]: Loss = -14.354129791259766\n",
      "Batch [15/131]: Loss = -15.808462142944336\n",
      "Batch [16/131]: Loss = -13.759795188903809\n",
      "Batch [17/131]: Loss = -14.199567794799805\n",
      "Batch [18/131]: Loss = -26.90422821044922\n",
      "Batch [19/131]: Loss = -2012.982421875\n",
      "Batch [20/131]: Loss = -3.4862189292907715\n",
      "Batch [21/131]: Loss = -8.290040016174316\n",
      "Batch [22/131]: Loss = -9.528059959411621\n",
      "Batch [23/131]: Loss = -9.675058364868164\n",
      "Batch [24/131]: Loss = -11.33438491821289\n",
      "Batch [25/131]: Loss = -12.973957061767578\n",
      "Batch [26/131]: Loss = -17.489940643310547\n",
      "Batch [27/131]: Loss = -15.75934886932373\n",
      "Batch [28/131]: Loss = -30.581100463867188\n",
      "Batch [29/131]: Loss = -51.57734680175781\n",
      "Batch [30/131]: Loss = 407.9309387207031\n",
      "Batch [31/131]: Loss = 7.879533290863037\n",
      "Batch [32/131]: Loss = 12.326927185058594\n",
      "Batch [33/131]: Loss = -0.12464240193367004\n",
      "Batch [34/131]: Loss = 1.4071779251098633\n",
      "Batch [35/131]: Loss = -1.9007573127746582\n",
      "Batch [36/131]: Loss = -3.6011159420013428\n",
      "Batch [37/131]: Loss = -3.4575719833374023\n",
      "Batch [38/131]: Loss = -6.896686553955078\n",
      "Batch [39/131]: Loss = -6.166744232177734\n",
      "Batch [40/131]: Loss = -9.330629348754883\n",
      "Batch [41/131]: Loss = -11.016942024230957\n",
      "Batch [42/131]: Loss = -10.575510025024414\n",
      "Batch [43/131]: Loss = -12.164201736450195\n",
      "Batch [44/131]: Loss = -12.508962631225586\n",
      "Batch [45/131]: Loss = -15.759835243225098\n",
      "Batch [46/131]: Loss = -23.535614013671875\n",
      "Batch [47/131]: Loss = -57.78023910522461\n",
      "Batch [48/131]: Loss = 18.638898849487305\n",
      "Batch [49/131]: Loss = 2.0152549743652344\n",
      "Batch [50/131]: Loss = 0.5183150768280029\n",
      "Batch [51/131]: Loss = -3.728259801864624\n",
      "Batch [52/131]: Loss = -5.550626754760742\n",
      "Batch [53/131]: Loss = -9.351115226745605\n",
      "Batch [54/131]: Loss = -13.013215065002441\n",
      "Batch [55/131]: Loss = -12.844454765319824\n",
      "Batch [56/131]: Loss = -15.70347785949707\n",
      "Batch [57/131]: Loss = -19.17657470703125\n",
      "Batch [58/131]: Loss = -20.130916595458984\n",
      "Batch [59/131]: Loss = -25.622459411621094\n",
      "Batch [60/131]: Loss = -26.613758087158203\n",
      "Batch [61/131]: Loss = -32.89696502685547\n",
      "Batch [62/131]: Loss = -69.12901306152344\n",
      "Batch [63/131]: Loss = 100.43205261230469\n",
      "Batch [64/131]: Loss = -4.252835273742676\n",
      "Batch [65/131]: Loss = -8.559515953063965\n",
      "Batch [66/131]: Loss = -8.378374099731445\n",
      "Batch [67/131]: Loss = -11.520748138427734\n",
      "Batch [68/131]: Loss = -27.602291107177734\n",
      "Batch [69/131]: Loss = 3.387449264526367\n",
      "Batch [70/131]: Loss = 3.917161464691162\n",
      "Batch [71/131]: Loss = -0.5324115753173828\n",
      "Batch [72/131]: Loss = -2.3698010444641113\n",
      "Batch [73/131]: Loss = -6.729971885681152\n",
      "Batch [74/131]: Loss = -8.934724807739258\n",
      "Batch [75/131]: Loss = -10.728011131286621\n",
      "Batch [76/131]: Loss = -13.952473640441895\n",
      "Batch [77/131]: Loss = -25.936622619628906\n",
      "Batch [78/131]: Loss = -24.332860946655273\n",
      "Batch [79/131]: Loss = -2104.15380859375\n",
      "Batch [80/131]: Loss = 38.55923843383789\n",
      "Batch [81/131]: Loss = 6.885110855102539\n",
      "Batch [82/131]: Loss = 5.893760681152344\n",
      "Batch [83/131]: Loss = 0.30032622814178467\n",
      "Batch [84/131]: Loss = 0.565093994140625\n",
      "Batch [85/131]: Loss = -5.764835834503174\n",
      "Batch [86/131]: Loss = -5.544343948364258\n",
      "Batch [87/131]: Loss = -13.188821792602539\n",
      "Batch [88/131]: Loss = -3.5414092540740967\n",
      "Batch [89/131]: Loss = -9.547795295715332\n",
      "Batch [90/131]: Loss = -13.124134063720703\n",
      "Batch [91/131]: Loss = -26.014053344726562\n",
      "Batch [92/131]: Loss = 172.42462158203125\n",
      "Batch [93/131]: Loss = -3.461941719055176\n",
      "Batch [94/131]: Loss = -5.783299446105957\n",
      "Batch [95/131]: Loss = -10.19862174987793\n",
      "Batch [96/131]: Loss = -7.474368572235107\n",
      "Batch [97/131]: Loss = 47.155967712402344\n",
      "Batch [98/131]: Loss = -7.991153717041016\n",
      "Batch [99/131]: Loss = -11.14870834350586\n",
      "Batch [100/131]: Loss = -3.3693437576293945\n",
      "Batch [101/131]: Loss = -126.49684143066406\n",
      "Batch [102/131]: Loss = -13.020991325378418\n",
      "Batch [103/131]: Loss = -9.241521835327148\n",
      "Batch [104/131]: Loss = -70.03765106201172\n",
      "Batch [105/131]: Loss = -45.16160202026367\n",
      "Batch [106/131]: Loss = -10.240852355957031\n",
      "Batch [107/131]: Loss = 35.04123306274414\n",
      "Batch [108/131]: Loss = 8.424625396728516\n",
      "Batch [109/131]: Loss = 58.18689727783203\n",
      "Batch [110/131]: Loss = 8.20419979095459\n",
      "Batch [111/131]: Loss = -52.17223358154297\n",
      "Batch [112/131]: Loss = -0.8567171096801758\n",
      "Batch [113/131]: Loss = 2.114382743835449\n",
      "Batch [114/131]: Loss = 0.4432334899902344\n",
      "Batch [115/131]: Loss = -1.3422417640686035\n",
      "Batch [116/131]: Loss = -2.5198864936828613\n",
      "Batch [117/131]: Loss = -4.857661247253418\n",
      "Batch [118/131]: Loss = -4.018479347229004\n",
      "Batch [119/131]: Loss = -7.317171096801758\n",
      "Batch [120/131]: Loss = -10.092374801635742\n",
      "Batch [121/131]: Loss = -7.777132034301758\n",
      "Batch [122/131]: Loss = -5.340895175933838\n",
      "Batch [123/131]: Loss = -11.384317398071289\n",
      "Batch [124/131]: Loss = -7.584222793579102\n",
      "Batch [125/131]: Loss = -8.197896003723145\n",
      "Batch [126/131]: Loss = -8.422287940979004\n",
      "Batch [127/131]: Loss = -6.482356071472168\n",
      "Batch [128/131]: Loss = -11.262593269348145\n",
      "Batch [129/131]: Loss = -14.8572998046875\n",
      "Batch [130/131]: Loss = -15.136473655700684\n",
      "Batch [131/131]: Loss = -14.858759880065918\n",
      "Epoch 7: Validation Loss = -5.901968326833513\n",
      "Validation loss did not improve for 1 epochs.\n",
      "Epoch 8/20\n",
      "Batch [1/131]: Loss = -9.575651168823242\n",
      "Batch [2/131]: Loss = -16.594715118408203\n",
      "Batch [3/131]: Loss = -10.870845794677734\n",
      "Batch [4/131]: Loss = -16.81035041809082\n",
      "Batch [5/131]: Loss = -13.229595184326172\n",
      "Batch [6/131]: Loss = -10.457076072692871\n",
      "Batch [7/131]: Loss = -16.11374855041504\n",
      "Batch [8/131]: Loss = -10.693711280822754\n",
      "Batch [9/131]: Loss = -7.534432411193848\n",
      "Batch [10/131]: Loss = -27.35924530029297\n",
      "Batch [11/131]: Loss = -47.87037658691406\n",
      "Batch [12/131]: Loss = 33.53679275512695\n",
      "Batch [13/131]: Loss = 10.094415664672852\n",
      "Batch [14/131]: Loss = 4.242313385009766\n",
      "Batch [15/131]: Loss = -2.0898730754852295\n",
      "Batch [16/131]: Loss = -1.0705394744873047\n",
      "Batch [17/131]: Loss = -5.472445964813232\n",
      "Batch [18/131]: Loss = -6.5789079666137695\n",
      "Batch [19/131]: Loss = -15.64277458190918\n",
      "Batch [20/131]: Loss = -4.7165327072143555\n",
      "Batch [21/131]: Loss = -6.617871284484863\n",
      "Batch [22/131]: Loss = -7.979323387145996\n",
      "Batch [23/131]: Loss = -9.011917114257812\n",
      "Batch [24/131]: Loss = -9.536725997924805\n",
      "Batch [25/131]: Loss = -11.512960433959961\n",
      "Batch [26/131]: Loss = -10.942483901977539\n",
      "Batch [27/131]: Loss = -12.573848724365234\n",
      "Batch [28/131]: Loss = -13.34090518951416\n",
      "Batch [29/131]: Loss = -14.203718185424805\n",
      "Batch [30/131]: Loss = -16.424274444580078\n",
      "Batch [31/131]: Loss = -18.95377540588379\n",
      "Batch [32/131]: Loss = -21.96164894104004\n",
      "Batch [33/131]: Loss = -59.753692626953125\n",
      "Batch [34/131]: Loss = 5.384675979614258\n",
      "Batch [35/131]: Loss = -12.821008682250977\n",
      "Batch [36/131]: Loss = -328.5660095214844\n",
      "Batch [37/131]: Loss = 10.322635650634766\n",
      "Batch [38/131]: Loss = 1.96976900100708\n",
      "Batch [39/131]: Loss = 0.5782794952392578\n",
      "Batch [40/131]: Loss = 1.0666694641113281\n",
      "Batch [41/131]: Loss = -0.20081567764282227\n",
      "Batch [42/131]: Loss = -1.0063621997833252\n",
      "Batch [43/131]: Loss = -0.7318716049194336\n",
      "Batch [44/131]: Loss = -2.723548412322998\n",
      "Batch [45/131]: Loss = -3.0921120643615723\n",
      "Batch [46/131]: Loss = -3.6279520988464355\n",
      "Batch [47/131]: Loss = -4.8507890701293945\n",
      "Batch [48/131]: Loss = -3.6277570724487305\n",
      "Batch [49/131]: Loss = -8.025161743164062\n",
      "Batch [50/131]: Loss = -7.1158599853515625\n",
      "Batch [51/131]: Loss = -3.7574682235717773\n",
      "Batch [52/131]: Loss = -10.261831283569336\n",
      "Batch [53/131]: Loss = -9.302558898925781\n",
      "Batch [54/131]: Loss = -6.507357597351074\n",
      "Batch [55/131]: Loss = -11.043335914611816\n",
      "Batch [56/131]: Loss = -8.53853702545166\n",
      "Batch [57/131]: Loss = -9.357699394226074\n",
      "Batch [58/131]: Loss = -14.925431251525879\n",
      "Batch [59/131]: Loss = -7.425378799438477\n",
      "Batch [60/131]: Loss = -15.061529159545898\n",
      "Batch [61/131]: Loss = -13.606717109680176\n",
      "Batch [62/131]: Loss = -16.34698486328125\n",
      "Batch [63/131]: Loss = -13.070040702819824\n",
      "Batch [64/131]: Loss = -16.520788192749023\n",
      "Batch [65/131]: Loss = -12.829830169677734\n",
      "Batch [66/131]: Loss = -35.07390594482422\n",
      "Batch [67/131]: Loss = -1069.2122802734375\n",
      "Batch [68/131]: Loss = 50.89934539794922\n",
      "Batch [69/131]: Loss = 8.312881469726562\n",
      "Batch [70/131]: Loss = -0.7573127746582031\n",
      "Batch [71/131]: Loss = -5.610449314117432\n",
      "Batch [72/131]: Loss = -6.33231258392334\n",
      "Batch [73/131]: Loss = -10.508960723876953\n",
      "Batch [74/131]: Loss = -33.664222717285156\n",
      "Batch [75/131]: Loss = -83.59481048583984\n",
      "Batch [76/131]: Loss = 18.623865127563477\n",
      "Batch [77/131]: Loss = 3.051041603088379\n",
      "Batch [78/131]: Loss = -8.188230514526367\n",
      "Batch [79/131]: Loss = -6.858907699584961\n",
      "Batch [80/131]: Loss = -11.287313461303711\n",
      "Batch [81/131]: Loss = -9.459117889404297\n",
      "Batch [82/131]: Loss = -12.398506164550781\n",
      "Batch [83/131]: Loss = -2.2081620693206787\n",
      "Batch [84/131]: Loss = -12.929768562316895\n",
      "Batch [85/131]: Loss = -12.554591178894043\n",
      "Batch [86/131]: Loss = -13.91677188873291\n",
      "Batch [87/131]: Loss = -14.962973594665527\n",
      "Batch [88/131]: Loss = -7.070579528808594\n",
      "Batch [89/131]: Loss = -12.156228065490723\n",
      "Batch [90/131]: Loss = -15.987983703613281\n",
      "Batch [91/131]: Loss = -17.13939094543457\n",
      "Batch [92/131]: Loss = 7.7816033363342285\n",
      "Batch [93/131]: Loss = 10.651593208312988\n",
      "Batch [94/131]: Loss = 3.3588078022003174\n",
      "Batch [95/131]: Loss = -2.0834245681762695\n",
      "Batch [96/131]: Loss = -4.852153778076172\n",
      "Batch [97/131]: Loss = -7.111307144165039\n",
      "Batch [98/131]: Loss = -7.669993877410889\n",
      "Batch [99/131]: Loss = -49.376590728759766\n",
      "Batch [100/131]: Loss = 3.2601523399353027\n",
      "Batch [101/131]: Loss = 0.04851818084716797\n",
      "Batch [102/131]: Loss = -1.4481334686279297\n",
      "Batch [103/131]: Loss = -0.9087677001953125\n",
      "Batch [104/131]: Loss = -1.654282569885254\n",
      "Batch [105/131]: Loss = -2.1755592823028564\n",
      "Batch [106/131]: Loss = -3.2516684532165527\n",
      "Batch [107/131]: Loss = -5.482149124145508\n",
      "Batch [108/131]: Loss = -3.6696653366088867\n",
      "Batch [109/131]: Loss = -5.191812515258789\n",
      "Batch [110/131]: Loss = -7.652401924133301\n",
      "Batch [111/131]: Loss = -13.182038307189941\n",
      "Batch [112/131]: Loss = 24.734926223754883\n",
      "Batch [113/131]: Loss = 20.73114585876465\n",
      "Batch [114/131]: Loss = 9.276018142700195\n",
      "Batch [115/131]: Loss = 12.004011154174805\n",
      "Batch [116/131]: Loss = -1.2026395797729492\n",
      "Batch [117/131]: Loss = -5.28682804107666\n",
      "Batch [118/131]: Loss = -8.249893188476562\n",
      "Batch [119/131]: Loss = -12.21732234954834\n",
      "Batch [120/131]: Loss = 410.7419738769531\n",
      "Batch [121/131]: Loss = 255.02630615234375\n",
      "Batch [122/131]: Loss = 34.8231086730957\n",
      "Batch [123/131]: Loss = -0.5490570068359375\n",
      "Batch [124/131]: Loss = -2.768869161605835\n",
      "Batch [125/131]: Loss = 1.5219523906707764\n",
      "Batch [126/131]: Loss = -12.604394912719727\n",
      "Batch [127/131]: Loss = -8.662384033203125\n",
      "Batch [128/131]: Loss = -8.088579177856445\n",
      "Batch [129/131]: Loss = -19.226543426513672\n",
      "Batch [130/131]: Loss = -20.695676803588867\n",
      "Batch [131/131]: Loss = -32.29523468017578\n",
      "Epoch 8: Validation Loss = -7.945633835262722\n",
      "Validation loss did not improve for 2 epochs.\n",
      "Epoch 9/20\n",
      "Batch [1/131]: Loss = 283.8258056640625\n",
      "Batch [2/131]: Loss = 3.6564135551452637\n",
      "Batch [3/131]: Loss = -0.572369396686554\n",
      "Batch [4/131]: Loss = -0.7670303583145142\n",
      "Batch [5/131]: Loss = -4.617766380310059\n",
      "Batch [6/131]: Loss = -4.4054718017578125\n",
      "Batch [7/131]: Loss = -4.758219242095947\n",
      "Batch [8/131]: Loss = -5.509932994842529\n",
      "Batch [9/131]: Loss = -6.272947788238525\n",
      "Batch [10/131]: Loss = -7.63658332824707\n",
      "Batch [11/131]: Loss = -8.495067596435547\n",
      "Batch [12/131]: Loss = -1.5998494625091553\n",
      "Batch [13/131]: Loss = -2.784862995147705\n",
      "Batch [14/131]: Loss = -5.218138694763184\n",
      "Batch [15/131]: Loss = -4.511595726013184\n",
      "Batch [16/131]: Loss = -4.618402481079102\n",
      "Batch [17/131]: Loss = -5.416659832000732\n",
      "Batch [18/131]: Loss = -6.5045857429504395\n",
      "Batch [19/131]: Loss = -7.816638469696045\n",
      "Batch [20/131]: Loss = -10.030567169189453\n",
      "Batch [21/131]: Loss = -9.485077857971191\n",
      "Batch [22/131]: Loss = -11.721149444580078\n",
      "Batch [23/131]: Loss = -10.417211532592773\n",
      "Batch [24/131]: Loss = -11.642337799072266\n",
      "Batch [25/131]: Loss = -13.886053085327148\n",
      "Batch [26/131]: Loss = -15.282408714294434\n",
      "Batch [27/131]: Loss = -17.086585998535156\n",
      "Batch [28/131]: Loss = -17.640750885009766\n",
      "Batch [29/131]: Loss = -19.37755584716797\n",
      "Batch [30/131]: Loss = -27.259220123291016\n",
      "Batch [31/131]: Loss = -33.42570877075195\n",
      "Batch [32/131]: Loss = 148.4270782470703\n",
      "Batch [33/131]: Loss = -5.846165657043457\n",
      "Batch [34/131]: Loss = -7.538793087005615\n",
      "Batch [35/131]: Loss = -7.198334693908691\n",
      "Batch [36/131]: Loss = -7.205214023590088\n",
      "Batch [37/131]: Loss = -8.066186904907227\n",
      "Batch [38/131]: Loss = -8.49242877960205\n",
      "Batch [39/131]: Loss = -9.464804649353027\n",
      "Batch [40/131]: Loss = -11.380280494689941\n",
      "Batch [41/131]: Loss = -11.788003921508789\n",
      "Batch [42/131]: Loss = -32.985225677490234\n",
      "Batch [43/131]: Loss = 17.864437103271484\n",
      "Batch [44/131]: Loss = 7.600534439086914\n",
      "Batch [45/131]: Loss = 0.478534460067749\n",
      "Batch [46/131]: Loss = 2.367666721343994\n",
      "Batch [47/131]: Loss = -2.0993337631225586\n",
      "Batch [48/131]: Loss = -1.848857045173645\n",
      "Batch [49/131]: Loss = -3.180952310562134\n",
      "Batch [50/131]: Loss = -3.473752021789551\n",
      "Batch [51/131]: Loss = -4.2660980224609375\n",
      "Batch [52/131]: Loss = -5.171350955963135\n",
      "Batch [53/131]: Loss = -5.323464393615723\n",
      "Batch [54/131]: Loss = -5.892446994781494\n",
      "Batch [55/131]: Loss = -6.60306453704834\n",
      "Batch [56/131]: Loss = -7.4198198318481445\n",
      "Batch [57/131]: Loss = -7.792210102081299\n",
      "Batch [58/131]: Loss = -7.523299217224121\n",
      "Batch [59/131]: Loss = -7.216497421264648\n",
      "Batch [60/131]: Loss = -8.599353790283203\n",
      "Batch [61/131]: Loss = -8.588134765625\n",
      "Batch [62/131]: Loss = -8.572155952453613\n",
      "Batch [63/131]: Loss = -9.730669975280762\n",
      "Batch [64/131]: Loss = -10.72994327545166\n",
      "Batch [65/131]: Loss = -12.576425552368164\n",
      "Batch [66/131]: Loss = -11.582357406616211\n",
      "Batch [67/131]: Loss = -17.177892684936523\n",
      "Batch [68/131]: Loss = -18.324752807617188\n",
      "Batch [69/131]: Loss = -28.610597610473633\n",
      "Batch [70/131]: Loss = -41.378902435302734\n",
      "Batch [71/131]: Loss = 39.043357849121094\n",
      "Batch [72/131]: Loss = -1.748610496520996\n",
      "Batch [73/131]: Loss = -19.71900749206543\n",
      "Batch [74/131]: Loss = 8.736652374267578\n",
      "Batch [75/131]: Loss = 3.9494361877441406\n",
      "Batch [76/131]: Loss = 0.762718915939331\n",
      "Batch [77/131]: Loss = -1.3669075965881348\n",
      "Batch [78/131]: Loss = -2.236366033554077\n",
      "Batch [79/131]: Loss = -2.2149548530578613\n",
      "Batch [80/131]: Loss = -3.9473299980163574\n",
      "Batch [81/131]: Loss = -4.708975315093994\n",
      "Batch [82/131]: Loss = -9.437685012817383\n",
      "Batch [83/131]: Loss = -8.167014122009277\n",
      "Batch [84/131]: Loss = -10.732311248779297\n",
      "Batch [85/131]: Loss = -8.279149055480957\n",
      "Batch [86/131]: Loss = -16.072612762451172\n",
      "Batch [87/131]: Loss = -4.961251258850098\n",
      "Batch [88/131]: Loss = -4.594620704650879\n",
      "Batch [89/131]: Loss = -12.714478492736816\n",
      "Batch [90/131]: Loss = -13.956037521362305\n",
      "Batch [91/131]: Loss = -13.602108001708984\n",
      "Batch [92/131]: Loss = -13.900461196899414\n",
      "Batch [93/131]: Loss = -6.042305946350098\n",
      "Batch [94/131]: Loss = -9.30941390991211\n",
      "Batch [95/131]: Loss = -10.066225051879883\n",
      "Batch [96/131]: Loss = -13.078712463378906\n",
      "Batch [97/131]: Loss = -6.81962251663208\n",
      "Batch [98/131]: Loss = -22.486026763916016\n",
      "Batch [99/131]: Loss = -14.856115341186523\n",
      "Batch [100/131]: Loss = -21.884708404541016\n",
      "Batch [101/131]: Loss = -29.438495635986328\n",
      "Batch [102/131]: Loss = -29.62651824951172\n",
      "Batch [103/131]: Loss = -45.61772155761719\n",
      "Batch [104/131]: Loss = -46.47885513305664\n",
      "Batch [105/131]: Loss = -2052.1005859375\n",
      "Batch [106/131]: Loss = 40.715606689453125\n",
      "Batch [107/131]: Loss = 15.61441421508789\n",
      "Batch [108/131]: Loss = 11.025940895080566\n",
      "Batch [109/131]: Loss = -2.2542805671691895\n",
      "Batch [110/131]: Loss = -2.3646318912506104\n",
      "Batch [111/131]: Loss = -0.662816047668457\n",
      "Batch [112/131]: Loss = -3.2015445232391357\n",
      "Batch [113/131]: Loss = -2.55615234375\n",
      "Batch [114/131]: Loss = -5.558959007263184\n",
      "Batch [115/131]: Loss = -2.5542051792144775\n",
      "Batch [116/131]: Loss = -6.348198413848877\n",
      "Batch [117/131]: Loss = -1.753662109375\n",
      "Batch [118/131]: Loss = -5.708112716674805\n",
      "Batch [119/131]: Loss = -5.730374336242676\n",
      "Batch [120/131]: Loss = -6.561769962310791\n",
      "Batch [121/131]: Loss = -6.449769496917725\n",
      "Batch [122/131]: Loss = -5.855332374572754\n",
      "Batch [123/131]: Loss = -6.80716609954834\n",
      "Batch [124/131]: Loss = -6.554191589355469\n",
      "Batch [125/131]: Loss = -7.106926441192627\n",
      "Batch [126/131]: Loss = -6.4957594871521\n",
      "Batch [127/131]: Loss = -6.661288261413574\n",
      "Batch [128/131]: Loss = -6.571044921875\n",
      "Batch [129/131]: Loss = -6.060357570648193\n",
      "Batch [130/131]: Loss = -6.812634468078613\n",
      "Batch [131/131]: Loss = -7.194129467010498\n",
      "Epoch 9: Validation Loss = -6.705144511328803\n",
      "Validation loss did not improve for 3 epochs.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "Training Complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': -9.043763743506538, 'train_loss': -17.6627015043761},\n",
       " {'val_loss': -8.032420449786716, 'train_loss': -6.824938629419749},\n",
       " {'val_loss': -0.9122463332282172, 'train_loss': -10.678890320180937},\n",
       " {'val_loss': -9.280166202121311, 'train_loss': -0.2179130925477006},\n",
       " {'val_loss': -4.6812880171669855, 'train_loss': -14.242163469545714},\n",
       " {'val_loss': -306.43472099635335, 'train_loss': -11.769752860979269},\n",
       " {'val_loss': -5.901968326833513, 'train_loss': -36.01037700781385},\n",
       " {'val_loss': -7.945633835262722, 'train_loss': -12.884168342779612},\n",
       " {'val_loss': -6.705144511328803, 'train_loss': -20.04830853675158}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(\n",
    "    epochs=20, \n",
    "    optimizer_class=torch.optim.Adam, \n",
    "    model=model, \n",
    "    learning_rate=0.001, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    lr_scheduler_class=torch.optim.lr_scheduler.ReduceLROnPlateau, \n",
    "    model_name=\"best_model_dl3.pt\", \n",
    "    patience=3, \n",
    "    factor=0.5,\n",
    "    device='cpu'  # Use CPU instead of CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38d883ff-adea-4927-8b16-6d0b2b73392d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_dice_score(pred: torch.Tensor, gt: torch.Tensor, threshold: float):\n",
    "    num_classes = pred.shape[0]\n",
    "    dice_score = 0\n",
    "    pred = (pred > threshold).float()\n",
    "    for cl in range(num_classes):\n",
    "        intersection = torch.sum(pred[cl] * gt)\n",
    "        union = torch.sum(pred[cl] + gt + 1e-8)\n",
    "        dice_score += ((2 * intersection) / union)\n",
    "\n",
    "    dice_score /= num_classes\n",
    "    \n",
    "    return dice_score.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23a387ab-6a24-4a96-abe0-72f8be023d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias']\n",
      "Unexpected keys: ['backbone.0.weight', 'backbone.1.weight', 'backbone.1.bias', 'backbone.1.running_mean', 'backbone.1.running_var', 'backbone.1.num_batches_tracked', 'backbone.4.0.conv1.weight', 'backbone.4.0.bn1.weight', 'backbone.4.0.bn1.bias', 'backbone.4.0.bn1.running_mean', 'backbone.4.0.bn1.running_var', 'backbone.4.0.bn1.num_batches_tracked', 'backbone.4.0.conv2.weight', 'backbone.4.0.bn2.weight', 'backbone.4.0.bn2.bias', 'backbone.4.0.bn2.running_mean', 'backbone.4.0.bn2.running_var', 'backbone.4.0.bn2.num_batches_tracked', 'backbone.4.0.conv3.weight', 'backbone.4.0.bn3.weight', 'backbone.4.0.bn3.bias', 'backbone.4.0.bn3.running_mean', 'backbone.4.0.bn3.running_var', 'backbone.4.0.bn3.num_batches_tracked', 'backbone.4.0.downsample.0.weight', 'backbone.4.0.downsample.1.weight', 'backbone.4.0.downsample.1.bias', 'backbone.4.0.downsample.1.running_mean', 'backbone.4.0.downsample.1.running_var', 'backbone.4.0.downsample.1.num_batches_tracked', 'backbone.4.1.conv1.weight', 'backbone.4.1.bn1.weight', 'backbone.4.1.bn1.bias', 'backbone.4.1.bn1.running_mean', 'backbone.4.1.bn1.running_var', 'backbone.4.1.bn1.num_batches_tracked', 'backbone.4.1.conv2.weight', 'backbone.4.1.bn2.weight', 'backbone.4.1.bn2.bias', 'backbone.4.1.bn2.running_mean', 'backbone.4.1.bn2.running_var', 'backbone.4.1.bn2.num_batches_tracked', 'backbone.4.1.conv3.weight', 'backbone.4.1.bn3.weight', 'backbone.4.1.bn3.bias', 'backbone.4.1.bn3.running_mean', 'backbone.4.1.bn3.running_var', 'backbone.4.1.bn3.num_batches_tracked', 'backbone.4.2.conv1.weight', 'backbone.4.2.bn1.weight', 'backbone.4.2.bn1.bias', 'backbone.4.2.bn1.running_mean', 'backbone.4.2.bn1.running_var', 'backbone.4.2.bn1.num_batches_tracked', 'backbone.4.2.conv2.weight', 'backbone.4.2.bn2.weight', 'backbone.4.2.bn2.bias', 'backbone.4.2.bn2.running_mean', 'backbone.4.2.bn2.running_var', 'backbone.4.2.bn2.num_batches_tracked', 'backbone.4.2.conv3.weight', 'backbone.4.2.bn3.weight', 'backbone.4.2.bn3.bias', 'backbone.4.2.bn3.running_mean', 'backbone.4.2.bn3.running_var', 'backbone.4.2.bn3.num_batches_tracked', 'backbone.5.0.conv1.weight', 'backbone.5.0.bn1.weight', 'backbone.5.0.bn1.bias', 'backbone.5.0.bn1.running_mean', 'backbone.5.0.bn1.running_var', 'backbone.5.0.bn1.num_batches_tracked', 'backbone.5.0.conv2.weight', 'backbone.5.0.bn2.weight', 'backbone.5.0.bn2.bias', 'backbone.5.0.bn2.running_mean', 'backbone.5.0.bn2.running_var', 'backbone.5.0.bn2.num_batches_tracked', 'backbone.5.0.conv3.weight', 'backbone.5.0.bn3.weight', 'backbone.5.0.bn3.bias', 'backbone.5.0.bn3.running_mean', 'backbone.5.0.bn3.running_var', 'backbone.5.0.bn3.num_batches_tracked', 'backbone.5.0.downsample.0.weight', 'backbone.5.0.downsample.1.weight', 'backbone.5.0.downsample.1.bias', 'backbone.5.0.downsample.1.running_mean', 'backbone.5.0.downsample.1.running_var', 'backbone.5.0.downsample.1.num_batches_tracked', 'backbone.5.1.conv1.weight', 'backbone.5.1.bn1.weight', 'backbone.5.1.bn1.bias', 'backbone.5.1.bn1.running_mean', 'backbone.5.1.bn1.running_var', 'backbone.5.1.bn1.num_batches_tracked', 'backbone.5.1.conv2.weight', 'backbone.5.1.bn2.weight', 'backbone.5.1.bn2.bias', 'backbone.5.1.bn2.running_mean', 'backbone.5.1.bn2.running_var', 'backbone.5.1.bn2.num_batches_tracked', 'backbone.5.1.conv3.weight', 'backbone.5.1.bn3.weight', 'backbone.5.1.bn3.bias', 'backbone.5.1.bn3.running_mean', 'backbone.5.1.bn3.running_var', 'backbone.5.1.bn3.num_batches_tracked', 'backbone.5.2.conv1.weight', 'backbone.5.2.bn1.weight', 'backbone.5.2.bn1.bias', 'backbone.5.2.bn1.running_mean', 'backbone.5.2.bn1.running_var', 'backbone.5.2.bn1.num_batches_tracked', 'backbone.5.2.conv2.weight', 'backbone.5.2.bn2.weight', 'backbone.5.2.bn2.bias', 'backbone.5.2.bn2.running_mean', 'backbone.5.2.bn2.running_var', 'backbone.5.2.bn2.num_batches_tracked', 'backbone.5.2.conv3.weight', 'backbone.5.2.bn3.weight', 'backbone.5.2.bn3.bias', 'backbone.5.2.bn3.running_mean', 'backbone.5.2.bn3.running_var', 'backbone.5.2.bn3.num_batches_tracked', 'backbone.5.3.conv1.weight', 'backbone.5.3.bn1.weight', 'backbone.5.3.bn1.bias', 'backbone.5.3.bn1.running_mean', 'backbone.5.3.bn1.running_var', 'backbone.5.3.bn1.num_batches_tracked', 'backbone.5.3.conv2.weight', 'backbone.5.3.bn2.weight', 'backbone.5.3.bn2.bias', 'backbone.5.3.bn2.running_mean', 'backbone.5.3.bn2.running_var', 'backbone.5.3.bn2.num_batches_tracked', 'backbone.5.3.conv3.weight', 'backbone.5.3.bn3.weight', 'backbone.5.3.bn3.bias', 'backbone.5.3.bn3.running_mean', 'backbone.5.3.bn3.running_var', 'backbone.5.3.bn3.num_batches_tracked', 'backbone.6.0.conv1.weight', 'backbone.6.0.bn1.weight', 'backbone.6.0.bn1.bias', 'backbone.6.0.bn1.running_mean', 'backbone.6.0.bn1.running_var', 'backbone.6.0.bn1.num_batches_tracked', 'backbone.6.0.conv2.weight', 'backbone.6.0.bn2.weight', 'backbone.6.0.bn2.bias', 'backbone.6.0.bn2.running_mean', 'backbone.6.0.bn2.running_var', 'backbone.6.0.bn2.num_batches_tracked', 'backbone.6.0.conv3.weight', 'backbone.6.0.bn3.weight', 'backbone.6.0.bn3.bias', 'backbone.6.0.bn3.running_mean', 'backbone.6.0.bn3.running_var', 'backbone.6.0.bn3.num_batches_tracked', 'backbone.6.0.downsample.0.weight', 'backbone.6.0.downsample.1.weight', 'backbone.6.0.downsample.1.bias', 'backbone.6.0.downsample.1.running_mean', 'backbone.6.0.downsample.1.running_var', 'backbone.6.0.downsample.1.num_batches_tracked', 'backbone.6.1.conv1.weight', 'backbone.6.1.bn1.weight', 'backbone.6.1.bn1.bias', 'backbone.6.1.bn1.running_mean', 'backbone.6.1.bn1.running_var', 'backbone.6.1.bn1.num_batches_tracked', 'backbone.6.1.conv2.weight', 'backbone.6.1.bn2.weight', 'backbone.6.1.bn2.bias', 'backbone.6.1.bn2.running_mean', 'backbone.6.1.bn2.running_var', 'backbone.6.1.bn2.num_batches_tracked', 'backbone.6.1.conv3.weight', 'backbone.6.1.bn3.weight', 'backbone.6.1.bn3.bias', 'backbone.6.1.bn3.running_mean', 'backbone.6.1.bn3.running_var', 'backbone.6.1.bn3.num_batches_tracked', 'backbone.6.2.conv1.weight', 'backbone.6.2.bn1.weight', 'backbone.6.2.bn1.bias', 'backbone.6.2.bn1.running_mean', 'backbone.6.2.bn1.running_var', 'backbone.6.2.bn1.num_batches_tracked', 'backbone.6.2.conv2.weight', 'backbone.6.2.bn2.weight', 'backbone.6.2.bn2.bias', 'backbone.6.2.bn2.running_mean', 'backbone.6.2.bn2.running_var', 'backbone.6.2.bn2.num_batches_tracked', 'backbone.6.2.conv3.weight', 'backbone.6.2.bn3.weight', 'backbone.6.2.bn3.bias', 'backbone.6.2.bn3.running_mean', 'backbone.6.2.bn3.running_var', 'backbone.6.2.bn3.num_batches_tracked', 'backbone.6.3.conv1.weight', 'backbone.6.3.bn1.weight', 'backbone.6.3.bn1.bias', 'backbone.6.3.bn1.running_mean', 'backbone.6.3.bn1.running_var', 'backbone.6.3.bn1.num_batches_tracked', 'backbone.6.3.conv2.weight', 'backbone.6.3.bn2.weight', 'backbone.6.3.bn2.bias', 'backbone.6.3.bn2.running_mean', 'backbone.6.3.bn2.running_var', 'backbone.6.3.bn2.num_batches_tracked', 'backbone.6.3.conv3.weight', 'backbone.6.3.bn3.weight', 'backbone.6.3.bn3.bias', 'backbone.6.3.bn3.running_mean', 'backbone.6.3.bn3.running_var', 'backbone.6.3.bn3.num_batches_tracked', 'backbone.6.4.conv1.weight', 'backbone.6.4.bn1.weight', 'backbone.6.4.bn1.bias', 'backbone.6.4.bn1.running_mean', 'backbone.6.4.bn1.running_var', 'backbone.6.4.bn1.num_batches_tracked', 'backbone.6.4.conv2.weight', 'backbone.6.4.bn2.weight', 'backbone.6.4.bn2.bias', 'backbone.6.4.bn2.running_mean', 'backbone.6.4.bn2.running_var', 'backbone.6.4.bn2.num_batches_tracked', 'backbone.6.4.conv3.weight', 'backbone.6.4.bn3.weight', 'backbone.6.4.bn3.bias', 'backbone.6.4.bn3.running_mean', 'backbone.6.4.bn3.running_var', 'backbone.6.4.bn3.num_batches_tracked', 'backbone.6.5.conv1.weight', 'backbone.6.5.bn1.weight', 'backbone.6.5.bn1.bias', 'backbone.6.5.bn1.running_mean', 'backbone.6.5.bn1.running_var', 'backbone.6.5.bn1.num_batches_tracked', 'backbone.6.5.conv2.weight', 'backbone.6.5.bn2.weight', 'backbone.6.5.bn2.bias', 'backbone.6.5.bn2.running_mean', 'backbone.6.5.bn2.running_var', 'backbone.6.5.bn2.num_batches_tracked', 'backbone.6.5.conv3.weight', 'backbone.6.5.bn3.weight', 'backbone.6.5.bn3.bias', 'backbone.6.5.bn3.running_mean', 'backbone.6.5.bn3.running_var', 'backbone.6.5.bn3.num_batches_tracked', 'backbone.6.6.conv1.weight', 'backbone.6.6.bn1.weight', 'backbone.6.6.bn1.bias', 'backbone.6.6.bn1.running_mean', 'backbone.6.6.bn1.running_var', 'backbone.6.6.bn1.num_batches_tracked', 'backbone.6.6.conv2.weight', 'backbone.6.6.bn2.weight', 'backbone.6.6.bn2.bias', 'backbone.6.6.bn2.running_mean', 'backbone.6.6.bn2.running_var', 'backbone.6.6.bn2.num_batches_tracked', 'backbone.6.6.conv3.weight', 'backbone.6.6.bn3.weight', 'backbone.6.6.bn3.bias', 'backbone.6.6.bn3.running_mean', 'backbone.6.6.bn3.running_var', 'backbone.6.6.bn3.num_batches_tracked', 'backbone.6.7.conv1.weight', 'backbone.6.7.bn1.weight', 'backbone.6.7.bn1.bias', 'backbone.6.7.bn1.running_mean', 'backbone.6.7.bn1.running_var', 'backbone.6.7.bn1.num_batches_tracked', 'backbone.6.7.conv2.weight', 'backbone.6.7.bn2.weight', 'backbone.6.7.bn2.bias', 'backbone.6.7.bn2.running_mean', 'backbone.6.7.bn2.running_var', 'backbone.6.7.bn2.num_batches_tracked', 'backbone.6.7.conv3.weight', 'backbone.6.7.bn3.weight', 'backbone.6.7.bn3.bias', 'backbone.6.7.bn3.running_mean', 'backbone.6.7.bn3.running_var', 'backbone.6.7.bn3.num_batches_tracked', 'backbone.6.8.conv1.weight', 'backbone.6.8.bn1.weight', 'backbone.6.8.bn1.bias', 'backbone.6.8.bn1.running_mean', 'backbone.6.8.bn1.running_var', 'backbone.6.8.bn1.num_batches_tracked', 'backbone.6.8.conv2.weight', 'backbone.6.8.bn2.weight', 'backbone.6.8.bn2.bias', 'backbone.6.8.bn2.running_mean', 'backbone.6.8.bn2.running_var', 'backbone.6.8.bn2.num_batches_tracked', 'backbone.6.8.conv3.weight', 'backbone.6.8.bn3.weight', 'backbone.6.8.bn3.bias', 'backbone.6.8.bn3.running_mean', 'backbone.6.8.bn3.running_var', 'backbone.6.8.bn3.num_batches_tracked', 'backbone.6.9.conv1.weight', 'backbone.6.9.bn1.weight', 'backbone.6.9.bn1.bias', 'backbone.6.9.bn1.running_mean', 'backbone.6.9.bn1.running_var', 'backbone.6.9.bn1.num_batches_tracked', 'backbone.6.9.conv2.weight', 'backbone.6.9.bn2.weight', 'backbone.6.9.bn2.bias', 'backbone.6.9.bn2.running_mean', 'backbone.6.9.bn2.running_var', 'backbone.6.9.bn2.num_batches_tracked', 'backbone.6.9.conv3.weight', 'backbone.6.9.bn3.weight', 'backbone.6.9.bn3.bias', 'backbone.6.9.bn3.running_mean', 'backbone.6.9.bn3.running_var', 'backbone.6.9.bn3.num_batches_tracked', 'backbone.6.10.conv1.weight', 'backbone.6.10.bn1.weight', 'backbone.6.10.bn1.bias', 'backbone.6.10.bn1.running_mean', 'backbone.6.10.bn1.running_var', 'backbone.6.10.bn1.num_batches_tracked', 'backbone.6.10.conv2.weight', 'backbone.6.10.bn2.weight', 'backbone.6.10.bn2.bias', 'backbone.6.10.bn2.running_mean', 'backbone.6.10.bn2.running_var', 'backbone.6.10.bn2.num_batches_tracked', 'backbone.6.10.conv3.weight', 'backbone.6.10.bn3.weight', 'backbone.6.10.bn3.bias', 'backbone.6.10.bn3.running_mean', 'backbone.6.10.bn3.running_var', 'backbone.6.10.bn3.num_batches_tracked', 'backbone.6.11.conv1.weight', 'backbone.6.11.bn1.weight', 'backbone.6.11.bn1.bias', 'backbone.6.11.bn1.running_mean', 'backbone.6.11.bn1.running_var', 'backbone.6.11.bn1.num_batches_tracked', 'backbone.6.11.conv2.weight', 'backbone.6.11.bn2.weight', 'backbone.6.11.bn2.bias', 'backbone.6.11.bn2.running_mean', 'backbone.6.11.bn2.running_var', 'backbone.6.11.bn2.num_batches_tracked', 'backbone.6.11.conv3.weight', 'backbone.6.11.bn3.weight', 'backbone.6.11.bn3.bias', 'backbone.6.11.bn3.running_mean', 'backbone.6.11.bn3.running_var', 'backbone.6.11.bn3.num_batches_tracked', 'backbone.6.12.conv1.weight', 'backbone.6.12.bn1.weight', 'backbone.6.12.bn1.bias', 'backbone.6.12.bn1.running_mean', 'backbone.6.12.bn1.running_var', 'backbone.6.12.bn1.num_batches_tracked', 'backbone.6.12.conv2.weight', 'backbone.6.12.bn2.weight', 'backbone.6.12.bn2.bias', 'backbone.6.12.bn2.running_mean', 'backbone.6.12.bn2.running_var', 'backbone.6.12.bn2.num_batches_tracked', 'backbone.6.12.conv3.weight', 'backbone.6.12.bn3.weight', 'backbone.6.12.bn3.bias', 'backbone.6.12.bn3.running_mean', 'backbone.6.12.bn3.running_var', 'backbone.6.12.bn3.num_batches_tracked', 'backbone.6.13.conv1.weight', 'backbone.6.13.bn1.weight', 'backbone.6.13.bn1.bias', 'backbone.6.13.bn1.running_mean', 'backbone.6.13.bn1.running_var', 'backbone.6.13.bn1.num_batches_tracked', 'backbone.6.13.conv2.weight', 'backbone.6.13.bn2.weight', 'backbone.6.13.bn2.bias', 'backbone.6.13.bn2.running_mean', 'backbone.6.13.bn2.running_var', 'backbone.6.13.bn2.num_batches_tracked', 'backbone.6.13.conv3.weight', 'backbone.6.13.bn3.weight', 'backbone.6.13.bn3.bias', 'backbone.6.13.bn3.running_mean', 'backbone.6.13.bn3.running_var', 'backbone.6.13.bn3.num_batches_tracked', 'backbone.6.14.conv1.weight', 'backbone.6.14.bn1.weight', 'backbone.6.14.bn1.bias', 'backbone.6.14.bn1.running_mean', 'backbone.6.14.bn1.running_var', 'backbone.6.14.bn1.num_batches_tracked', 'backbone.6.14.conv2.weight', 'backbone.6.14.bn2.weight', 'backbone.6.14.bn2.bias', 'backbone.6.14.bn2.running_mean', 'backbone.6.14.bn2.running_var', 'backbone.6.14.bn2.num_batches_tracked', 'backbone.6.14.conv3.weight', 'backbone.6.14.bn3.weight', 'backbone.6.14.bn3.bias', 'backbone.6.14.bn3.running_mean', 'backbone.6.14.bn3.running_var', 'backbone.6.14.bn3.num_batches_tracked', 'backbone.6.15.conv1.weight', 'backbone.6.15.bn1.weight', 'backbone.6.15.bn1.bias', 'backbone.6.15.bn1.running_mean', 'backbone.6.15.bn1.running_var', 'backbone.6.15.bn1.num_batches_tracked', 'backbone.6.15.conv2.weight', 'backbone.6.15.bn2.weight', 'backbone.6.15.bn2.bias', 'backbone.6.15.bn2.running_mean', 'backbone.6.15.bn2.running_var', 'backbone.6.15.bn2.num_batches_tracked', 'backbone.6.15.conv3.weight', 'backbone.6.15.bn3.weight', 'backbone.6.15.bn3.bias', 'backbone.6.15.bn3.running_mean', 'backbone.6.15.bn3.running_var', 'backbone.6.15.bn3.num_batches_tracked', 'backbone.6.16.conv1.weight', 'backbone.6.16.bn1.weight', 'backbone.6.16.bn1.bias', 'backbone.6.16.bn1.running_mean', 'backbone.6.16.bn1.running_var', 'backbone.6.16.bn1.num_batches_tracked', 'backbone.6.16.conv2.weight', 'backbone.6.16.bn2.weight', 'backbone.6.16.bn2.bias', 'backbone.6.16.bn2.running_mean', 'backbone.6.16.bn2.running_var', 'backbone.6.16.bn2.num_batches_tracked', 'backbone.6.16.conv3.weight', 'backbone.6.16.bn3.weight', 'backbone.6.16.bn3.bias', 'backbone.6.16.bn3.running_mean', 'backbone.6.16.bn3.running_var', 'backbone.6.16.bn3.num_batches_tracked', 'backbone.6.17.conv1.weight', 'backbone.6.17.bn1.weight', 'backbone.6.17.bn1.bias', 'backbone.6.17.bn1.running_mean', 'backbone.6.17.bn1.running_var', 'backbone.6.17.bn1.num_batches_tracked', 'backbone.6.17.conv2.weight', 'backbone.6.17.bn2.weight', 'backbone.6.17.bn2.bias', 'backbone.6.17.bn2.running_mean', 'backbone.6.17.bn2.running_var', 'backbone.6.17.bn2.num_batches_tracked', 'backbone.6.17.conv3.weight', 'backbone.6.17.bn3.weight', 'backbone.6.17.bn3.bias', 'backbone.6.17.bn3.running_mean', 'backbone.6.17.bn3.running_var', 'backbone.6.17.bn3.num_batches_tracked', 'backbone.6.18.conv1.weight', 'backbone.6.18.bn1.weight', 'backbone.6.18.bn1.bias', 'backbone.6.18.bn1.running_mean', 'backbone.6.18.bn1.running_var', 'backbone.6.18.bn1.num_batches_tracked', 'backbone.6.18.conv2.weight', 'backbone.6.18.bn2.weight', 'backbone.6.18.bn2.bias', 'backbone.6.18.bn2.running_mean', 'backbone.6.18.bn2.running_var', 'backbone.6.18.bn2.num_batches_tracked', 'backbone.6.18.conv3.weight', 'backbone.6.18.bn3.weight', 'backbone.6.18.bn3.bias', 'backbone.6.18.bn3.running_mean', 'backbone.6.18.bn3.running_var', 'backbone.6.18.bn3.num_batches_tracked', 'backbone.6.19.conv1.weight', 'backbone.6.19.bn1.weight', 'backbone.6.19.bn1.bias', 'backbone.6.19.bn1.running_mean', 'backbone.6.19.bn1.running_var', 'backbone.6.19.bn1.num_batches_tracked', 'backbone.6.19.conv2.weight', 'backbone.6.19.bn2.weight', 'backbone.6.19.bn2.bias', 'backbone.6.19.bn2.running_mean', 'backbone.6.19.bn2.running_var', 'backbone.6.19.bn2.num_batches_tracked', 'backbone.6.19.conv3.weight', 'backbone.6.19.bn3.weight', 'backbone.6.19.bn3.bias', 'backbone.6.19.bn3.running_mean', 'backbone.6.19.bn3.running_var', 'backbone.6.19.bn3.num_batches_tracked', 'backbone.6.20.conv1.weight', 'backbone.6.20.bn1.weight', 'backbone.6.20.bn1.bias', 'backbone.6.20.bn1.running_mean', 'backbone.6.20.bn1.running_var', 'backbone.6.20.bn1.num_batches_tracked', 'backbone.6.20.conv2.weight', 'backbone.6.20.bn2.weight', 'backbone.6.20.bn2.bias', 'backbone.6.20.bn2.running_mean', 'backbone.6.20.bn2.running_var', 'backbone.6.20.bn2.num_batches_tracked', 'backbone.6.20.conv3.weight', 'backbone.6.20.bn3.weight', 'backbone.6.20.bn3.bias', 'backbone.6.20.bn3.running_mean', 'backbone.6.20.bn3.running_var', 'backbone.6.20.bn3.num_batches_tracked', 'backbone.6.21.conv1.weight', 'backbone.6.21.bn1.weight', 'backbone.6.21.bn1.bias', 'backbone.6.21.bn1.running_mean', 'backbone.6.21.bn1.running_var', 'backbone.6.21.bn1.num_batches_tracked', 'backbone.6.21.conv2.weight', 'backbone.6.21.bn2.weight', 'backbone.6.21.bn2.bias', 'backbone.6.21.bn2.running_mean', 'backbone.6.21.bn2.running_var', 'backbone.6.21.bn2.num_batches_tracked', 'backbone.6.21.conv3.weight', 'backbone.6.21.bn3.weight', 'backbone.6.21.bn3.bias', 'backbone.6.21.bn3.running_mean', 'backbone.6.21.bn3.running_var', 'backbone.6.21.bn3.num_batches_tracked', 'backbone.6.22.conv1.weight', 'backbone.6.22.bn1.weight', 'backbone.6.22.bn1.bias', 'backbone.6.22.bn1.running_mean', 'backbone.6.22.bn1.running_var', 'backbone.6.22.bn1.num_batches_tracked', 'backbone.6.22.conv2.weight', 'backbone.6.22.bn2.weight', 'backbone.6.22.bn2.bias', 'backbone.6.22.bn2.running_mean', 'backbone.6.22.bn2.running_var', 'backbone.6.22.bn2.num_batches_tracked', 'backbone.6.22.conv3.weight', 'backbone.6.22.bn3.weight', 'backbone.6.22.bn3.bias', 'backbone.6.22.bn3.running_mean', 'backbone.6.22.bn3.running_var', 'backbone.6.22.bn3.num_batches_tracked', 'aspp.aconv1.weight', 'aspp.aconv1.bias', 'aspp.aconv2.weight', 'aspp.aconv2.bias', 'aspp.aconv3.weight', 'aspp.aconv3.bias', 'aspp.aconv4.weight', 'aspp.aconv4.bias', 'aspp.aconv5.weight', 'aspp.aconv5.bias', 'aspp.bn.weight', 'aspp.bn.bias', 'aspp.bn.running_mean', 'aspp.bn.running_var', 'aspp.bn.num_batches_tracked', 'aspp.pred_conv.weight', 'aspp.pred_conv.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=57600, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n",
    "        \n",
    "        # Adjust fc1 to match the flattened size after convolution\n",
    "        self.fc1 = nn.Linear(64 * 30 * 30, 10)  # Changed 65536 to 57600 based on convolution output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MyModel()\n",
    "\n",
    "# Load the state_dict with strict=False to ignore missing/unexpected keys\n",
    "state_dict = torch.load(\"C:\\\\Users\\\\pcs\\\\OneDrive\\\\Desktop\\\\Project Major\\\\best_model_dl3.pt\", map_location=device, weights_only=True)\n",
    "\n",
    "\n",
    "# Load state_dict with strict=False to avoid errors with missing/extra keys\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(\"Missing keys:\", missing_keys)\n",
    "print(\"Unexpected keys:\", unexpected_keys)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3855199-a88a-4bc8-b6c2-29562ba5c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def visualize_result(img: torch.Tensor, gt: torch.Tensor, model: nn.Module, thresh: float, index: int):\n",
    "    # Get predictions from the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)  # Use the model to make predictions\n",
    "\n",
    "    # For classification tasks, we will take the class with the highest probability as the prediction\n",
    "    pred = torch.argmax(pred, dim=1)  # Assuming the output of the model is class logits\n",
    "\n",
    "    # Get the specific image and prediction for the provided index\n",
    "    pred = pred[index]\n",
    "    img = img[index]\n",
    "    gt = gt[index]\n",
    "\n",
    "    # Convert the image tensor to a NumPy array for visualization\n",
    "    img_array = img.permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "    # For the predicted and ground truth values, display the class (assuming it's a classification task)\n",
    "    pred_array = pred.detach().cpu().numpy()\n",
    "    gt_array = gt.detach().cpu().numpy()\n",
    "\n",
    "    # Create a figure with 1 row and 3 columns to display the original image, prediction, and ground truth\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "    # Plot the original image\n",
    "    ax[0].imshow(img_array)\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "\n",
    "    # Plot the prediction\n",
    "    ax[1].imshow(pred_array, cmap='hot')  # Use the 'hot' colormap for better visualization\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(\"Prediction\")\n",
    "\n",
    "    # Plot the ground truth\n",
    "    ax[2].imshow(gt_array, cmap='hot')\n",
    "    ax[2].axis(\"off\")\n",
    "    ax[2].set_title(\"Ground Truth\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "864d0f7b-df27-4d56-80c1-fe82c6c2ea0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x16646400 and 57600x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m img, t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mvisualize_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m, in \u001b[0;36mvisualize_result\u001b[1;34m(img, gt, model, thresh, index)\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the model to make predictions\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# For classification tasks, we will take the class with the highest probability as the prediction\u001b[39;00m\n\u001b[0;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming the output of the model is class logits\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[51], line 15\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x16646400 and 57600x10)"
     ]
    }
   ],
   "source": [
    "img, t = next(iter(val_loader))\n",
    "\n",
    "visualize_result(img, t, model, thresh=0.2, index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ba431c2-c682-4db6-9585-0b93b1583e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = torch.load(\"C:\\\\Users\\\\pcs\\\\OneDrive\\\\Desktop\\\\Project Major\\\\best_model_dl3.pt\", map_location=device, weights_only=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6973c66c-1646-45a6-bcc0-552cb6009368",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m, in \u001b[0;36mvisualize_result\u001b[1;34m(img, gt, model, thresh, index)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_result\u001b[39m(img: torch\u001b[38;5;241m.\u001b[39mTensor, gt: torch\u001b[38;5;241m.\u001b[39mTensor, model: nn\u001b[38;5;241m.\u001b[39mModule, thresh: \u001b[38;5;28mfloat\u001b[39m, index: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Get predictions from the model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the model to make predictions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Get the prediction for the specific index\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     pred \u001b[38;5;241m=\u001b[39m pred[index]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "visualize_result(img, t, m1, 0.2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb419a46-aa51-4e7c-b233-427e647b4e53",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m d3_dices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m----> 3\u001b[0m     d3_dice \u001b[38;5;241m=\u001b[39m compute_dice_score(\u001b[43mm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_dl3.pt\u001b[39m\u001b[38;5;124m'\u001b[39m][i], t[i], \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      4\u001b[0m     d3_dices\u001b[38;5;241m.\u001b[39mappend(d3_dice)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDice score for index \u001b[39m\u001b[38;5;124m\"\u001b[39m, i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand DeepLab v3: \u001b[39m\u001b[38;5;124m\"\u001b[39m, d3_dice)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "d3_dices = []\n",
    "for i in range(img.shape[0]):\n",
    "    d3_dice = compute_dice_score(m1(img)['best_model_dl3.pt'][i], t[i], 0.2)\n",
    "    d3_dices.append(d3_dice)\n",
    "    print(\"Dice score for index \", i, \"and DeepLab v3: \", d3_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04f8112e-e9e5-4644-adcb-5a0e1a05373a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (1425371054.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[31], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    visualize_result(img, t, predictions,0.2,1)\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(m1)\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Get predictions from the model\n",
    "predictions = model(img)\n",
    "\n",
    "# Pass predictions to visualize_result\n",
    "visualize_result(img, t, predictions,0.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c41f4610-144e-4844-8e23-256d4b0a7415",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (2266830209.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[34], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    visualize_result(img, t, weights,0.2,1)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "weights = m1['backbone.0.weight']\n",
    "visualize_result(img, t, weights,0.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9abea7a5-10ba-4c91-a292-863f76614931",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (1481154784.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    visualize_result(img, t, weights,0.2,1)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Dummy OrderedDict\n",
    "m1 = OrderedDict([('backbone.0.weight', [[1, 2], [3, 4]])])\n",
    "weights = m1['backbone.0.weight']\n",
    "visualize_result(img, t, weights,0.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedbac9-9b88-41e3-9f95-5dd8099369b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
